Bootstrap: docker
From: nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04

%labels
    Author Yushuhang
    Description Container for lm-compose with CUDA 11.8 + PyTorch 2.3 + GPTQ/AWQ support

%environment
    export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    export TRANSFORMERS_CACHE=/workspace/hf_cache
    export HF_HOME=/workspace/hf_cache
    export CUDA_HOME=/usr/local/cuda
    export PATH=/opt/conda/bin:$PATH
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    export TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6;8.9"

%post
    apt-get update && apt-get install -y \
        git build-essential wget curl vim \
        ca-certificates libglib2.0-0 libsm6 libxrender1 libxext6 \
        libssl-dev libffi-dev libstdc++6 cmake ninja-build \
        && apt-get clean

    if ! command -v nvcc &> /dev/null; then
        echo "âš ï¸ Warning: nvcc not found in PATH. Please ensure CUDA is usable at runtime." >&2
    else
        echo "âœ… nvcc detected: $(which nvcc)"
    fi

    # å®‰è£… Conda
    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
    bash miniconda.sh -b -p /opt/conda && rm miniconda.sh
    export PATH=/opt/conda/bin:$PATH

    # åˆ›å»ºç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
    /opt/conda/bin/conda create -n lm-compose python=3.11 -y
    /opt/conda/bin/conda run -n lm-compose pip install --no-cache-dir torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 \
        --index-url https://download.pytorch.org/whl/cu118
    /opt/conda/bin/conda run -n lm-compose pip install -U pip setuptools wheel
    /opt/conda/bin/conda run -n lm-compose pip install pandas numpy transformers accelerate gekko packaging==24.0 huggingface_hub huggingface-cli

    # å…‹éš†ä¸»é¡¹ç›®ï¼ˆAutoGPTQ/AWQ å®‰è£…å»¶è¿Ÿï¼‰
    mkdir -p /workspace && cd /workspace
    git clone https://github.com/Harry-Yu-Shuhang/composable-interventions.git lm-compose
    cd lm-compose
    /opt/conda/bin/conda run -n lm-compose pip install -e .

%runscript
    echo "âœ… å®¹å™¨å¯åŠ¨æˆåŠŸï¼å½“å‰ç›®å½•: $(pwd)"

    # åˆå§‹åŒ– Conda
    source /opt/conda/etc/profile.d/conda.sh
    conda activate lm-compose

    export TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6;8.9"

    # è‡ªåŠ¨ç™»å½• HuggingFace
    TOKEN_FILE="/workspace/.hf_token"
    if [ -f "$TOKEN_FILE" ]; then
        huggingface-cli login --token "$(cat $TOKEN_FILE)" || echo "âš ï¸ HuggingFace ç™»å½•å¤±è´¥"
    fi

    # å®‰è£… AutoGPTQï¼ˆå¦‚ç¼ºï¼‰
    if ! python -c "import auto_gptq" 2>/dev/null; then
        echo "ğŸ“¦ å®‰è£… AutoGPTQ..."
        cd /workspace/lm-compose/AutoGPTQ
        pip install -e . || echo "âŒ AutoGPTQ å®‰è£…å¤±è´¥ï¼Œè¯·æ£€æŸ¥ CUDA ä¸ç¼–è¯‘å™¨é…ç½®"
    fi

    # å®‰è£… AutoAWQï¼ˆå¦‚ç¼ºï¼‰
    if ! python -c "import awq" 2>/dev/null; then
        echo "ğŸ“¦ å®‰è£… AutoAWQ..."
        cd /workspace/lm-compose/AutoAWQ
        pip install -e .
    fi

    # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
    MODEL_DIR="/workspace/models/deepseek-math-7b-instruct"
    if [ ! -d "$MODEL_DIR" ]; then
        echo "ğŸ“¥ æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå°è¯•ä¸‹è½½..."
        python -c "from huggingface_hub import snapshot_download; snapshot_download('deepseek-ai/deepseek-math-7b-instruct', local_dir='$MODEL_DIR', local_dir_use_symlinks=False)"
    fi

    exec "$@"
