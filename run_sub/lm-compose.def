Bootstrap: docker
From: nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04

%labels
    Author Yushuhang
    Description Container for lm-compose with CUDA 11.8 + PyTorch 2.3 + GPTQ/AWQ support

%environment
    export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    export TRANSFORMERS_CACHE=/workspace/hf_cache
    export HF_HOME=/workspace/hf_cache
    export CUDA_HOME=/usr/local/cuda
    export PATH=/opt/conda/bin:$PATH
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
    export TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6;8.9"

%post
    apt-get update && apt-get install -y \
        git build-essential wget curl vim \
        ca-certificates libglib2.0-0 libsm6 libxrender1 libxext6 \
        libssl-dev libffi-dev libstdc++6 cmake ninja-build \
        && apt-get clean

    if ! command -v nvcc &> /dev/null; then
        echo "⚠️ Warning: nvcc not found in PATH. Please ensure CUDA is usable at runtime." >&2
    else
        echo "✅ nvcc detected: $(which nvcc)"
    fi

    # 安装 Conda
    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
    bash miniconda.sh -b -p /opt/conda && rm miniconda.sh
    export PATH=/opt/conda/bin:$PATH

    # 创建环境并安装依赖
    /opt/conda/bin/conda create -n lm-compose python=3.11 -y
    /opt/conda/bin/conda run -n lm-compose pip install --no-cache-dir torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 \
        --index-url https://download.pytorch.org/whl/cu118
    /opt/conda/bin/conda run -n lm-compose pip install -U pip setuptools wheel
    /opt/conda/bin/conda run -n lm-compose pip install pandas numpy transformers accelerate gekko packaging==24.0 huggingface_hub huggingface-cli

    # 克隆主项目（AutoGPTQ/AWQ 安装延迟）
    mkdir -p /workspace && cd /workspace
    git clone https://github.com/Harry-Yu-Shuhang/composable-interventions.git lm-compose
    cd lm-compose
    /opt/conda/bin/conda run -n lm-compose pip install -e .

%runscript
    echo "✅ 容器启动成功！当前目录: $(pwd)"

    # 初始化 Conda
    source /opt/conda/etc/profile.d/conda.sh
    conda activate lm-compose

    export TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6;8.9"

    # 自动登录 HuggingFace
    TOKEN_FILE="/workspace/.hf_token"
    if [ -f "$TOKEN_FILE" ]; then
        huggingface-cli login --token "$(cat $TOKEN_FILE)" || echo "⚠️ HuggingFace 登录失败"
    fi

    # 安装 AutoGPTQ（如缺）
    if ! python -c "import auto_gptq" 2>/dev/null; then
        echo "📦 安装 AutoGPTQ..."
        cd /workspace/lm-compose/AutoGPTQ
        pip install -e . || echo "❌ AutoGPTQ 安装失败，请检查 CUDA 与编译器配置"
    fi

    # 安装 AutoAWQ（如缺）
    if ! python -c "import awq" 2>/dev/null; then
        echo "📦 安装 AutoAWQ..."
        cd /workspace/lm-compose/AutoAWQ
        pip install -e .
    fi

    # 自动下载模型
    MODEL_DIR="/workspace/models/deepseek-math-7b-instruct"
    if [ ! -d "$MODEL_DIR" ]; then
        echo "📥 模型未找到，尝试下载..."
        python -c "from huggingface_hub import snapshot_download; snapshot_download('deepseek-ai/deepseek-math-7b-instruct', local_dir='$MODEL_DIR', local_dir_use_symlinks=False)"
    fi

    exec "$@"
