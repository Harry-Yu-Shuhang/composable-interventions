from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "deepseek-ai/deepseek-llm-1.3b-base"
save_path = "./models/deepseek-1.3b"

# å¼ºåˆ¶ä½¿ç”¨ CPU
device = torch.device("cpu")

print(f"ğŸ”„ Downloading model {model_name} to {save_path}...")

# ä¸‹è½½å¹¶ä¿å­˜æ¨¡å‹æƒé‡
model = AutoModelForCausalLM.from_pretrained(model_name, device_map=None)
model.to(device)
model.save_pretrained(save_path)

# ä¸‹è½½å¹¶ä¿å­˜ tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.save_pretrained(save_path)

print("âœ… Done! Model and tokenizer saved to", save_path)