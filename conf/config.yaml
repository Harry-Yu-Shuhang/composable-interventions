defaults:
  - edit: ft  # Default setting without editing
  - compression: wanda  # Default setting without compression
  - unlearn: cut  # Default setting without unlearning
  - _self_

model_name: meta-llama/Llama-2-7b-chat-hf
#meta-llama/Meta-Llama-3-8B
#meta-llama/Llama-2-7b-chat-hf
dtype: torch.bfloat16
device: 0
model_parallel: true
seed: 0
tag: "default"
wandb: disabled # disabled or online
interventions: [edit]  # List of interventions

edit_dataset: "zsre"
stats_dir: "/scratch/sux7mp/stats"
max_length: 30
batch_size: 50

compression_dataset: c4
dataset: c4

number_of_edits: 50
edit_set: 50

load_ckpt: False
ckpt_path: /scratch/sux7mp/saved_models/checkpoint_20231221_113020.pth
save_ckpt: False

# RMU Unlearning Llama-3 Hyperparameters
unlearn_method: rmu
rmu_retain_corpora: ["wikitext", "wikitext"]
rmu_forget_corpora: ["bio-forget-corpus", "cyber-forget-corpus"]
rmu_alpha: [1000, 1000]
rmu_steering_coeffs: [20, 20]
rmu_lr: 5e-5
rmu_min_len: 0
rmu_max_len: 2000
rmu_batch_size: 4
rmu_max_num_batches: 250
rmu_layer_id: 5
rmu_layer_ids: [3, 4, 5]
rmu_param_ids: [5]
rmu_seed: 42