number_of_edits: 32
alg_name: "FT"
model_name: "meta-llama/Llama-2-7b-chat-hf"
model: "meta-llama/Llama-2-7b-chat-hf"
# model_name: "meta-llama/Llama-2-7b-hf"
# model: "meta-llama/Llama-2-7b-hf"
# model: decapoda-research/llama-7b-hf
stats_dir: "./data/stats"
device: 0
# layers: [4, 5, 6, 7, 8]
layers: [21]
rewrite_module_tmp: "model.layers.{}.mlp.down_proj"
layer_module_tmp: "model.layers.{}"
mlp_module_tmp: "model.layers.{}.mlp"
attn_module_tmp: "model.layers.{}.self_attn"
ln_f_module: "model.norm"
lm_head_module: "lm_head"
max_length: 40
batch_size: 32
model_parallel: False
# FT
lr: 5e-4
weight_decay: 0
norm_constraint: false
num_steps: 100

load_ckpt: false
edit: true
compress: false
method: prune # prune or quant
seed: 0
save_ckpt: false
save: out/