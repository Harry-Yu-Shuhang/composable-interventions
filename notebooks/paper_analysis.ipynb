{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the font family to serif\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "# Seaborn settings\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.color_palette(\"pastel\")\n",
    "\n",
    "# plotting constants\n",
    "TITLE_FONT_SIZE = 18\n",
    "LEGEND_FONT_SIZE = 12\n",
    "WSPACE = 0.3\n",
    "FIGURE_HEIGHT = 3\n",
    "LINE_WIDTH = 2\n",
    "FIG_SIZE = 3\n",
    "MARKER_SIZE = 8\n",
    "X_LABEL_ROTATION = 20\n",
    "\n",
    "# Set colors for compositons with compression\n",
    "colors = {\"Wanda\": \"C1\", \"SparseGPT\": \"C2\", \"AWQ\": \"C3\", \"GPTQ\": \"C4\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull and Dedup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_columns = [\n",
    "    # Overall\n",
    "    \"tag\",\n",
    "    # \"seed\",\n",
    "    \"_timestamp\",\n",
    "\n",
    "    # Interventions\n",
    "    \"interventions\", \"edit\", \"unlearn\", \"compression\", \"model_name\",\n",
    "\n",
    "    # Editing\n",
    "    \"edit_set\", \n",
    "    \"edit_dataset\", \"number_of_edits\",\n",
    "\n",
    "    # Unlearning\n",
    "    \"rmu_layer_id\",\n",
    "\n",
    "    # Compression\n",
    "    \"wbits\", \"compression_dataset\", \"sparsity_ratio\",\n",
    "]\n",
    "evaluation_columns = [\n",
    "    \"qa_question_count_limit\",  # An artifical max number of questions to ask during evaluation. Should be none when not debugging.\n",
    "    \"mmlu accuracy\",            # The accuracy of the model on the MMLU dataset. This measures overall model utility. Llama-3 should be ~62%\n",
    "    \"wmdp_bio accuracy\",        # The accuracy of the model on the WMDP bio split. This is the unlearning target. Should be ~25% when RMU is applied.\n",
    "    \"wmdp_cyber accuracy\",      # The accuracy of the model on the WMDP cyber split. This is the unlearning target. Should be ~25% when RMU is applied.\n",
    "    \"PPL\",                      # TODO:\n",
    "    \"PPL edits\",                # Perplexity for the edits. Should be low when editing is applied.\n",
    "    \"PPl QA\",                   # Perplexity for the QA. Should be low when QA is applied.\n",
    "    \"Generalization\",           # TODO: \n",
    "    \"FLOPs\",                    # TODO: \n",
    "    \"Success recall\",           # TODO:\n",
    "    \"Generalization recall\",    # TODO:\n",
    "    \"Locality\",                 # TODO:\n",
    "    \"Average bits\",             # TODO:\n",
    "    \"Rewrite accuracy\",         # TODO:\n",
    "    \"PPl edits unmasked\",       # TODO:\n",
    "    \"Local recall\",             # TODO:\n",
    "    \"Latency\",                  # TODO:\n",
    "]\n",
    "relevant_columns = setting_columns + evaluation_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dri-ice/Composable_Interventions:  47%|████▋     | 2101/4471 [00:13<00:14, 165.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing run evuxnltk: '_timestamp'\n",
      "Error processing run mje2wvj7: '_timestamp'\n",
      "Error processing run um0dxn3y: '_timestamp'\n",
      "Error processing run isna6rgu: '_timestamp'\n",
      "Error processing run luhstpn5: '_timestamp'\n",
      "Error processing run lrh5z3wp: '_timestamp'\n",
      "Error processing run 2do500pc: '_timestamp'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dri-ice/Composable_Interventions:  48%|████▊     | 2151/4471 [00:13<00:13, 172.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing run 71jdht68: '_timestamp'\n",
      "Error processing run 64ed5z4t: '_timestamp'\n",
      "Error processing run 1wj0u6cj: '_timestamp'\n",
      "Error processing run cc3cmdlj: '_timestamp'\n",
      "Error processing run 7t3n8sq1: '_timestamp'\n",
      "Error processing run o1ai36xl: '_timestamp'\n",
      "Error processing run 31j4yjsr: '_timestamp'\n",
      "Error processing run 2nv88i8v: '_timestamp'\n",
      "Error processing run sdhehb2z: '_timestamp'\n",
      "Error processing run r6kpsu09: '_timestamp'\n",
      "Error processing run arid375k: '_timestamp'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dri-ice/Composable_Interventions: 100%|█████████▉| 4451/4471 [00:30<00:00, 131.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing run n0iel6ok: '_timestamp'\n",
      "Error processing run xr5mede5: '_timestamp'\n",
      "Error processing run 27f8pxs0: '_timestamp'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dri-ice/Composable_Interventions: 100%|██████████| 4471/4471 [00:30<00:00, 144.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Composable_Interventions has all the results\n",
    "project_paths = [\n",
    "    \"dri-ice/Composable_Interventions\",\n",
    "    # \"dri-ice/AK_Tests\"\n",
    "]\n",
    "\n",
    "filter_dict = { \"state\": \"finished\" }\n",
    "data_frames = []\n",
    "for project_path in project_paths:\n",
    "    runs = api.runs(project_path, filters=filter_dict)\n",
    "    \n",
    "    # Iterate over eachrun and capture the c        onfig and summary metrics\n",
    "    for run in tqdm(runs, desc=project_path):\n",
    "        try:\n",
    "            run_start_datetime = datetime.fromtimestamp(run.summary_metrics[\"_timestamp\"])\n",
    "            start_cutoff = datetime.strptime(\"2024-05-18 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "            end_cutoff = datetime.strptime(\"2024-06-19 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "            if run_start_datetime < start_cutoff or run_start_datetime > end_cutoff:\n",
    "                continue\n",
    "\n",
    "            skip_tags = [\"test\", \"hparam_search\"]\n",
    "            should_skip = False\n",
    "            for tag in skip_tags:\n",
    "                if tag in run.config[\"tag\"].lower():\n",
    "                    should_skip = True\n",
    "            \n",
    "            if should_skip:\n",
    "                continue\n",
    "\n",
    "            config_frame = pd.DataFrame([run.config])\n",
    "            summary_frame = pd.DataFrame([run.summary_metrics])\n",
    "            combined_frame = pd.concat([config_frame, summary_frame], axis=1)\n",
    "            data_frames.append(combined_frame)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing run {run.id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping memit-gd for edit dataset counterfact\n",
      "Skipping ft-gd for edit dataset counterfact\n",
      "Skipping lora-gd for edit dataset counterfact\n",
      "Skipping memit-gd for edit dataset mquake\n",
      "Skipping ft-gd for edit dataset mquake\n",
      "Skipping lora-gd for edit dataset mquake\n",
      "Skipping gd-lora for edit dataset counterfact\n",
      "Skipping gd-ft for edit dataset counterfact\n",
      "Skipping gd-memit for edit dataset counterfact\n",
      "Skipping gd-memit for edit dataset mquake\n",
      "Skipping gd-ft for edit dataset mquake\n",
      "Skipping gd-lora for edit dataset mquake\n",
      "Skipping ft-ga for edit dataset counterfact\n",
      "Skipping lora-ga for edit dataset counterfact\n",
      "Skipping memit-ga for edit dataset counterfact\n",
      "Skipping ft-ga for edit dataset mquake\n",
      "Skipping memit-ga for edit dataset mquake\n",
      "Skipping lora-ga for edit dataset mquake\n",
      "Skipping ga-memit for edit dataset counterfact\n",
      "Skipping ga-ft for edit dataset counterfact\n",
      "Skipping ga-lora for edit dataset counterfact\n",
      "Skipping ga-memit for edit dataset mquake\n",
      "Skipping ga-lora for edit dataset mquake\n",
      "Skipping ga-ft for edit dataset mquake\n",
      "Skipping lora-ga for edit dataset counterfact\n",
      "Skipping ft-ga for edit dataset mquake\n",
      "Skipping ga-ft for edit dataset mquake\n",
      "Skipping ga-memit for edit dataset mquake\n",
      "Skipping lora-ga for edit dataset mquake\n",
      "Skipping ft-ga for edit dataset counterfact\n",
      "Skipping ga-lora for edit dataset mquake\n",
      "Skipping ga-memit for edit dataset counterfact\n",
      "Skipping memit-ga for edit dataset counterfact\n",
      "Skipping ga-ft for edit dataset counterfact\n",
      "Skipping memit-ga for edit dataset mquake\n",
      "Skipping ga-lora for edit dataset counterfact\n",
      "Skipping gd-lora for edit dataset counterfact\n",
      "Skipping gd-lora for edit dataset mquake\n",
      "Skipping memit-gd for edit dataset counterfact\n",
      "Skipping ft-gd for edit dataset mquake\n",
      "Skipping ft-gd for edit dataset counterfact\n",
      "Skipping lora-gd for edit dataset counterfact\n",
      "Skipping memit-gd for edit dataset mquake\n",
      "Skipping gd-ft for edit dataset mquake\n",
      "Skipping gd-memit for edit dataset counterfact\n",
      "Skipping gd-ft for edit dataset counterfact\n",
      "Skipping gd-memit for edit dataset mquake\n",
      "Skipping lora-gd for edit dataset mquake\n",
      "Skipping memit-ga for edit dataset counterfact\n",
      "Skipping ga-ft for edit dataset counterfact\n",
      "Skipping ga-memit for edit dataset mquake\n",
      "Skipping ft-ga for edit dataset mquake\n",
      "Skipping memit-ga for edit dataset mquake\n",
      "Skipping ga-memit for edit dataset counterfact\n",
      "Skipping ga-ft for edit dataset mquake\n",
      "Skipping ft-ga for edit dataset counterfact\n",
      "Skipping lora-rmu for edit dataset counterfact\n",
      "Skipping memit-rmu for edit dataset mquake\n",
      "Skipping rmu-memit for edit dataset counterfact\n",
      "Skipping rmu-ft for edit dataset counterfact\n",
      "Skipping rmu-memit for edit dataset mquake\n",
      "Skipping rmu-lora for edit dataset counterfact\n",
      "Skipping memit-rmu for edit dataset counterfact\n",
      "Skipping rmu-lora for edit dataset mquake\n",
      "Skipping rmu-ft for edit dataset mquake\n",
      "Skipping ft-rmu for edit dataset counterfact\n",
      "Skipping rmu-memit for edit dataset counterfact\n",
      "Skipping rmu-ft for edit dataset counterfact\n",
      "Skipping lora-rmu for edit dataset counterfact\n",
      "Skipping rmu-lora for edit dataset counterfact\n",
      "Skipping AWQ8bit-to-lora for edit dataset counterfact\n",
      "Skipping AWQ8bit-to-lora for edit dataset mquake\n",
      "Skipping AWQ4bit-to-lora for edit dataset counterfact\n",
      "Skipping AWQ4bit-to-lora for edit dataset mquake\n",
      "Skipping AWQ2bit-to-lora for edit dataset counterfact\n",
      "Skipping AWQ2bit-to-lora for edit dataset mquake\n",
      "Skipping lora-to-AWQ8bit for edit dataset mquake\n",
      "Skipping lora-to-AWQ8bit for edit dataset counterfact\n",
      "Skipping lora-to-AWQ4bit for edit dataset counterfact\n",
      "Skipping lora-to-AWQ4bit for edit dataset mquake\n",
      "Skipping lora-to-AWQ2bit for edit dataset counterfact\n",
      "Skipping lora-to-AWQ2bit for edit dataset mquake\n",
      "Skipping SparseGPT0.65%-to-lora for edit dataset counterfact\n",
      "Skipping SparseGPT0.65%-to-lora for edit dataset mquake\n",
      "Skipping SparseGPT0.45%-to-lora for edit dataset counterfact\n",
      "Skipping SparseGPT0.45%-to-lora for edit dataset mquake\n",
      "Skipping SparseGPT0.25%-to-lora for edit dataset counterfact\n",
      "Skipping SparseGPT0.25%-to-lora for edit dataset mquake\n",
      "Skipping Wanda0.45%-to-lora for edit dataset counterfact\n",
      "Skipping lora-to-SparseGPT0.65% for edit dataset counterfact\n",
      "Skipping Wanda0.45%-to-lora for edit dataset mquake\n",
      "Skipping lora-to-SparseGPT0.45% for edit dataset mquake\n",
      "Skipping Wanda0.65%-to-ft for edit dataset counterfact\n",
      "Skipping Wanda0.45%-to-ft for edit dataset mquake\n",
      "Skipping Wanda0.25%-to-ft for edit dataset counterfact\n",
      "Skipping Wanda0.25%-to-ft for edit dataset mquake\n",
      "Skipping ft-to-SparseGPT0.65% for edit dataset counterfact\n",
      "Skipping ft-to-SparseGPT0.65% for edit dataset mquake\n",
      "Skipping ft-to-SparseGPT0.25% for edit dataset counterfact\n",
      "Skipping ft-to-SparseGPT0.25% for edit dataset mquake\n",
      "Skipping ft-to-Wanda0.65% for edit dataset mquake\n",
      "Skipping ft-to-Wanda0.45% for edit dataset mquake\n",
      "Skipping ft-to-Wanda0.25% for edit dataset mquake\n",
      "Skipping ft_Edit for edit dataset counterfact\n",
      "Skipping AWQ8bit-to-lora for edit dataset counterfact\n",
      "Skipping AWQ4bit-to-lora for edit dataset mquake\n",
      "Skipping lora-to-AWQ2bit for edit dataset mquake\n",
      "Skipping lora-to-Wanda0.65% for edit dataset counterfact\n",
      "Skipping lora-to-Wanda0.65% for edit dataset mquake\n",
      "Skipping lora-to-SparseGPT0.65% for edit dataset mquake\n",
      "Skipping lora-to-SparseGPT0.65% for edit dataset counterfact\n",
      "Skipping lora_Edit for edit dataset counterfact\n",
      "Skipping Wanda0.25%-to-lora for edit dataset mquake\n",
      "Skipping Wanda0.25%-to-lora for edit dataset counterfact\n",
      "Skipping lora-to-Wanda0.25% for edit dataset mquake\n",
      "Skipping lora-to-Wanda0.25% for edit dataset counterfact\n",
      "Skipping lora-to-SparseGPT0.25% for edit dataset counterfact\n",
      "Skipping lora-to-SparseGPT0.25% for edit dataset mquake\n",
      "Skipping lora_Edit for edit dataset mquake\n",
      "Skipping lora-to-SparseGPT0.45% for edit dataset counterfact\n",
      "Skipping lora-to-SparseGPT0.45% for edit dataset mquake\n"
     ]
    }
   ],
   "source": [
    "def should_keep_frame(frame):\n",
    "    if frame[\"edit_dataset\"] == \"zsre\":\n",
    "        return True\n",
    "    \n",
    "    if \"edit\" not in frame[\"interventions\"]:\n",
    "        return True\n",
    "    \n",
    "    print(f\"Skipping {frame['tag']} for edit dataset {frame['edit_dataset']}\")\n",
    "    return False\n",
    "\n",
    "# Sort by \"tag\" and \"_timestamp\" in descending order to have the most recent run first\n",
    "all_runs_df = pd.concat(data_frames, ignore_index=True)[relevant_columns]\n",
    "all_runs_df[\"interventions\"] = all_runs_df[\"interventions\"].astype(str)\n",
    "\n",
    "# Keep only the current edit dataset\n",
    "all_runs_df = all_runs_df[all_runs_df.apply(lambda x: should_keep_frame(x), axis=1)]\n",
    "\n",
    "# WARNING: WHAT DOES EDIT SET 50 MEAN COMPARED TO EDIT SET 1?\n",
    "# all_runs_df = all_runs_df[all_runs_df[\"edit_set\"] == 50]\n",
    "# all_runs_df_sorted = all_runs_df.sort_values(by=[\"tag\", \"_timestamp\"], ascending=[True, False])\n",
    "all_runs_df[\"date\"] = pd.to_datetime(all_runs_df[\"_timestamp\"], unit=\"s\")\n",
    "all_runs_df_sorted = all_runs_df.sort_values(by=[\"_timestamp\"], ascending=[False])\n",
    "all_runs_df_sorted[\"Avg WMDP\"] = (all_runs_df_sorted[\"wmdp_bio accuracy\"] + all_runs_df_sorted[\"wmdp_cyber accuracy\"]) / 2\n",
    "all_runs_df_sorted = all_runs_df_sorted[all_runs_df_sorted[\"qa_question_count_limit\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_m/m_v8tmqs05lf3_0frmhh2tkc0000gn/T/ipykernel_7608/2902710175.py:12: FutureWarning: ['interventions', 'edit', 'unlearn', 'compression', 'model_name', 'edit_dataset', 'compression_dataset', 'PPL', 'PPL edits', 'PPl QA', 'FLOPs', 'PPl edits unmasked'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  grouped_df = latest_runs_df.groupby(\"tag\").agg([\"mean\", standard_error])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>edit_set</th>\n",
       "      <th>number_of_edits</th>\n",
       "      <th>rmu_layer_id</th>\n",
       "      <th>wbits</th>\n",
       "      <th>sparsity_ratio</th>\n",
       "      <th>qa_question_count_limit_x</th>\n",
       "      <th>mmlu accuracy</th>\n",
       "      <th>wmdp_bio accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>model_name</th>\n",
       "      <th>edit_dataset</th>\n",
       "      <th>compression_dataset</th>\n",
       "      <th>qa_question_count_limit_y</th>\n",
       "      <th>PPL</th>\n",
       "      <th>PPL edits</th>\n",
       "      <th>PPl QA</th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>PPl edits unmasked</th>\n",
       "      <th>date_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWQ2bit-to-ft</td>\n",
       "      <td>1.718168e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258743</td>\n",
       "      <td>0.243591</td>\n",
       "      <td>...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>33638.4375</td>\n",
       "      <td>338052.34375</td>\n",
       "      <td>102475.617188</td>\n",
       "      <td>-1</td>\n",
       "      <td>78554.226562</td>\n",
       "      <td>2024-05-20 17:38:54.680141568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AWQ2bit-to-lora</td>\n",
       "      <td>1.718359e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.262028</td>\n",
       "      <td>0.242419</td>\n",
       "      <td>...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>141960.90625</td>\n",
       "      <td>37042.675781</td>\n",
       "      <td>267692.4375</td>\n",
       "      <td>-1</td>\n",
       "      <td>93121.015625</td>\n",
       "      <td>2024-06-14 09:58:09.475097088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AWQ2bit-to-memit</td>\n",
       "      <td>1.718128e+09</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.264049</td>\n",
       "      <td>0.241424</td>\n",
       "      <td>...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>1735678.75</td>\n",
       "      <td>996271.5625</td>\n",
       "      <td>1198751.125</td>\n",
       "      <td>-1</td>\n",
       "      <td>1074956.375</td>\n",
       "      <td>2024-05-20 17:01:28.464071680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AWQ3bit-to-ft</td>\n",
       "      <td>1.718614e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509479</td>\n",
       "      <td>0.594580</td>\n",
       "      <td>...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>7.569585</td>\n",
       "      <td>50514.761719</td>\n",
       "      <td>627.454346</td>\n",
       "      <td>-1</td>\n",
       "      <td>945.212585</td>\n",
       "      <td>2024-06-17 08:37:00.186556416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWQ3bit-to-lora</td>\n",
       "      <td>1.718611e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.510034</td>\n",
       "      <td>0.604242</td>\n",
       "      <td>...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>55.437218</td>\n",
       "      <td>334348.21875</td>\n",
       "      <td>9609.279297</td>\n",
       "      <td>-1</td>\n",
       "      <td>16729.5</td>\n",
       "      <td>2024-06-17 07:55:00.258938112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tag    _timestamp   edit_set  number_of_edits  rmu_layer_id  \\\n",
       "0     AWQ2bit-to-ft  1.718168e+09   9.545455             50.0          -1.0   \n",
       "1   AWQ2bit-to-lora  1.718359e+09   5.500000             50.0          -1.0   \n",
       "2  AWQ2bit-to-memit  1.718128e+09  10.666667             50.0          -1.0   \n",
       "3     AWQ3bit-to-ft  1.718614e+09   5.500000             50.0          -1.0   \n",
       "4   AWQ3bit-to-lora  1.718611e+09   5.500000             50.0          -1.0   \n",
       "\n",
       "   wbits  sparsity_ratio  qa_question_count_limit_x  mmlu accuracy  \\\n",
       "0    2.0             0.0                        NaN       0.258743   \n",
       "1    2.0             0.0                        NaN       0.262028   \n",
       "2    2.0             0.0                        NaN       0.264049   \n",
       "3    3.0             0.0                        NaN       0.509479   \n",
       "4    3.0             0.0                        NaN       0.510034   \n",
       "\n",
       "   wmdp_bio accuracy  ...                  model_name  edit_dataset  \\\n",
       "0           0.243591  ...  meta-llama/Meta-Llama-3-8B          zsre   \n",
       "1           0.242419  ...  meta-llama/Meta-Llama-3-8B          zsre   \n",
       "2           0.241424  ...  meta-llama/Meta-Llama-3-8B          zsre   \n",
       "3           0.594580  ...  meta-llama/Meta-Llama-3-8B          zsre   \n",
       "4           0.604242  ...  meta-llama/Meta-Llama-3-8B          zsre   \n",
       "\n",
       "   compression_dataset  qa_question_count_limit_y           PPL     PPL edits  \\\n",
       "0                   c4                       None    33638.4375  338052.34375   \n",
       "1                   c4                       None  141960.90625  37042.675781   \n",
       "2                   c4                       None    1735678.75   996271.5625   \n",
       "3                   c4                       None      7.569585  50514.761719   \n",
       "4                   c4                       None     55.437218  334348.21875   \n",
       "\n",
       "          PPl QA  FLOPs  PPl edits unmasked                        date_y  \n",
       "0  102475.617188     -1        78554.226562 2024-05-20 17:38:54.680141568  \n",
       "1    267692.4375     -1        93121.015625 2024-06-14 09:58:09.475097088  \n",
       "2    1198751.125     -1         1074956.375 2024-05-20 17:01:28.464071680  \n",
       "3     627.454346     -1          945.212585 2024-06-17 08:37:00.186556416  \n",
       "4    9609.279297     -1             16729.5 2024-06-17 07:55:00.258938112  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by the recency column, for example, \"date\"\n",
    "all_runs_df_sorted = all_runs_df_sorted.sort_values(by=\"date\")\n",
    "\n",
    "# Drop duplicates, keeping only the most recent occurrence for each \"tag\" and \"edit_set\"\n",
    "latest_runs_df = all_runs_df_sorted.drop_duplicates(subset=[\"tag\", \"edit_set\"], keep=\"last\")\n",
    "\n",
    "# Define a function to calculate standard error\n",
    "def standard_error(x):\n",
    "    return x.std() / np.sqrt(len(x))\n",
    "\n",
    "# Group by the \"tag\" column and calculate the mean for numerical columns\n",
    "grouped_df = latest_runs_df.groupby(\"tag\").agg([\"mean\", standard_error])\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "grouped_df.columns = [f\"{col[0]}_{col[1]}\" for col in grouped_df.columns]\n",
    "\n",
    "# Split the columns into means and standard errors\n",
    "mean_columns = [col for col in grouped_df.columns if col.endswith(\"_mean\")]\n",
    "se_columns = [col for col in grouped_df.columns if col.endswith(\"_standard_error\")]\n",
    "\n",
    "# Create separate DataFrames for means and standard errors\n",
    "mean_df = grouped_df[mean_columns].rename(columns=lambda x: x.replace(\"_mean\", \"\"))\n",
    "se_df = grouped_df[se_columns].rename(columns=lambda x: x.replace(\"_standard_error\", \"_se\"))\n",
    "\n",
    "# Merge the means and standard errors back into one DataFrame\n",
    "all_runs_df_sorted_averaged = pd.concat([mean_df, se_df], axis=1).copy()\n",
    "del all_runs_df_sorted\n",
    "\n",
    "# Reset index if needed\n",
    "all_runs_df_sorted_averaged.reset_index(inplace=True)\n",
    "\n",
    "# Add non-numerical columns from the latest_runs_df\n",
    "non_numerical_columns = latest_runs_df.select_dtypes(exclude=[np.number]).drop_duplicates(subset=[\"tag\"])\n",
    "all_runs_df_sorted_averaged = all_runs_df_sorted_averaged.merge(non_numerical_columns, on=\"tag\", how=\"left\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "all_runs_df_sorted_averaged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AWQ3bit-to-memit' 'awq2bit-ga' 'awq2bit-gd' 'awq3bit-ga' 'awq3bit-gd'\n",
      " 'awq4bit-ga' 'awq4bit-gd' 'awq5bit-ga' 'awq5bit-gd' 'awq6bit-ga'\n",
      " 'awq6bit-gd' 'awq8bit-ga' 'awq8bit-gd' 'ga-awq2bit' 'ga-awq3bit'\n",
      " 'ga-awq4bit' 'ga-awq5bit' 'ga-awq6bit' 'ga-awq8bit' 'ga-gptq2bit'\n",
      " 'ga-gptq3bit' 'ga-gptq4bit' 'ga-gptq8bit' 'ga-none' 'ga-sparsegpt0.25\\\\%'\n",
      " 'ga-sparsegpt0.35\\\\%' 'ga-sparsegpt0.45\\\\%' 'ga-sparsegpt0.55\\\\%'\n",
      " 'ga-sparsegpt0.65\\\\%' 'ga-sparsegpt0.75\\\\%' 'ga-wanda0.25\\\\%'\n",
      " 'ga-wanda0.35\\\\%' 'ga-wanda0.45\\\\%' 'ga-wanda0.55\\\\%' 'ga-wanda0.65\\\\%'\n",
      " 'ga-wanda0.75\\\\%' 'gd-awq2bit' 'gd-awq3bit' 'gd-awq4bit' 'gd-awq5bit'\n",
      " 'gd-awq6bit' 'gd-awq8bit' 'gd-gptq2bit' 'gd-gptq3bit' 'gd-gptq4bit'\n",
      " 'gd-gptq8bit' 'gd-none' 'gd-sparsegpt0.25\\\\%' 'gd-sparsegpt0.35\\\\%'\n",
      " 'gd-sparsegpt0.45\\\\%' 'gd-sparsegpt0.55\\\\%' 'gd-sparsegpt0.65\\\\%'\n",
      " 'gd-sparsegpt0.75\\\\%' 'gd-wanda0.25\\\\%' 'gd-wanda0.35\\\\%'\n",
      " 'gd-wanda0.45\\\\%' 'gd-wanda0.55\\\\%' 'gd-wanda0.65\\\\%' 'gd-wanda0.75\\\\%'\n",
      " 'gptq2bit-ga' 'gptq2bit-gd' 'gptq3bit-ga' 'gptq3bit-gd' 'gptq4bit-ga'\n",
      " 'gptq4bit-gd' 'gptq8bit-ga' 'gptq8bit-gd' 'memit-to-Wanda0.25%'\n",
      " 'sparsegpt0.25\\\\%-ga' 'sparsegpt0.25\\\\%-gd' 'sparsegpt0.35\\\\%-ga'\n",
      " 'sparsegpt0.35\\\\%-gd' 'sparsegpt0.35\\\\%-rmu' 'sparsegpt0.45\\\\%-ga'\n",
      " 'sparsegpt0.45\\\\%-gd' 'sparsegpt0.55\\\\%-ga' 'sparsegpt0.55\\\\%-gd'\n",
      " 'sparsegpt0.65\\\\%-ga' 'sparsegpt0.65\\\\%-gd' 'sparsegpt0.75\\\\%-ga'\n",
      " 'sparsegpt0.75\\\\%-gd' 'wanda0.25\\\\%-ga' 'wanda0.25\\\\%-gd'\n",
      " 'wanda0.35\\\\%-ga' 'wanda0.35\\\\%-gd' 'wanda0.45\\\\%-ga' 'wanda0.45\\\\%-gd'\n",
      " 'wanda0.55\\\\%-ga' 'wanda0.55\\\\%-gd' 'wanda0.65\\\\%-ga' 'wanda0.65\\\\%-gd'\n",
      " 'wanda0.75\\\\%-ga' 'wanda0.75\\\\%-gd']\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for rows where \"edit_set\" is either 1 or 50\n",
    "filtered_df = all_runs_df_sorted_averaged[all_runs_df_sorted_averaged[\"edit_set\"].isin([1, 50])]\n",
    "\n",
    "# Print the unique \"tag\" values\n",
    "unique_tags = filtered_df[\"tag\"].unique()\n",
    "\n",
    "# print([tag for tag in unique_tags if \"rmu\" in tag])\n",
    "print(unique_tags)\n",
    "# print(all_runs_df_sorted_averaged.shape)\n",
    "# print(all_runs_df_sorted.columns)\n",
    "# print(all_runs_df_sorted_averaged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "AWQ2bit-to-ft             1\n",
       "lora-to-AWQ8bit           1\n",
       "lora-to-SparseGPT0.45%    1\n",
       "lora-to-SparseGPT0.35%    1\n",
       "lora-to-SparseGPT0.25%    1\n",
       "                         ..\n",
       "awq6bit-rmu               1\n",
       "awq6bit-gd                1\n",
       "awq6bit-ga                1\n",
       "awq5bit-rmu               1\n",
       "wanda0.75\\%-rmu           1\n",
       "Length: 311, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of experiments: 311\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates, keeping only the first occurrence (which is the most recent due to sorting)\n",
    "# all_runs_df_deduplicated = all_runs_df_sorted.drop_duplicates(subset=[col for col in setting_columns if col not in [\"_timestamp\", \"tag\", \"date\"]], keep=\"first\")\n",
    "all_runs_df_deduplicated = all_runs_df_sorted_averaged.drop_duplicates(subset=\"tag\", keep=\"first\")\n",
    "all_runs_df_deduplicated[\"interventions\"] = all_runs_df_deduplicated[\"interventions\"].apply(lambda x : ast.literal_eval(x))\n",
    "\n",
    "rename_dict = {\n",
    "    \"meta-llama/Meta-Llama-3-8B\" : \"Llama-3 (8b)\",\n",
    "    \"ft\" : \"Fine-tune\",\n",
    "    \"memit\" : \"MEMIT\",\n",
    "    \"lora\" : \"LoRA\",\n",
    "    \"wanda\" : \"Wanda\",\n",
    "    \"sparsegpt\" : \"SparseGPT\",\n",
    "    \"gptq\" : \"GPTQ\",\n",
    "    \"awq\" : \"AWQ\",\n",
    "    \"rmu\" : \"RMU\",\n",
    "    \"ga\": \"GA\",\n",
    "    \"gd\": \"GD\",\n",
    "}\n",
    "metrics = all_runs_df_deduplicated\n",
    "metrics[\"model_name\"] = metrics[\"model_name\"].apply(lambda x : rename_dict.get(x, None))\n",
    "metrics[\"edit\"] = metrics[\"edit\"].apply(lambda x : rename_dict.get(x, None))\n",
    "metrics[\"compression\"] = metrics[\"compression\"].apply(lambda x : rename_dict.get(x, None))\n",
    "metrics[\"unlearn\"] = metrics[\"unlearn\"].apply(lambda x : rename_dict.get(x, None))\n",
    "all_runs_df_deduplicated = metrics\n",
    "display(all_runs_df_deduplicated.value_counts(\"tag\"))\n",
    "print(f\"Number of experiments: {len(all_runs_df_deduplicated)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# TODO: Get a second pair of eyes on this this math\n",
    "\n",
    "# Math for determining number of interventions\n",
    "awq_settings = 6\n",
    "gptq_settings = 4 # only support quantize to [2, 3, 4, 8] bits.\n",
    "wanda_count = 6\n",
    "sparsegpt_count = 6\n",
    "editor_settings = 3\n",
    "composition_factor = 2\n",
    "\n",
    "editor_count = composition_factor * (awq_settings + gptq_settings + wanda_count + sparsegpt_count + 1) * editor_settings\n",
    "print(editor_count // 2)\n",
    "\n",
    "rmu_count = composition_factor * (awq_settings + gptq_settings + wanda_count + sparsegpt_count + editor_settings)\n",
    "print(rmu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GA     51\n",
       "GD     51\n",
       "RMU    51\n",
       "Name: unlearn, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_runs_df_deduplicated[\"unlearn\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_UNLEARNING = 3\n",
    "NUM_EDITING = 3\n",
    "NUM_COMPRESSION = 4 + 6 + 6 + 6\n",
    "combination_of_unlearning = 2 * NUM_UNLEARNING * NUM_COMPRESSION\n",
    "combination_of_unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>edit_set</th>\n",
       "      <th>number_of_edits</th>\n",
       "      <th>rmu_layer_id</th>\n",
       "      <th>wbits</th>\n",
       "      <th>sparsity_ratio</th>\n",
       "      <th>qa_question_count_limit_x</th>\n",
       "      <th>mmlu accuracy</th>\n",
       "      <th>wmdp_bio accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>model_name</th>\n",
       "      <th>edit_dataset</th>\n",
       "      <th>compression_dataset</th>\n",
       "      <th>qa_question_count_limit_y</th>\n",
       "      <th>PPL</th>\n",
       "      <th>PPL edits</th>\n",
       "      <th>PPl QA</th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>PPl edits unmasked</th>\n",
       "      <th>date_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>awq2bit-ga</td>\n",
       "      <td>1.718194e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>0.240377</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>-1</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 12:02:27.059515904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>awq2bit-gd</td>\n",
       "      <td>1.718204e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>0.240377</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>4496028624899119061335958001549312.0</td>\n",
       "      <td>2617349.75</td>\n",
       "      <td>6866961.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>5905774.5</td>\n",
       "      <td>2024-06-12 14:48:16.610423040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>awq2bit-rmu</td>\n",
       "      <td>1.717570e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>0.240377</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>1749321.75</td>\n",
       "      <td>1055937.75</td>\n",
       "      <td>999726.5</td>\n",
       "      <td>-1</td>\n",
       "      <td>915356.5</td>\n",
       "      <td>2024-05-20 19:19:14.861625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>awq3bit-ga</td>\n",
       "      <td>1.718194e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.272896</td>\n",
       "      <td>0.241948</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>-1</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 12:03:07.964953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>awq3bit-gd</td>\n",
       "      <td>1.718204e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.341832</td>\n",
       "      <td>0.245876</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>166895.90625</td>\n",
       "      <td>83131.453125</td>\n",
       "      <td>20670.785156</td>\n",
       "      <td>-1</td>\n",
       "      <td>3257.42627</td>\n",
       "      <td>2024-06-12 14:47:07.710373120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>wanda0.65\\%-gd</td>\n",
       "      <td>1.718204e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229098</td>\n",
       "      <td>0.249018</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>16268.005859</td>\n",
       "      <td>5423145.5</td>\n",
       "      <td>81961160.0</td>\n",
       "      <td>447.25 GFLOPS</td>\n",
       "      <td>89329664.0</td>\n",
       "      <td>2024-06-12 14:53:38.419730176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>wanda0.65\\%-rmu</td>\n",
       "      <td>1.716764e+09</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229360</td>\n",
       "      <td>0.249542</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>45.534767</td>\n",
       "      <td>83373.453125</td>\n",
       "      <td>1477.523804</td>\n",
       "      <td>760 GFLOPS</td>\n",
       "      <td>1549.510742</td>\n",
       "      <td>2024-05-20 19:27:06.522696448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>wanda0.75\\%-ga</td>\n",
       "      <td>1.718194e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229811</td>\n",
       "      <td>0.249018</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>8125971838094910145582358934348365824.0</td>\n",
       "      <td>176520989218511062933962752.0</td>\n",
       "      <td>31936578699555629957120.0</td>\n",
       "      <td>357.84 GFLOPS</td>\n",
       "      <td>31529243875858664390656.0</td>\n",
       "      <td>2024-06-12 12:09:17.554701056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>wanda0.75\\%-gd</td>\n",
       "      <td>1.718204e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.246902</td>\n",
       "      <td>0.247447</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>10226426.0</td>\n",
       "      <td>743558.8125</td>\n",
       "      <td>145133.09375</td>\n",
       "      <td>357.84 GFLOPS</td>\n",
       "      <td>128967.648438</td>\n",
       "      <td>2024-06-12 14:55:21.376342784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>wanda0.75\\%-rmu</td>\n",
       "      <td>1.717030e+09</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.231769</td>\n",
       "      <td>0.250982</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>391.873352</td>\n",
       "      <td>151281.125</td>\n",
       "      <td>3917.281982</td>\n",
       "      <td>581.18 GFLOPS</td>\n",
       "      <td>3727.288574</td>\n",
       "      <td>2024-05-22 05:36:22.919470336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tag    _timestamp   edit_set  number_of_edits  rmu_layer_id  \\\n",
       "89        awq2bit-ga  1.718194e+09   1.000000             50.0     -1.000000   \n",
       "90        awq2bit-gd  1.718204e+09   1.000000             50.0     -1.000000   \n",
       "91       awq2bit-rmu  1.717570e+09   9.545455             50.0      3.181818   \n",
       "92        awq3bit-ga  1.718194e+09   1.000000             50.0     -1.000000   \n",
       "93        awq3bit-gd  1.718204e+09   1.000000             50.0     -1.000000   \n",
       "..               ...           ...        ...              ...           ...   \n",
       "306   wanda0.65\\%-gd  1.718204e+09   1.000000             50.0     -1.000000   \n",
       "307  wanda0.65\\%-rmu  1.716764e+09  18.666667             50.0      3.666667   \n",
       "308   wanda0.75\\%-ga  1.718194e+09   1.000000             50.0     -1.000000   \n",
       "309   wanda0.75\\%-gd  1.718204e+09   1.000000             50.0     -1.000000   \n",
       "310  wanda0.75\\%-rmu  1.717030e+09   4.500000             50.0      3.000000   \n",
       "\n",
       "     wbits  sparsity_ratio  qa_question_count_limit_x  mmlu accuracy  \\\n",
       "89     2.0            0.00                        NaN       0.268908   \n",
       "90     2.0            0.00                        NaN       0.268908   \n",
       "91     2.0            0.00                        NaN       0.268908   \n",
       "92     3.0            0.00                        NaN       0.272896   \n",
       "93     3.0            0.00                        NaN       0.341832   \n",
       "..     ...             ...                        ...            ...   \n",
       "306    4.0            0.65                        NaN       0.229098   \n",
       "307    4.0            0.65                        NaN       0.229360   \n",
       "308    4.0            0.75                        NaN       0.229811   \n",
       "309    4.0            0.75                        NaN       0.246902   \n",
       "310    4.0            0.75                        NaN       0.231769   \n",
       "\n",
       "     wmdp_bio accuracy  ...    model_name  edit_dataset  compression_dataset  \\\n",
       "89            0.240377  ...  Llama-3 (8b)          zsre                   c4   \n",
       "90            0.240377  ...  Llama-3 (8b)          zsre                   c4   \n",
       "91            0.240377  ...  Llama-3 (8b)          zsre                   c4   \n",
       "92            0.241948  ...  Llama-3 (8b)          zsre                   c4   \n",
       "93            0.245876  ...  Llama-3 (8b)          zsre                   c4   \n",
       "..                 ...  ...           ...           ...                  ...   \n",
       "306           0.249018  ...  Llama-3 (8b)          zsre                   c4   \n",
       "307           0.249542  ...  Llama-3 (8b)          zsre                   c4   \n",
       "308           0.249018  ...  Llama-3 (8b)          zsre                   c4   \n",
       "309           0.247447  ...  Llama-3 (8b)          zsre                   c4   \n",
       "310           0.250982  ...  Llama-3 (8b)          zsre                   c4   \n",
       "\n",
       "     qa_question_count_limit_y                                      PPL  \\\n",
       "89                        None                                 Infinity   \n",
       "90                        None     4496028624899119061335958001549312.0   \n",
       "91                        None                               1749321.75   \n",
       "92                        None                                 Infinity   \n",
       "93                        None                             166895.90625   \n",
       "..                         ...                                      ...   \n",
       "306                       None                             16268.005859   \n",
       "307                       None                                45.534767   \n",
       "308                       None  8125971838094910145582358934348365824.0   \n",
       "309                       None                               10226426.0   \n",
       "310                       None                               391.873352   \n",
       "\n",
       "                         PPL edits                     PPl QA          FLOPs  \\\n",
       "89                        Infinity                   Infinity             -1   \n",
       "90                      2617349.75                  6866961.5             -1   \n",
       "91                      1055937.75                   999726.5             -1   \n",
       "92                        Infinity                   Infinity             -1   \n",
       "93                    83131.453125               20670.785156             -1   \n",
       "..                             ...                        ...            ...   \n",
       "306                      5423145.5                 81961160.0  447.25 GFLOPS   \n",
       "307                   83373.453125                1477.523804     760 GFLOPS   \n",
       "308  176520989218511062933962752.0  31936578699555629957120.0  357.84 GFLOPS   \n",
       "309                    743558.8125               145133.09375  357.84 GFLOPS   \n",
       "310                     151281.125                3917.281982  581.18 GFLOPS   \n",
       "\n",
       "            PPl edits unmasked                        date_y  \n",
       "89                    Infinity 2024-06-12 12:02:27.059515904  \n",
       "90                   5905774.5 2024-06-12 14:48:16.610423040  \n",
       "91                    915356.5 2024-05-20 19:19:14.861625600  \n",
       "92                    Infinity 2024-06-12 12:03:07.964953600  \n",
       "93                  3257.42627 2024-06-12 14:47:07.710373120  \n",
       "..                         ...                           ...  \n",
       "306                 89329664.0 2024-06-12 14:53:38.419730176  \n",
       "307                1549.510742 2024-05-20 19:27:06.522696448  \n",
       "308  31529243875858664390656.0 2024-06-12 12:09:17.554701056  \n",
       "309              128967.648438 2024-06-12 14:55:21.376342784  \n",
       "310                3727.288574 2024-05-22 05:36:22.919470336  \n",
       "\n",
       "[66 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>edit_set</th>\n",
       "      <th>number_of_edits</th>\n",
       "      <th>rmu_layer_id</th>\n",
       "      <th>wbits</th>\n",
       "      <th>sparsity_ratio</th>\n",
       "      <th>qa_question_count_limit_x</th>\n",
       "      <th>mmlu accuracy</th>\n",
       "      <th>wmdp_bio accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>model_name</th>\n",
       "      <th>edit_dataset</th>\n",
       "      <th>compression_dataset</th>\n",
       "      <th>qa_question_count_limit_y</th>\n",
       "      <th>PPL</th>\n",
       "      <th>PPL edits</th>\n",
       "      <th>PPl QA</th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>PPl edits unmasked</th>\n",
       "      <th>date_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>ga-awq2bit</td>\n",
       "      <td>1.718191e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.246546</td>\n",
       "      <td>0.247447</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>1769133.875</td>\n",
       "      <td>1243236.75</td>\n",
       "      <td>1118986.25</td>\n",
       "      <td>-1</td>\n",
       "      <td>1245522.5</td>\n",
       "      <td>2024-06-12 11:13:37.028556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>ga-awq3bit</td>\n",
       "      <td>1.718191e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.441675</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>-1</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 11:17:15.184798720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>ga-awq4bit</td>\n",
       "      <td>1.718191e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>0.524745</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>-1</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 11:20:13.570003456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>ga-awq5bit</td>\n",
       "      <td>1.718191e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.498576</td>\n",
       "      <td>0.556952</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>-1</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 11:20:19.390620672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>ga-awq6bit</td>\n",
       "      <td>1.718191e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.489389</td>\n",
       "      <td>0.559309</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>-1</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 11:21:07.118664448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>rmu-wanda0.35\\%</td>\n",
       "      <td>1.717690e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.532602</td>\n",
       "      <td>0.258759</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>6.360727</td>\n",
       "      <td>58684.835938</td>\n",
       "      <td>459.011627</td>\n",
       "      <td>1.3 TFLOPS</td>\n",
       "      <td>647.534851</td>\n",
       "      <td>2024-06-06 13:50:28.648183040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>rmu-wanda0.45\\%</td>\n",
       "      <td>1.717546e+09</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.478009</td>\n",
       "      <td>0.272270</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>7.707963</td>\n",
       "      <td>25395.089844</td>\n",
       "      <td>538.735168</td>\n",
       "      <td>1.12 TFLOPS</td>\n",
       "      <td>620.10907</td>\n",
       "      <td>2024-05-20 19:02:13.503230464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>rmu-wanda0.55\\%</td>\n",
       "      <td>1.717684e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.350570</td>\n",
       "      <td>0.259309</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>13.729619</td>\n",
       "      <td>46809.050781</td>\n",
       "      <td>728.273926</td>\n",
       "      <td>938.82 GFLOPS</td>\n",
       "      <td>732.329468</td>\n",
       "      <td>2024-06-06 14:15:58.615832064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>rmu-wanda0.65\\%</td>\n",
       "      <td>1.717552e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>0.242020</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>52.669495</td>\n",
       "      <td>78055.304688</td>\n",
       "      <td>1446.553223</td>\n",
       "      <td>760 GFLOPS</td>\n",
       "      <td>1535.829468</td>\n",
       "      <td>2024-05-20 19:06:01.385294080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>rmu-wanda0.75\\%</td>\n",
       "      <td>1.717685e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.231342</td>\n",
       "      <td>0.243205</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>334.445282</td>\n",
       "      <td>195344.125</td>\n",
       "      <td>4011.156738</td>\n",
       "      <td>581.18 GFLOPS</td>\n",
       "      <td>4462.537598</td>\n",
       "      <td>2024-06-06 14:33:10.895355136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tag    _timestamp   edit_set  number_of_edits  rmu_layer_id  \\\n",
       "133       ga-awq2bit  1.718191e+09   1.000000             50.0     -1.000000   \n",
       "134       ga-awq3bit  1.718191e+09   1.000000             50.0     -1.000000   \n",
       "135       ga-awq4bit  1.718191e+09   1.000000             50.0     -1.000000   \n",
       "136       ga-awq5bit  1.718191e+09   1.000000             50.0     -1.000000   \n",
       "137       ga-awq6bit  1.718191e+09   1.000000             50.0     -1.000000   \n",
       "..               ...           ...        ...              ...           ...   \n",
       "270  rmu-wanda0.35\\%  1.717690e+09   5.500000             50.0      3.000000   \n",
       "271  rmu-wanda0.45\\%  1.717546e+09  10.000000             50.0      3.200000   \n",
       "272  rmu-wanda0.55\\%  1.717684e+09   5.500000             50.0      3.000000   \n",
       "273  rmu-wanda0.65\\%  1.717552e+09   9.545455             50.0      3.181818   \n",
       "274  rmu-wanda0.75\\%  1.717685e+09   5.500000             50.0      3.000000   \n",
       "\n",
       "     wbits  sparsity_ratio  qa_question_count_limit_x  mmlu accuracy  \\\n",
       "133    2.0            0.00                        NaN       0.246546   \n",
       "134    3.0            0.00                        NaN       0.441675   \n",
       "135    4.0            0.00                        NaN       0.465318   \n",
       "136    5.0            0.00                        NaN       0.498576   \n",
       "137    6.0            0.00                        NaN       0.489389   \n",
       "..     ...             ...                        ...            ...   \n",
       "270    4.0            0.35                        NaN       0.532602   \n",
       "271    4.0            0.45                        NaN       0.478009   \n",
       "272    4.0            0.55                        NaN       0.350570   \n",
       "273    4.0            0.65                        NaN       0.229500   \n",
       "274    4.0            0.75                        NaN       0.231342   \n",
       "\n",
       "     wmdp_bio accuracy  ...    model_name  edit_dataset  compression_dataset  \\\n",
       "133           0.247447  ...  Llama-3 (8b)          zsre                   c4   \n",
       "134           0.492537  ...  Llama-3 (8b)          zsre                   c4   \n",
       "135           0.524745  ...  Llama-3 (8b)          zsre                   c4   \n",
       "136           0.556952  ...  Llama-3 (8b)          zsre                   c4   \n",
       "137           0.559309  ...  Llama-3 (8b)          zsre                   c4   \n",
       "..                 ...  ...           ...           ...                  ...   \n",
       "270           0.258759  ...  Llama-3 (8b)          zsre                   c4   \n",
       "271           0.272270  ...  Llama-3 (8b)          zsre                   c4   \n",
       "272           0.259309  ...  Llama-3 (8b)          zsre                   c4   \n",
       "273           0.242020  ...  Llama-3 (8b)          zsre                   c4   \n",
       "274           0.243205  ...  Llama-3 (8b)          zsre                   c4   \n",
       "\n",
       "     qa_question_count_limit_y          PPL     PPL edits       PPl QA  \\\n",
       "133                       None  1769133.875    1243236.75   1118986.25   \n",
       "134                       None     Infinity      Infinity     Infinity   \n",
       "135                       None     Infinity      Infinity     Infinity   \n",
       "136                       None     Infinity      Infinity     Infinity   \n",
       "137                       None     Infinity      Infinity     Infinity   \n",
       "..                         ...          ...           ...          ...   \n",
       "270                       None     6.360727  58684.835938   459.011627   \n",
       "271                       None     7.707963  25395.089844   538.735168   \n",
       "272                       None    13.729619  46809.050781   728.273926   \n",
       "273                       None    52.669495  78055.304688  1446.553223   \n",
       "274                       None   334.445282    195344.125  4011.156738   \n",
       "\n",
       "             FLOPs  PPl edits unmasked                        date_y  \n",
       "133             -1           1245522.5 2024-06-12 11:13:37.028556800  \n",
       "134             -1            Infinity 2024-06-12 11:17:15.184798720  \n",
       "135             -1            Infinity 2024-06-12 11:20:13.570003456  \n",
       "136             -1            Infinity 2024-06-12 11:20:19.390620672  \n",
       "137             -1            Infinity 2024-06-12 11:21:07.118664448  \n",
       "..             ...                 ...                           ...  \n",
       "270     1.3 TFLOPS          647.534851 2024-06-06 13:50:28.648183040  \n",
       "271    1.12 TFLOPS           620.10907 2024-05-20 19:02:13.503230464  \n",
       "272  938.82 GFLOPS          732.329468 2024-06-06 14:15:58.615832064  \n",
       "273     760 GFLOPS         1535.829468 2024-05-20 19:06:01.385294080  \n",
       "274  581.18 GFLOPS         4462.537598 2024-06-06 14:33:10.895355136  \n",
       "\n",
       "[66 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data = all_runs_df_deduplicated\n",
    "\n",
    "categories = {\n",
    "    \"No Intervention\": data[data[\"interventions\"].apply(lambda x: x == [])].copy(),\n",
    "    \"Editing\": data[data[\"interventions\"].apply(lambda x: x == [\"edit\"])].copy(),\n",
    "    \"Compression\": data[data[\"interventions\"].apply(lambda x: x == [\"compress\"])].copy(),\n",
    "    \"Edit to Compression\": data[data[\"interventions\"].apply(lambda x: x == [\"edit\", \"compress\"])].copy(),\n",
    "    \"Compression to Edit\": data[data[\"interventions\"].apply(lambda x: x == [\"compress\", \"edit\"])].copy(),\n",
    "    \"Unlearn\": data[data[\"interventions\"].apply(lambda x: x == [\"unlearn\"])].copy(),\n",
    "    \"Edit to Unlearn\": data[data[\"interventions\"].apply(lambda x: x == [\"edit\", \"unlearn\"])].copy(),\n",
    "    \"Unlearn to Edit\": data[data[\"interventions\"].apply(lambda x: x == [\"unlearn\", \"edit\"])].copy(),\n",
    "    \"Compress to Unlearn\": data[data[\"interventions\"].apply(lambda x: x == [\"compress\", \"unlearn\"])].copy(),\n",
    "    \"Unlearn to Compress\": data[data[\"interventions\"].apply(lambda x: x == [\"unlearn\", \"compress\"])].copy()\n",
    "}\n",
    "\n",
    "assert len(categories[\"No Intervention\"]) == 1, f\"{len(categories['No Intervention'])} != 1\"\n",
    "assert len(categories[\"Editing\"]) == 3, f\"{len(categories['Editing'])} != 3\"\n",
    "\n",
    "# display(categories[\"Compression\"])\n",
    "assert len(categories[\"Compression\"]) == (awq_settings + gptq_settings + wanda_count + sparsegpt_count), f\"{len(categories['Compression'])} != {awq_settings + gptq_settings + wanda_count + sparsegpt_count}\"\n",
    "\n",
    "# assert len(categories[\"Edit to Compression\"]) == editor_count // 2, f\"{len(categories['Edit to Compression'])} != {editor_count // 2}\"\n",
    "\n",
    "assert len(categories[\"Compression to Edit\"]) == (editor_count // 2 ) - 3, f\"{len(categories['Compression to Edit'])} != {editor_count // 2}\" # TODO: Fix this by getting the latest results\n",
    "assert len(categories[\"Unlearn\"]) == 3, f\"{len(categories['Unlearn'])} != 3\"\n",
    "assert len(categories[\"Edit to Unlearn\"]) == 9, f\"{len(categories['Edit to Unlearn'])} != 9\"\n",
    "assert len(categories[\"Unlearn to Edit\"]) == 9, f\"{len(categories['Unlearn to Edit'])} != 9\"\n",
    "\n",
    "display(categories[\"Compress to Unlearn\"])\n",
    "assert len(categories[\"Compress to Unlearn\"]) == combination_of_unlearning // 2, f\"{len(categories['Compress to Unlearn'])} != {combination_of_unlearning // 2}\"\n",
    "\n",
    "display(categories[\"Unlearn to Compress\"])\n",
    "assert len(categories[\"Unlearn to Compress\"]) == combination_of_unlearning // 2, f\"{len(categories['Unlearn to Compress'])} != {rmucombination_of_unlearning_count // 2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.0\n",
      "0.25\n",
      "0.5\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# def calculate_wmdp_unlearn_error(accuracy, random_acc=0.25):\n",
    "#     return 1 - ((accuracy - random_acc) / (1 - random_acc)) if accuracy > random_acc else 1\n",
    "\n",
    "def calculate_wmdp_unlearn_error(accuracy, random_acc=0.25):\n",
    "    \"\"\"0.25 is the random accuracy. The closer the accuracy is to 0.25, the better the unlearning.\"\"\"\n",
    "    return abs(accuracy - random_acc)\n",
    "\n",
    "# Tests\n",
    "print(calculate_wmdp_unlearn_error(0.0))\n",
    "print(calculate_wmdp_unlearn_error(0.25))\n",
    "print(calculate_wmdp_unlearn_error(0.5))\n",
    "print(calculate_wmdp_unlearn_error(0.75))\n",
    "print(calculate_wmdp_unlearn_error(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU OI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine-tune</th>\n",
       "      <th>LoRA</th>\n",
       "      <th>MEMIT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GD</th>\n",
       "      <th>RMU</th>\n",
       "      <th>AWQ</th>\n",
       "      <th>GPTQ</th>\n",
       "      <th>SparseGPT</th>\n",
       "      <th>Wanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fine-tune</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.1820</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoRA</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.1414</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.0234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GD</th>\n",
       "      <td>0.1820</td>\n",
       "      <td>0.1414</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMU</th>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWQ</th>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTQ</th>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.2398</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SparseGPT</th>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wanda</th>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fine-tune    LoRA   MEMIT      GA      GD     RMU     AWQ    GPTQ  \\\n",
       "Fine-tune        NaN     NaN     NaN  0.0071  0.1820  0.0122  0.0167  0.0032   \n",
       "LoRA             NaN     NaN     NaN  0.0706  0.1414  0.0390  0.0023  0.0120   \n",
       "MEMIT            NaN     NaN     NaN  0.0409  0.2225  0.0025  0.0084  0.0139   \n",
       "GA            0.0071  0.0706  0.0409     NaN     NaN     NaN  0.0174  0.0921   \n",
       "GD            0.1820  0.1414  0.2225     NaN     NaN     NaN  0.0700  0.2398   \n",
       "RMU           0.0122  0.0390  0.0025     NaN     NaN     NaN  0.0085  0.0500   \n",
       "AWQ           0.0167  0.0023  0.0084  0.0174  0.0700  0.0085     NaN     NaN   \n",
       "GPTQ          0.0032  0.0120  0.0139  0.0921  0.2398  0.0500     NaN     NaN   \n",
       "SparseGPT     0.0037  0.0001  0.0048  0.0580  0.0927  0.0080     NaN     NaN   \n",
       "Wanda         0.0043  0.0009  0.0033  0.0234  0.0100  0.0148     NaN     NaN   \n",
       "\n",
       "           SparseGPT   Wanda  \n",
       "Fine-tune     0.0037  0.0043  \n",
       "LoRA          0.0001  0.0009  \n",
       "MEMIT         0.0048  0.0033  \n",
       "GA            0.0580  0.0234  \n",
       "GD            0.0927  0.0100  \n",
       "RMU           0.0080  0.0148  \n",
       "AWQ              NaN     NaN  \n",
       "GPTQ             NaN     NaN  \n",
       "SparseGPT        NaN     NaN  \n",
       "Wanda            NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU MCE Values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine-tune</th>\n",
       "      <th>LoRA</th>\n",
       "      <th>MEMIT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GD</th>\n",
       "      <th>RMU</th>\n",
       "      <th>AWQ</th>\n",
       "      <th>GPTQ</th>\n",
       "      <th>SparseGPT</th>\n",
       "      <th>Wanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fine-tune</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4745</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.4305</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.4138</td>\n",
       "      <td>0.3978</td>\n",
       "      <td>0.3979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoRA</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6435</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>0.4443</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.4189</td>\n",
       "      <td>0.4039</td>\n",
       "      <td>0.4036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5074</td>\n",
       "      <td>0.4207</td>\n",
       "      <td>0.4381</td>\n",
       "      <td>0.4142</td>\n",
       "      <td>0.4109</td>\n",
       "      <td>0.4037</td>\n",
       "      <td>0.4078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>0.4745</td>\n",
       "      <td>0.6435</td>\n",
       "      <td>0.5074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.4917</td>\n",
       "      <td>0.4591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GD</th>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>0.4207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5803</td>\n",
       "      <td>0.4358</td>\n",
       "      <td>0.4227</td>\n",
       "      <td>0.4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMU</th>\n",
       "      <td>0.4305</td>\n",
       "      <td>0.4443</td>\n",
       "      <td>0.4381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4383</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>0.4341</td>\n",
       "      <td>0.4288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWQ</th>\n",
       "      <td>0.4100</td>\n",
       "      <td>0.4144</td>\n",
       "      <td>0.4142</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.5803</td>\n",
       "      <td>0.4383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTQ</th>\n",
       "      <td>0.4138</td>\n",
       "      <td>0.4189</td>\n",
       "      <td>0.4109</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>0.4358</td>\n",
       "      <td>0.4205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SparseGPT</th>\n",
       "      <td>0.3978</td>\n",
       "      <td>0.4039</td>\n",
       "      <td>0.4037</td>\n",
       "      <td>0.4917</td>\n",
       "      <td>0.4227</td>\n",
       "      <td>0.4341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wanda</th>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.4036</td>\n",
       "      <td>0.4078</td>\n",
       "      <td>0.4591</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.4288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fine-tune    LoRA   MEMIT      GA      GD     RMU     AWQ    GPTQ  \\\n",
       "Fine-tune        NaN     NaN     NaN  0.4745  0.4100  0.4305  0.4100  0.4138   \n",
       "LoRA             NaN     NaN     NaN  0.6435  0.4130  0.4443  0.4144  0.4189   \n",
       "MEMIT            NaN     NaN     NaN  0.5074  0.4207  0.4381  0.4142  0.4109   \n",
       "GA            0.4745  0.6435  0.5074     NaN     NaN     NaN  0.5347  0.5246   \n",
       "GD            0.4100  0.4130  0.4207     NaN     NaN     NaN  0.5803  0.4358   \n",
       "RMU           0.4305  0.4443  0.4381     NaN     NaN     NaN  0.4383  0.4205   \n",
       "AWQ           0.4100  0.4144  0.4142  0.5347  0.5803  0.4383     NaN     NaN   \n",
       "GPTQ          0.4138  0.4189  0.4109  0.5246  0.4358  0.4205     NaN     NaN   \n",
       "SparseGPT     0.3978  0.4039  0.4037  0.4917  0.4227  0.4341     NaN     NaN   \n",
       "Wanda         0.3979  0.4036  0.4078  0.4591  0.4200  0.4288     NaN     NaN   \n",
       "\n",
       "           SparseGPT   Wanda  \n",
       "Fine-tune     0.3978  0.3979  \n",
       "LoRA          0.4039  0.4036  \n",
       "MEMIT         0.4037  0.4078  \n",
       "GA            0.4917  0.4591  \n",
       "GD            0.4227  0.4200  \n",
       "RMU           0.4341  0.4288  \n",
       "AWQ              NaN     NaN  \n",
       "GPTQ             NaN     NaN  \n",
       "SparseGPT        NaN     NaN  \n",
       "Wanda            NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WMDP OI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine-tune</th>\n",
       "      <th>LoRA</th>\n",
       "      <th>MEMIT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GD</th>\n",
       "      <th>RMU</th>\n",
       "      <th>AWQ</th>\n",
       "      <th>GPTQ</th>\n",
       "      <th>SparseGPT</th>\n",
       "      <th>Wanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fine-tune</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoRA</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0661</td>\n",
       "      <td>0.2437</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0661</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.0285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GD</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2437</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.1733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMU</th>\n",
       "      <td>0.0450</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWQ</th>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTQ</th>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SparseGPT</th>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wanda</th>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.1733</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fine-tune    LoRA   MEMIT      GA      GD     RMU     AWQ    GPTQ  \\\n",
       "Fine-tune        NaN     NaN     NaN  0.0046  0.0001  0.0450  0.0139  0.0090   \n",
       "LoRA             NaN     NaN     NaN  0.0661  0.2437  0.0022  0.0006  0.0097   \n",
       "MEMIT            NaN     NaN     NaN  0.0508  0.0205  0.0086  0.0023  0.0050   \n",
       "GA            0.0046  0.0661  0.0508     NaN     NaN     NaN  0.0232  0.0880   \n",
       "GD            0.0001  0.2437  0.0205     NaN     NaN     NaN  0.0012  0.0230   \n",
       "RMU           0.0450  0.0022  0.0086     NaN     NaN     NaN  0.0150  0.1801   \n",
       "AWQ           0.0139  0.0006  0.0023  0.0232  0.0012  0.0150     NaN     NaN   \n",
       "GPTQ          0.0090  0.0097  0.0050  0.0880  0.0230  0.1801     NaN     NaN   \n",
       "SparseGPT     0.0043  0.0019  0.0029  0.0300  0.2218  0.0241     NaN     NaN   \n",
       "Wanda         0.0045  0.0008  0.0143  0.0285  0.1733  0.0332     NaN     NaN   \n",
       "\n",
       "           SparseGPT   Wanda  \n",
       "Fine-tune     0.0043  0.0045  \n",
       "LoRA          0.0019  0.0008  \n",
       "MEMIT         0.0029  0.0143  \n",
       "GA            0.0300  0.0285  \n",
       "GD            0.2218  0.1733  \n",
       "RMU           0.0241  0.0332  \n",
       "AWQ              NaN     NaN  \n",
       "GPTQ             NaN     NaN  \n",
       "SparseGPT        NaN     NaN  \n",
       "Wanda            NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WMDP MCE Values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine-tune</th>\n",
       "      <th>LoRA</th>\n",
       "      <th>MEMIT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GD</th>\n",
       "      <th>RMU</th>\n",
       "      <th>AWQ</th>\n",
       "      <th>GPTQ</th>\n",
       "      <th>SparseGPT</th>\n",
       "      <th>Wanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fine-tune</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.2854</td>\n",
       "      <td>0.2923</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.3080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoRA</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>0.2972</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.2934</td>\n",
       "      <td>0.2969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.3016</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>0.2929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1757</td>\n",
       "      <td>0.0943</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.2086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GD</th>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMU</th>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.0354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWQ</th>\n",
       "      <td>0.2854</td>\n",
       "      <td>0.2972</td>\n",
       "      <td>0.3016</td>\n",
       "      <td>0.1757</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTQ</th>\n",
       "      <td>0.2923</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.0943</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SparseGPT</th>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.2934</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wanda</th>\n",
       "      <td>0.3080</td>\n",
       "      <td>0.2969</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fine-tune    LoRA   MEMIT      GA      GD     RMU     AWQ    GPTQ  \\\n",
       "Fine-tune        NaN     NaN     NaN  0.2164  0.0437  0.0251  0.2854  0.2923   \n",
       "LoRA             NaN     NaN     NaN  0.0262  0.0493  0.0447  0.2972  0.2841   \n",
       "MEMIT            NaN     NaN     NaN  0.1466  0.0057  0.0428  0.3016  0.2977   \n",
       "GA            0.2164  0.0262  0.1466     NaN     NaN     NaN  0.1757  0.0943   \n",
       "GD            0.0437  0.0493  0.0057     NaN     NaN     NaN  0.0066  0.0062   \n",
       "RMU           0.0251  0.0447  0.0428     NaN     NaN     NaN  0.0233  0.0167   \n",
       "AWQ           0.2854  0.2972  0.3016  0.1757  0.0066  0.0233     NaN     NaN   \n",
       "GPTQ          0.2923  0.2841  0.2977  0.0943  0.0062  0.0167     NaN     NaN   \n",
       "SparseGPT     0.3000  0.2934  0.3070  0.1818  0.0250  0.0350     NaN     NaN   \n",
       "Wanda         0.3080  0.2969  0.2929  0.2086  0.0975  0.0354     NaN     NaN   \n",
       "\n",
       "           SparseGPT   Wanda  \n",
       "Fine-tune     0.3000  0.3080  \n",
       "LoRA          0.2934  0.2969  \n",
       "MEMIT         0.3070  0.2929  \n",
       "GA            0.1818  0.2086  \n",
       "GD            0.0250  0.0975  \n",
       "RMU           0.0350  0.0354  \n",
       "AWQ              NaN     NaN  \n",
       "GPTQ             NaN     NaN  \n",
       "SparseGPT        NaN     NaN  \n",
       "Wanda            NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewrite OI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine-tune</th>\n",
       "      <th>LoRA</th>\n",
       "      <th>MEMIT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GD</th>\n",
       "      <th>RMU</th>\n",
       "      <th>AWQ</th>\n",
       "      <th>GPTQ</th>\n",
       "      <th>SparseGPT</th>\n",
       "      <th>Wanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fine-tune</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.3570</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.0270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoRA</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.5587</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4821</td>\n",
       "      <td>0.3992</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.2448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.4821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GD</th>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.5587</td>\n",
       "      <td>0.3992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMU</th>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWQ</th>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTQ</th>\n",
       "      <td>0.3570</td>\n",
       "      <td>0.3706</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SparseGPT</th>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0367</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wanda</th>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0798</td>\n",
       "      <td>0.2448</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fine-tune    LoRA   MEMIT      GA      GD     RMU     AWQ    GPTQ  \\\n",
       "Fine-tune        NaN     NaN     NaN  0.0686  0.6705  0.0090  0.0389  0.3570   \n",
       "LoRA             NaN     NaN     NaN  0.9960  0.5587  0.0004  0.0835  0.3706   \n",
       "MEMIT            NaN     NaN     NaN  0.4821  0.3992  0.0125  0.0099  0.0323   \n",
       "GA            0.0686  0.9960  0.4821     NaN     NaN     NaN  0.0000  0.0000   \n",
       "GD            0.6705  0.5587  0.3992     NaN     NaN     NaN  0.0117  0.0067   \n",
       "RMU           0.0090  0.0004  0.0125     NaN     NaN     NaN  0.0059  0.0162   \n",
       "AWQ           0.0389  0.0835  0.0099  0.0000  0.0117  0.0059     NaN     NaN   \n",
       "GPTQ          0.3570  0.3706  0.0323  0.0000  0.0067  0.0162     NaN     NaN   \n",
       "SparseGPT     0.0349  0.0148  0.0367  0.0000  0.0044  0.0141     NaN     NaN   \n",
       "Wanda         0.0270  0.0798  0.2448  0.0000  0.0000  0.0005     NaN     NaN   \n",
       "\n",
       "           SparseGPT   Wanda  \n",
       "Fine-tune     0.0349  0.0270  \n",
       "LoRA          0.0148  0.0798  \n",
       "MEMIT         0.0367  0.2448  \n",
       "GA            0.0000  0.0000  \n",
       "GD            0.0044  0.0000  \n",
       "RMU           0.0141  0.0005  \n",
       "AWQ              NaN     NaN  \n",
       "GPTQ             NaN     NaN  \n",
       "SparseGPT        NaN     NaN  \n",
       "Wanda            NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewrite MCE Values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine-tune</th>\n",
       "      <th>LoRA</th>\n",
       "      <th>MEMIT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GD</th>\n",
       "      <th>RMU</th>\n",
       "      <th>AWQ</th>\n",
       "      <th>GPTQ</th>\n",
       "      <th>SparseGPT</th>\n",
       "      <th>Wanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fine-tune</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9314</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.2061</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoRA</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.0519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5179</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.0541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>0.9314</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.5179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GD</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMU</th>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>0.9804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWQ</th>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTQ</th>\n",
       "      <td>0.2061</td>\n",
       "      <td>0.4199</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SparseGPT</th>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wanda</th>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fine-tune    LoRA   MEMIT      GA      GD     RMU     AWQ    GPTQ  \\\n",
       "Fine-tune        NaN     NaN     NaN  0.9314  0.0050  0.0017  0.0062  0.2061   \n",
       "LoRA             NaN     NaN     NaN  0.0040  0.0040  0.0040  0.0040  0.4199   \n",
       "MEMIT            NaN     NaN     NaN  0.5179  0.0696  0.0313  0.0710  0.1554   \n",
       "GA            0.9314  0.0040  0.5179     NaN     NaN     NaN  1.0000  1.0000   \n",
       "GD            0.0050  0.0040  0.0696     NaN     NaN     NaN  0.9883  0.9933   \n",
       "RMU           0.0017  0.0040  0.0313     NaN     NaN     NaN  0.9759  0.9794   \n",
       "AWQ           0.0062  0.0040  0.0710  1.0000  0.9883  0.9759     NaN     NaN   \n",
       "GPTQ          0.2061  0.4199  0.1554  1.0000  0.9933  0.9794     NaN     NaN   \n",
       "SparseGPT     0.0062  0.1374  0.0929  1.0000  0.9933  0.9784     NaN     NaN   \n",
       "Wanda         0.0135  0.0519  0.0541  1.0000  1.0000  0.9804     NaN     NaN   \n",
       "\n",
       "           SparseGPT   Wanda  \n",
       "Fine-tune     0.0062  0.0135  \n",
       "LoRA          0.1374  0.0519  \n",
       "MEMIT         0.0929  0.0541  \n",
       "GA            1.0000  1.0000  \n",
       "GD            0.9933  1.0000  \n",
       "RMU           0.9784  0.9804  \n",
       "AWQ              NaN     NaN  \n",
       "GPTQ             NaN     NaN  \n",
       "SparseGPT        NaN     NaN  \n",
       "Wanda            NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization OI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine-tune</th>\n",
       "      <th>LoRA</th>\n",
       "      <th>MEMIT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GD</th>\n",
       "      <th>RMU</th>\n",
       "      <th>AWQ</th>\n",
       "      <th>GPTQ</th>\n",
       "      <th>SparseGPT</th>\n",
       "      <th>Wanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fine-tune</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.5607</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoRA</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7835</td>\n",
       "      <td>0.4808</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.2672</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.0059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4096</td>\n",
       "      <td>0.4077</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.7835</td>\n",
       "      <td>0.4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GD</th>\n",
       "      <td>0.5607</td>\n",
       "      <td>0.4808</td>\n",
       "      <td>0.4077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMU</th>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWQ</th>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTQ</th>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.2672</td>\n",
       "      <td>0.1089</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SparseGPT</th>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wanda</th>\n",
       "      <td>0.0381</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fine-tune    LoRA   MEMIT      GA      GD     RMU     AWQ    GPTQ  \\\n",
       "Fine-tune        NaN     NaN     NaN  0.0401  0.5607  0.0285  0.0537  0.3564   \n",
       "LoRA             NaN     NaN     NaN  0.7835  0.4808  0.0393  0.1400  0.2672   \n",
       "MEMIT            NaN     NaN     NaN  0.4096  0.4077  0.0359  0.0033  0.1089   \n",
       "GA            0.0401  0.7835  0.4096     NaN     NaN     NaN  0.0000  0.0000   \n",
       "GD            0.5607  0.4808  0.4077     NaN     NaN     NaN  0.0000  0.0067   \n",
       "RMU           0.0285  0.0393  0.0359     NaN     NaN     NaN  0.0027  0.0023   \n",
       "AWQ           0.0537  0.1400  0.0033  0.0000  0.0000  0.0027     NaN     NaN   \n",
       "GPTQ          0.3564  0.2672  0.1089  0.0000  0.0067  0.0023     NaN     NaN   \n",
       "SparseGPT     0.0152  0.0713  0.0347  0.0000  0.0067  0.0032     NaN     NaN   \n",
       "Wanda         0.0381  0.0059  0.2080  0.0000  0.0067  0.0057     NaN     NaN   \n",
       "\n",
       "           SparseGPT   Wanda  \n",
       "Fine-tune     0.0152  0.0381  \n",
       "LoRA          0.0713  0.0059  \n",
       "MEMIT         0.0347  0.2080  \n",
       "GA            0.0000  0.0000  \n",
       "GD            0.0067  0.0067  \n",
       "RMU           0.0032  0.0057  \n",
       "AWQ              NaN     NaN  \n",
       "GPTQ             NaN     NaN  \n",
       "SparseGPT        NaN     NaN  \n",
       "Wanda            NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization MCE Values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine-tune</th>\n",
       "      <th>LoRA</th>\n",
       "      <th>MEMIT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GD</th>\n",
       "      <th>RMU</th>\n",
       "      <th>AWQ</th>\n",
       "      <th>GPTQ</th>\n",
       "      <th>SparseGPT</th>\n",
       "      <th>Wanda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fine-tune</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9599</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>0.1848</td>\n",
       "      <td>0.1608</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.2044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoRA</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.2852</td>\n",
       "      <td>0.2592</td>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>0.4623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMIT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5904</td>\n",
       "      <td>0.1067</td>\n",
       "      <td>0.0703</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.1977</td>\n",
       "      <td>0.1359</td>\n",
       "      <td>0.1039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GA</th>\n",
       "      <td>0.9599</td>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.5904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GD</th>\n",
       "      <td>0.1892</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>0.1067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMU</th>\n",
       "      <td>0.1848</td>\n",
       "      <td>0.2852</td>\n",
       "      <td>0.0703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>0.9765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWQ</th>\n",
       "      <td>0.1608</td>\n",
       "      <td>0.2592</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPTQ</th>\n",
       "      <td>0.4010</td>\n",
       "      <td>0.5944</td>\n",
       "      <td>0.1977</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SparseGPT</th>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>0.1359</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wanda</th>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.4623</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fine-tune    LoRA   MEMIT      GA      GD     RMU     AWQ    GPTQ  \\\n",
       "Fine-tune        NaN     NaN     NaN  0.9599  0.1892  0.1848  0.1608  0.4010   \n",
       "LoRA             NaN     NaN     NaN  0.2165  0.2915  0.2852  0.2592  0.5944   \n",
       "MEMIT            NaN     NaN     NaN  0.5904  0.1067  0.0703  0.1095  0.1977   \n",
       "GA            0.9599  0.2165  0.5904     NaN     NaN     NaN  1.0000  1.0000   \n",
       "GD            0.1892  0.2915  0.1067     NaN     NaN     NaN  1.0000  0.9933   \n",
       "RMU           0.1848  0.2852  0.0703     NaN     NaN     NaN  0.9772  0.9801   \n",
       "AWQ           0.1608  0.2592  0.1095  1.0000  1.0000  0.9772     NaN     NaN   \n",
       "GPTQ          0.4010  0.5944  0.1977  1.0000  0.9933  0.9801     NaN     NaN   \n",
       "SparseGPT     0.2260  0.4767  0.1359  1.0000  0.9933  0.9770     NaN     NaN   \n",
       "Wanda         0.2044  0.4623  0.1039  1.0000  0.9933  0.9765     NaN     NaN   \n",
       "\n",
       "           SparseGPT   Wanda  \n",
       "Fine-tune     0.2260  0.2044  \n",
       "LoRA          0.4767  0.4623  \n",
       "MEMIT         0.1359  0.1039  \n",
       "GA            1.0000  1.0000  \n",
       "GD            0.9933  0.9933  \n",
       "RMU           0.9770  0.9765  \n",
       "AWQ              NaN     NaN  \n",
       "GPTQ             NaN     NaN  \n",
       "SparseGPT        NaN     NaN  \n",
       "Wanda            NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define intervention names and types\n",
    "intervention_names = [intervention for intervention in list(data[\"edit\"].unique()) + list(data[\"unlearn\"].unique()) + list(data[\"compression\"].unique()) if intervention is not None]\n",
    "intervention_type = {\n",
    "    \"LoRA\": \"edit\",\n",
    "    \"MEMIT\": \"edit\",\n",
    "    \"Fine-tune\": \"edit\",\n",
    "    \"SparseGPT\": \"compression\",\n",
    "    \"Wanda\": \"compression\",\n",
    "    \"GPTQ\": \"compression\",\n",
    "    \"AWQ\": \"compression\",\n",
    "    \"RMU\": \"unlearn\",\n",
    "    \"GA\": \"unlearn\",\n",
    "    \"GD\": \"unlearn\",\n",
    "}\n",
    "\n",
    "# Initialize heatmap data frames with default values\n",
    "default_value = None\n",
    "mmlu_oi_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "wmdp_oi_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "edit_oi_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "generalization_oi_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "\n",
    "# Initialize max value data frames\n",
    "mmlu_mce_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "wmdp_mce_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "edit_mce_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "generalization_mce_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "\n",
    "# Populate the heatmap and max value data frames\n",
    "for first_intervention in intervention_names:\n",
    "    for second_intervention in intervention_names:\n",
    "        first_intervention_type = intervention_type[first_intervention]\n",
    "        second_intervention_type = intervention_type[second_intervention]\n",
    "        if first_intervention_type == second_intervention_type:\n",
    "            continue\n",
    "\n",
    "        compositions = data[(data[first_intervention_type] == first_intervention) & (data[second_intervention_type] == second_intervention)]\n",
    "        if first_intervention in [\"SparseGPT\", \"Wanda\"] or second_intervention in [\"SparseGPT\", \"Wanda\"]:\n",
    "            compositions = compositions[compositions[\"sparsity_ratio\"] == 0.25]\n",
    "        elif first_intervention in [\"GPTQ\", \"AWQ\"] or second_intervention in [\"GPTQ\", \"AWQ\"]:\n",
    "            compositions = compositions[compositions[\"wbits\"] == 4]\n",
    "        \n",
    "        assert len(compositions) == 2, f\"Expected 2 compositions for {first_intervention} and {second_intervention}, but found {len(compositions)}\"\n",
    "        \n",
    "        # Calculate OIs\n",
    "        mmlu_diff = abs(compositions[\"mmlu accuracy\"].iloc[0] - compositions[\"mmlu accuracy\"].iloc[1]).round(4)\n",
    "        mmlu_oi_data[first_intervention][second_intervention] = mmlu_diff\n",
    "        \n",
    "        avg_wmdp_diff = abs(((compositions.iloc[0][\"wmdp_cyber accuracy\"] + compositions.iloc[0][\"wmdp_bio accuracy\"]) / 2) - ((compositions.iloc[1][\"wmdp_cyber accuracy\"] + compositions.iloc[1][\"wmdp_bio accuracy\"]) / 2)).round(4)\n",
    "        wmdp_oi_data[first_intervention][second_intervention] = avg_wmdp_diff\n",
    "        \n",
    "        edit_diff = abs(compositions[\"Rewrite accuracy\"].iloc[0] - compositions[\"Rewrite accuracy\"].iloc[1]).round(4)\n",
    "        edit_oi_data[first_intervention][second_intervention] = edit_diff\n",
    "\n",
    "        generalization_diff = abs(compositions[\"Generalization\"].iloc[0] - compositions[\"Generalization\"].iloc[1]).round(4)\n",
    "        generalization_oi_data[first_intervention][second_intervention] = generalization_diff\n",
    "        \n",
    "        # Calculate MCE values\n",
    "        mmlu_mce = 1 - max(compositions[\"mmlu accuracy\"].iloc[0], compositions[\"mmlu accuracy\"].iloc[1]).round(4)\n",
    "        mmlu_mce_data[first_intervention][second_intervention] = mmlu_mce\n",
    "        \n",
    "        avg_wmdp_acc = min((compositions.iloc[0][\"wmdp_cyber accuracy\"] + compositions.iloc[0][\"wmdp_bio accuracy\"]) / 2, (compositions.iloc[1][\"wmdp_cyber accuracy\"] + compositions.iloc[1][\"wmdp_bio accuracy\"]) / 2).round(4)\n",
    "        wmdp_mce = calculate_wmdp_unlearn_error(avg_wmdp_acc)\n",
    "        wmdp_mce_data[first_intervention][second_intervention] = wmdp_mce\n",
    "        \n",
    "        edit_mce = 1 - max(compositions[\"Rewrite accuracy\"].iloc[0], compositions[\"Rewrite accuracy\"].iloc[1]).round(4)\n",
    "        edit_mce_data[first_intervention][second_intervention] = edit_mce\n",
    "\n",
    "        generalization_mce = 1 - max(compositions[\"Generalization\"].iloc[0], compositions[\"Generalization\"].iloc[1]).round(4)\n",
    "        generalization_mce_data[first_intervention][second_intervention] = generalization_mce\n",
    "\n",
    "# Display the results\n",
    "print(\"MMLU OI\")\n",
    "display(mmlu_oi_data)\n",
    "\n",
    "print(\"MMLU MCE Values\")\n",
    "display(mmlu_mce_data)\n",
    "\n",
    "print(\"WMDP OI\")\n",
    "display(wmdp_oi_data)\n",
    "\n",
    "print(\"WMDP MCE Values\")\n",
    "display(wmdp_mce_data)\n",
    "\n",
    "print(\"Rewrite OI\")\n",
    "display(edit_oi_data)\n",
    "\n",
    "print(\"Rewrite MCE Values\")\n",
    "display(edit_mce_data)\n",
    "\n",
    "print(\"Generalization OI\")\n",
    "display(generalization_oi_data)\n",
    "\n",
    "print(\"Generalization MCE Values\")\n",
    "display(generalization_mce_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_order = [\"Wanda\", \"SparseGPT\", \"AWQ\", \"GPTQ\"]\n",
    "editor_order = [\"Fine-tune\", \"MEMIT\", \"LoRA\"]\n",
    "unlearn_order = [\"GA\", \"GD\", \"RMU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_value(value):\n",
    "    if pd.isnull(value):\n",
    "        return ''\n",
    "    elif value > .995:\n",
    "        return '1'\n",
    "    else:\n",
    "        return f'{value:.2f}'[1:] if value < 1 else f'{value:.2f}'\n",
    "\n",
    "def latex_bold_if_min(value: str, max_value: float):\n",
    "    return f'\\\\textbf{{{value}}}' if value == format_value(min_value) else value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KE ←→ Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Row Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{tabular}{lcccccccccccccccccc}\n",
      "        \\toprule\n",
      "        & \\multicolumn{6}{c}{\\textbf{Edit Success}} & \\multicolumn{6}{c}{\\textbf{Generalization}} & \\multicolumn{6}{c}{\\textbf{MMLU}} \\\\\n",
      "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13} \\cmidrule(lr){14-19}\n",
      "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
      "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13} \\cmidrule(lr){14-16} \\cmidrule(lr){17-19}\n",
      "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
      "        \\midrule\n",
      "        Wanda & .01 & .05 & .05 & .03 & .24 & .08 & .20 & .10 & .46 & .04 & .21 & .01 & .40 & .41 & .40 & .00 & .00 & .00 \\\\\n",
      "        SparseGPT & .01 & .09 & .14 & .03 & .04 & .01 & .23 & .14 & .48 & .02 & .03 & .07 & .40 & .40 & .40 & .00 & .00 & .00 \\\\\n",
      "        \\cdashlinelr{1-19}\n",
      "        AWQ & .01 & .07 & .00 & .04 & .01 & .08 & .16 & .11 & .26 & .05 & .00 & .14 & .41 & .41 & .41 & .02 & .01 & .00 \\\\\n",
      "        GPTQ & .21 & .16 & .42 & .36 & .03 & .37 & .40 & .20 & .59 & .36 & .11 & .27 & .41 & .41 & .42 & .00 & .01 & .01 \\\\\n",
      "        \\midrule\n",
      "        \\textit{Average} & .06 & .09 & .15 & .11 & .08 & .14 & .25 & .14 & .45 & .12 & .09 & .12 & .40 & .41 & .41 & .01 & .01 & .00 \\\\\n",
      "        \\bottomrule \\\\\n",
      "    \\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_latex_table_ke_mc(edit_mce_df, edit_oi_df, gen_mce_df, gen_oi_df, mmlu_mce_df, mmlu_oi_df, edit_interventions, mmlu_interventions):\n",
    "    latex_code = r\"\"\"\n",
    "    \\begin{tabular}{lcccccccccccccccccc}\n",
    "        \\toprule\n",
    "        & \\multicolumn{6}{c}{\\textbf{Edit Success}} & \\multicolumn{6}{c}{\\textbf{Generalization}} & \\multicolumn{6}{c}{\\textbf{MMLU}} \\\\\n",
    "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13} \\cmidrule(lr){14-19}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13} \\cmidrule(lr){14-16} \\cmidrule(lr){17-19}\n",
    "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
    "        \\midrule\n",
    "\"\"\"\n",
    "    table_values = []\n",
    "\n",
    "    for compressor in compression_order:\n",
    "        if compressor == \"AWQ\":\n",
    "            latex_code += r\"        \\cdashlinelr{1-19}\" + \"\\n\"\n",
    "\n",
    "        row_values = []\n",
    "        table_row = f\"        {compressor}\"\n",
    "        for metrics_category in [(edit_mce_df, edit_oi_df), (gen_mce_df, gen_oi_df), (mmlu_mce_df, mmlu_oi_df)]:\n",
    "            for sub_metric in metrics_category:\n",
    "                for editor in editor_order:\n",
    "                    table_row += f\" & {format_value(sub_metric[editor][compressor])}\"\n",
    "                    row_values.append(sub_metric[editor][compressor])\n",
    "        \n",
    "        table_row += r\" \\\\\"\n",
    "        latex_code += table_row + \"\\n\"\n",
    "        table_values.append(row_values)\n",
    "    \n",
    "    latex_code += r\"        \\midrule\" + \"\\n\"\n",
    "    avg_row = r\"        \\textit{Average}\"\n",
    "    for col_avg in np.array(table_values).mean(0).tolist():\n",
    "        avg_row += f\" & {format_value(col_avg)}\"\n",
    "    \n",
    "    latex_code += avg_row + r\" \\\\\" + \"\\n\"\n",
    "    latex_code += r'''        \\bottomrule \\\\\n",
    "    \\end{tabular}\n",
    "'''\n",
    "\n",
    "    print(latex_code)\n",
    "\n",
    "\n",
    "generate_latex_table_ke_mc(\n",
    "    edit_mce_data,\n",
    "    edit_oi_data,\n",
    "    generalization_mce_data,\n",
    "    generalization_oi_data,\n",
    "    mmlu_mce_data,\n",
    "    mmlu_oi_data,\n",
    "    editor_order,\n",
    "    compression_order,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Row Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Row: KE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{tabular}{lcccccccccccc}\n",
      "        \\toprule\n",
      "        & \\multicolumn{6}{c}{\\textbf{Edit Success}} & \\multicolumn{6}{c}{\\textbf{Generalization}} \\\\\n",
      "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
      "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
      "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
      "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
      "        \\midrule\n",
      "        \\textbf{Wanda} & .01 & .05 & .05 & .03 & .24 & .08 & .20 & .10 & .46 & .04 & .21 & .01 \\\\\n",
      "        \\textbf{SparseGPT} & .01 & .09 & .14 & .03 & .04 & .01 & .23 & .14 & .48 & .02 & .03 & .07 \\\\\n",
      "        \\cdashlinelr{1-13}\n",
      "        \\textbf{AWQ} & .01 & .07 & .00 & .04 & .01 & .08 & .16 & .11 & .26 & .05 & .00 & .14 \\\\\n",
      "        \\textbf{GPTQ} & .21 & .16 & .42 & .36 & .03 & .37 & .40 & .20 & .59 & .36 & .11 & .27 \\\\\n",
      "        \\midrule\n",
      "        \\textit{Average} & .06 & .09 & .15 & .11 & .08 & .14 & .25 & .14 & .45 & .12 & .09 & .12 \\\\\n",
      "        \\bottomrule \\\\\n",
      "    \\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_latex_table_ke_mc_edit_only(edit_mce_df, edit_oi_df, gen_mce_df, gen_oi_df, edit_interventions, compression_order):\n",
    "    latex_code = r\"\"\"\n",
    "    \\begin{tabular}{lcccccccccccc}\n",
    "        \\toprule\n",
    "        & \\multicolumn{6}{c}{\\textbf{Edit Success}} & \\multicolumn{6}{c}{\\textbf{Generalization}} \\\\\n",
    "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
    "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
    "        \\midrule\n",
    "\"\"\"\n",
    "    table_values = []\n",
    "\n",
    "    for compressor in compression_order:\n",
    "        if compressor == \"AWQ\":\n",
    "            latex_code += r\"        \\cdashlinelr{1-13}\" + \"\\n\"\n",
    "\n",
    "        row_values = []\n",
    "        table_row = r\"        \\textbf{\" + compressor + \"}\"\n",
    "        for metrics_category in [(edit_mce_df, edit_oi_df), (gen_mce_df, gen_oi_df)]:\n",
    "            for sub_metric in metrics_category:\n",
    "                for editor in editor_order:\n",
    "                    table_row += f\" & {format_value(sub_metric[editor][compressor])}\"\n",
    "                    row_values.append(sub_metric[editor][compressor])\n",
    "        \n",
    "        table_row += r\" \\\\\"\n",
    "        latex_code += table_row + \"\\n\"\n",
    "        table_values.append(row_values)\n",
    "    \n",
    "    latex_code += r\"        \\midrule\" + \"\\n\"\n",
    "    avg_row = r\"        \\textit{Average}\"\n",
    "    for col_avg in np.array(table_values).mean(0).tolist():\n",
    "        avg_row += f\" & {format_value(col_avg)}\"\n",
    "    \n",
    "    latex_code += avg_row + r\" \\\\\" + \"\\n\"\n",
    "    latex_code += r'''        \\bottomrule \\\\\n",
    "    \\end{tabular}\n",
    "'''\n",
    "\n",
    "    print(latex_code)\n",
    "\n",
    "\n",
    "generate_latex_table_ke_mc_edit_only(\n",
    "    edit_mce_data,\n",
    "    edit_oi_data,\n",
    "    generalization_mce_data,\n",
    "    generalization_oi_data,\n",
    "    editor_order,\n",
    "    compression_order,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Row: MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{tabular}{lcccccc}\n",
      "        & \\multicolumn{6}{c}{\\textbf{MMU}} \\\\\n",
      "        \\cmidrule(lr){2-7}\n",
      "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
      "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n",
      "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
      "        \\midrule\n",
      "        \\textbf{Wanda} & .40 & .41 & .40 & .00 & .00 & .00 \\\\\n",
      "        \\textbf{SparseGPT} & .40 & .40 & .40 & .00 & .00 & .00 \\\\\n",
      "        \\cdashlinelr{1-7}\n",
      "        \\textbf{AWQ} & .41 & .41 & .41 & .02 & .01 & .00 \\\\\n",
      "        \\textbf{GPTQ} & .41 & .41 & .42 & .00 & .01 & .01 \\\\\n",
      "        \\midrule\n",
      "        \\textit{Average} & .40 & .41 & .41 & .01 & .01 & .00 \\\\\n",
      "        \\bottomrule \\\\\n",
      "    \\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_latex_table_ke_mc_mmlu_only(mmlu_mce_df, mmlu_oi_df, editor_order, compression_order):\n",
    "    latex_code = r\"\"\"\n",
    "    \\begin{tabular}{lcccccc}\n",
    "        & \\multicolumn{6}{c}{\\textbf{MMU}} \\\\\n",
    "        \\cmidrule(lr){2-7}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n",
    "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
    "        \\midrule\n",
    "\"\"\"\n",
    "    table_values = []\n",
    "\n",
    "    for compressor in compression_order:\n",
    "        if compressor == \"AWQ\":\n",
    "            latex_code += r\"        \\cdashlinelr{1-7}\" + \"\\n\"\n",
    "\n",
    "        row_values = []\n",
    "        table_row = r\"        \\textbf{\" + compressor + \"}\"\n",
    "        for metrics_category in [(mmlu_mce_df, mmlu_oi_df)]:\n",
    "            for sub_metric in metrics_category:\n",
    "                for editor in editor_order:\n",
    "                    table_row += f\" & {format_value(sub_metric[editor][compressor])}\"\n",
    "                    row_values.append(sub_metric[editor][compressor])\n",
    "        \n",
    "        table_row += r\" \\\\\"\n",
    "        latex_code += table_row + \"\\n\"\n",
    "        table_values.append(row_values)\n",
    "    \n",
    "    latex_code += r\"        \\midrule\" + \"\\n\"\n",
    "    avg_row = r\"        \\textit{Average}\"\n",
    "    for col_avg in np.array(table_values).mean(0).tolist():\n",
    "        avg_row += f\" & {format_value(col_avg)}\"\n",
    "    \n",
    "    latex_code += avg_row + r\" \\\\\" + \"\\n\"\n",
    "    latex_code += r'''        \\bottomrule \\\\\n",
    "    \\end{tabular}\n",
    "'''\n",
    "\n",
    "    print(latex_code)\n",
    "\n",
    "\n",
    "generate_latex_table_ke_mc_mmlu_only(\n",
    "    mmlu_mce_data,\n",
    "    mmlu_oi_data,\n",
    "    editor_order,\n",
    "    compression_order,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MU ←→ MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{tabular}{lcccccccccccc}\n",
      "        \\toprule\n",
      "        & \\multicolumn{6}{c}{\\textbf{WMDP}} & \\multicolumn{6}{c}{\\textbf{MMLU}} \\\\\n",
      "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
      "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
      "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
      "        \\textbf{Method} & \\textbf{GA} & \\textbf{GD} & \\textbf{RMU} & \\textbf{GA} & \\textbf{GD} & \\textbf{RMU} & \\textbf{GA} & \\textbf{GD} & \\textbf{RMU} & \\textbf{GA} & \\textbf{GD} & \\textbf{RMU} \\\\\n",
      "        \\midrule\n",
      "        \\textbf{Wanda} & .21 & .10 & .04 & .03 & .17 & .03 & .46 & .42 & .43 & .02 & .01 & .01 \\\\\n",
      "        \\textbf{SparseGPT} & .18 & .03 & .03 & .03 & .22 & .02 & .49 & .42 & .43 & .06 & .09 & .01 \\\\\n",
      "        \\cdashlinelr{1-13}\n",
      "        \\textbf{AWQ} & .18 & .01 & .02 & .02 & .00 & .01 & .53 & .58 & .44 & .02 & .07 & .01 \\\\\n",
      "        \\textbf{GPTQ} & .09 & .01 & .02 & .09 & .02 & .18 & .52 & .44 & .42 & .09 & .24 & .05 \\\\\n",
      "        \\midrule\n",
      "        \\textit{Average} & .17 & .03 & .03 & .04 & .10 & .06 & .50 & .46 & .43 & .05 & .10 & .02 \\\\\n",
      "        \\bottomrule \\\\\n",
      "    \\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Have WMDP and MMLU in the same table\n",
    "def generate_latex_table_mu_mc():\n",
    "    latex_code = r\"\"\"\n",
    "    \\begin{tabular}{lcccccccccccc}\n",
    "        \\toprule\n",
    "        & \\multicolumn{6}{c}{\\textbf{WMDP}} & \\multicolumn{6}{c}{\\textbf{MMLU}} \\\\\n",
    "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
    "        \\textbf{Method} & \\textbf{GA} & \\textbf{GD} & \\textbf{RMU} & \\textbf{GA} & \\textbf{GD} & \\textbf{RMU} & \\textbf{GA} & \\textbf{GD} & \\textbf{RMU} & \\textbf{GA} & \\textbf{GD} & \\textbf{RMU} \\\\\n",
    "        \\midrule\n",
    "\"\"\"\n",
    "    table_values = []\n",
    "\n",
    "    for compressor in compression_order:\n",
    "        if compressor == \"AWQ\":\n",
    "            latex_code += r\"        \\cdashlinelr{1-13}\" + \"\\n\"\n",
    "\n",
    "        row_values = []\n",
    "        table_row = r\"        \\textbf{\" + compressor + \"}\"\n",
    "        for metrics_category in [(wmdp_mce_data, wmdp_oi_data), (mmlu_mce_data, mmlu_oi_data)]:\n",
    "            for sub_metric in metrics_category:\n",
    "                for unlearner in unlearn_order:\n",
    "                    table_row += f\" & {format_value(sub_metric[unlearner][compressor])}\"\n",
    "                    row_values.append(sub_metric[unlearner][compressor])\n",
    "        \n",
    "        table_row += r\" \\\\\"\n",
    "        latex_code += table_row + \"\\n\"\n",
    "        table_values.append(row_values)\n",
    "    \n",
    "    latex_code += r\"        \\midrule\" + \"\\n\"\n",
    "    avg_row = r\"        \\textit{Average}\"\n",
    "    for col_avg in np.array(table_values).mean(0).tolist():\n",
    "        avg_row += f\" & {format_value(col_avg)}\"\n",
    "\n",
    "    latex_code += avg_row + r\" \\\\\" + \"\\n\"\n",
    "    latex_code += r'''        \\bottomrule \\\\\n",
    "    \\end{tabular}\n",
    "'''\n",
    "\n",
    "    print(latex_code)\n",
    "\n",
    "\n",
    "generate_latex_table_mu_mc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KE ←→ MU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Row: KE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{tabular}{lcccccccccccc}\n",
      "        \\toprule\n",
      "        & \\multicolumn{6}{c}{\\textbf{Edit Success}} & \\multicolumn{6}{c}{\\textbf{Generalization}} \\\\\n",
      "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
      "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
      "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
      "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
      "        \\midrule\n",
      "        \\textbf{GA} & .93 & .52 & .00 & .07 & .48 & 1 & .96 & .59 & .22 & .04 & .41 & .78 \\\\\n",
      "        \\textbf{GD} & .01 & .07 & .00 & .67 & .40 & .56 & .19 & .11 & .29 & .56 & .41 & .48 \\\\\n",
      "        \\textbf{RMU} & .00 & .03 & .00 & .01 & .01 & .00 & .18 & .07 & .29 & .03 & .04 & .04 \\\\\n",
      "        \\midrule\n",
      "        \\textit{Average} & .31 & .21 & .00 & .25 & .30 & .52 & .44 & .26 & .26 & .21 & .28 & .43 \\\\\n",
      "        \\bottomrule \\\\\n",
      "    \\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_latex_table_ke_mu_ke_metrics():\n",
    "    latex_code = r\"\"\"\n",
    "    \\begin{tabular}{lcccccccccccc}\n",
    "        \\toprule\n",
    "        & \\multicolumn{6}{c}{\\textbf{Edit Success}} & \\multicolumn{6}{c}{\\textbf{Generalization}} \\\\\n",
    "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
    "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
    "        \\midrule\n",
    "\"\"\"\n",
    "    table_values = []\n",
    "\n",
    "    for unlearner in unlearn_order:\n",
    "        row_values = []\n",
    "        table_row = r\"        \\textbf{\" + unlearner + \"}\"\n",
    "        for metrics_category in [(edit_mce_data, edit_oi_data), (generalization_mce_data, generalization_oi_data)]:\n",
    "            for sub_metric in metrics_category:\n",
    "                for editor in editor_order:\n",
    "                    table_row += f\" & {format_value(sub_metric[editor][unlearner])}\"\n",
    "                    row_values.append(sub_metric[editor][unlearner])\n",
    "        \n",
    "        table_row += r\" \\\\\"\n",
    "        latex_code += table_row + \"\\n\"\n",
    "        table_values.append(row_values)\n",
    "    \n",
    "    latex_code += r\"        \\midrule\" + \"\\n\"\n",
    "    avg_row = r\"        \\textit{Average}\"\n",
    "    for col_avg in np.array(table_values).mean(0).tolist():\n",
    "        avg_row += f\" & {format_value(col_avg)}\"\n",
    "    \n",
    "    latex_code += avg_row + r\" \\\\\" + \"\\n\"\n",
    "    latex_code += r'''        \\bottomrule \\\\\n",
    "    \\end{tabular}\n",
    "'''\n",
    "\n",
    "    print(latex_code)\n",
    "\n",
    "\n",
    "generate_latex_table_ke_mu_ke_metrics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Row: MU Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \\begin{tabular}{lcccccccccccc}\n",
      "        \\toprule\n",
      "        & \\multicolumn{6}{c}{\\textbf{WMDP}} & \\multicolumn{6}{c}{\\textbf{MMLU}} \\\\\n",
      "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
      "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
      "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
      "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
      "        \\midrule\n",
      "        \\textbf{GA} & .22 & .15 & .03 & .00 & .05 & .07 & .47 & .51 & .64 & .01 & .04 & .07 \\\\\n",
      "        \\textbf{GD} & .04 & .01 & .05 & .00 & .02 & .24 & .41 & .42 & .41 & .18 & .22 & .14 \\\\\n",
      "        \\textbf{RMU} & .03 & .04 & .04 & .04 & .01 & .00 & .43 & .44 & .44 & .01 & .00 & .04 \\\\\n",
      "        \\midrule\n",
      "        \\textit{Average} & .10 & .07 & .04 & .02 & .03 & .10 & .44 & .46 & .50 & .07 & .09 & .08 \\\\\n",
      "        \\bottomrule \\\\\n",
      "    \\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Have WMDP and MMLU in the same table\n",
    "def generate_latex_table_ke_mu_mu_metrics():\n",
    "    latex_code = r\"\"\"\n",
    "    \\begin{tabular}{lcccccccccccc}\n",
    "        \\toprule\n",
    "        & \\multicolumn{6}{c}{\\textbf{WMDP}} & \\multicolumn{6}{c}{\\textbf{MMLU}} \\\\\n",
    "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{MCE ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{OI ($\\downarrow$)}} \\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
    "        \\textbf{Method} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} & \\textbf{FT} & \\textbf{MEMIT} & \\textbf{LoRA} \\\\\n",
    "        \\midrule\n",
    "\"\"\"\n",
    "    table_values = []\n",
    "\n",
    "    for unlearner in unlearn_order:\n",
    "        row_values = []\n",
    "        table_row = r\"        \\textbf{\" + unlearner + \"}\"\n",
    "        for metrics_category in [(wmdp_mce_data, wmdp_oi_data), (mmlu_mce_data, mmlu_oi_data)]:\n",
    "            for sub_metric in metrics_category:\n",
    "                for editor in editor_order:\n",
    "                    table_row += f\" & {format_value(sub_metric[editor][unlearner])}\"\n",
    "                    row_values.append(sub_metric[editor][unlearner])\n",
    "        \n",
    "        table_row += r\" \\\\\"\n",
    "        latex_code += table_row + \"\\n\"\n",
    "        table_values.append(row_values)\n",
    "    \n",
    "    latex_code += r\"        \\midrule\" + \"\\n\"\n",
    "    avg_row = r\"        \\textit{Average}\"\n",
    "    for col_avg in np.array(table_values).mean(0).tolist():\n",
    "        avg_row += f\" & {format_value(col_avg)}\"\n",
    "\n",
    "    latex_code += avg_row + r\" \\\\\" + \"\\n\"\n",
    "    latex_code += r'''        \\bottomrule \\\\\n",
    "    \\end{tabular}\n",
    "'''\n",
    "\n",
    "    print(latex_code)\n",
    "\n",
    "\n",
    "generate_latex_table_ke_mu_mu_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Detailed Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- None    : Done\n",
    "- KE ←→ MC: Done\n",
    "- MC ←→ KE: Done\n",
    "- KE ←→ MU: Todo\n",
    "- MU ←→ KE: Todo\n",
    "- MU ←→ MC: Todo\n",
    "- MC ←→ MU: Todo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>edit_set</th>\n",
       "      <th>number_of_edits</th>\n",
       "      <th>rmu_layer_id</th>\n",
       "      <th>wbits</th>\n",
       "      <th>sparsity_ratio</th>\n",
       "      <th>qa_question_count_limit_x</th>\n",
       "      <th>mmlu accuracy</th>\n",
       "      <th>wmdp_bio accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>edit_dataset</th>\n",
       "      <th>compression_dataset</th>\n",
       "      <th>qa_question_count_limit_y</th>\n",
       "      <th>PPL</th>\n",
       "      <th>PPL edits</th>\n",
       "      <th>PPl QA</th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>PPl edits unmasked</th>\n",
       "      <th>date_y</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWQ2bit-to-ft</td>\n",
       "      <td>1.718168e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.258743</td>\n",
       "      <td>0.243591</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>33638.4375</td>\n",
       "      <td>338052.34375</td>\n",
       "      <td>102475.617188</td>\n",
       "      <td>-1</td>\n",
       "      <td>78554.226562</td>\n",
       "      <td>2024-05-20 17:38:54.680141568</td>\n",
       "      <td>AWQ (2bit) $\\rightarrow$FT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AWQ2bit-to-lora</td>\n",
       "      <td>1.718359e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.262028</td>\n",
       "      <td>0.242419</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>141960.90625</td>\n",
       "      <td>37042.675781</td>\n",
       "      <td>267692.4375</td>\n",
       "      <td>-1</td>\n",
       "      <td>93121.015625</td>\n",
       "      <td>2024-06-14 09:58:09.475097088</td>\n",
       "      <td>AWQ (2bit) $\\rightarrow$LoRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AWQ2bit-to-memit</td>\n",
       "      <td>1.718128e+09</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.264049</td>\n",
       "      <td>0.241424</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>1735678.75</td>\n",
       "      <td>996271.5625</td>\n",
       "      <td>1198751.125</td>\n",
       "      <td>-1</td>\n",
       "      <td>1074956.375</td>\n",
       "      <td>2024-05-20 17:01:28.464071680</td>\n",
       "      <td>AWQ (2bit) $\\rightarrow$MEMIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AWQ3bit-to-ft</td>\n",
       "      <td>1.718614e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.509479</td>\n",
       "      <td>0.594580</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>7.569585</td>\n",
       "      <td>50514.761719</td>\n",
       "      <td>627.454346</td>\n",
       "      <td>-1</td>\n",
       "      <td>945.212585</td>\n",
       "      <td>2024-06-17 08:37:00.186556416</td>\n",
       "      <td>AWQ (3bit) $\\rightarrow$FT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AWQ3bit-to-lora</td>\n",
       "      <td>1.718611e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.510034</td>\n",
       "      <td>0.604242</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>55.437218</td>\n",
       "      <td>334348.21875</td>\n",
       "      <td>9609.279297</td>\n",
       "      <td>-1</td>\n",
       "      <td>16729.5</td>\n",
       "      <td>2024-06-17 07:55:00.258938112</td>\n",
       "      <td>AWQ (3bit) $\\rightarrow$LoRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>wanda0.65\\%-gd</td>\n",
       "      <td>1.718204e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229098</td>\n",
       "      <td>0.249018</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>16268.005859</td>\n",
       "      <td>5423145.5</td>\n",
       "      <td>81961160.0</td>\n",
       "      <td>447.25 GFLOPS</td>\n",
       "      <td>89329664.0</td>\n",
       "      <td>2024-06-12 14:53:38.419730176</td>\n",
       "      <td>Wanda 0.65$\\rightarrow$GD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>wanda0.65\\%-rmu</td>\n",
       "      <td>1.716764e+09</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229360</td>\n",
       "      <td>0.249542</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>45.534767</td>\n",
       "      <td>83373.453125</td>\n",
       "      <td>1477.523804</td>\n",
       "      <td>760 GFLOPS</td>\n",
       "      <td>1549.510742</td>\n",
       "      <td>2024-05-20 19:27:06.522696448</td>\n",
       "      <td>Wanda 0.65$\\rightarrow$RMU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>wanda0.75\\%-ga</td>\n",
       "      <td>1.718194e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229811</td>\n",
       "      <td>0.249018</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>8125971838094910145582358934348365824.0</td>\n",
       "      <td>176520989218511062933962752.0</td>\n",
       "      <td>31936578699555629957120.0</td>\n",
       "      <td>357.84 GFLOPS</td>\n",
       "      <td>31529243875858664390656.0</td>\n",
       "      <td>2024-06-12 12:09:17.554701056</td>\n",
       "      <td>Wanda 0.75$\\rightarrow$GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>wanda0.75\\%-gd</td>\n",
       "      <td>1.718204e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.246902</td>\n",
       "      <td>0.247447</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>10226426.0</td>\n",
       "      <td>743558.8125</td>\n",
       "      <td>145133.09375</td>\n",
       "      <td>357.84 GFLOPS</td>\n",
       "      <td>128967.648438</td>\n",
       "      <td>2024-06-12 14:55:21.376342784</td>\n",
       "      <td>Wanda 0.75$\\rightarrow$GD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>wanda0.75\\%-rmu</td>\n",
       "      <td>1.717030e+09</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.231769</td>\n",
       "      <td>0.250982</td>\n",
       "      <td>...</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>391.873352</td>\n",
       "      <td>151281.125</td>\n",
       "      <td>3917.281982</td>\n",
       "      <td>581.18 GFLOPS</td>\n",
       "      <td>3727.288574</td>\n",
       "      <td>2024-05-22 05:36:22.919470336</td>\n",
       "      <td>Wanda 0.75$\\rightarrow$RMU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tag    _timestamp   edit_set  number_of_edits  rmu_layer_id  \\\n",
       "0       AWQ2bit-to-ft  1.718168e+09   9.545455             50.0     -1.000000   \n",
       "1     AWQ2bit-to-lora  1.718359e+09   5.500000             50.0     -1.000000   \n",
       "2    AWQ2bit-to-memit  1.718128e+09  10.666667             50.0     -1.000000   \n",
       "3       AWQ3bit-to-ft  1.718614e+09   5.500000             50.0     -1.000000   \n",
       "4     AWQ3bit-to-lora  1.718611e+09   5.500000             50.0     -1.000000   \n",
       "..                ...           ...        ...              ...           ...   \n",
       "306    wanda0.65\\%-gd  1.718204e+09   1.000000             50.0     -1.000000   \n",
       "307   wanda0.65\\%-rmu  1.716764e+09  18.666667             50.0      3.666667   \n",
       "308    wanda0.75\\%-ga  1.718194e+09   1.000000             50.0     -1.000000   \n",
       "309    wanda0.75\\%-gd  1.718204e+09   1.000000             50.0     -1.000000   \n",
       "310   wanda0.75\\%-rmu  1.717030e+09   4.500000             50.0      3.000000   \n",
       "\n",
       "     wbits  sparsity_ratio  qa_question_count_limit_x  mmlu accuracy  \\\n",
       "0      2.0            0.00                        NaN       0.258743   \n",
       "1      2.0            0.00                        NaN       0.262028   \n",
       "2      2.0            0.00                        NaN       0.264049   \n",
       "3      3.0            0.00                        NaN       0.509479   \n",
       "4      3.0            0.00                        NaN       0.510034   \n",
       "..     ...             ...                        ...            ...   \n",
       "306    4.0            0.65                        NaN       0.229098   \n",
       "307    4.0            0.65                        NaN       0.229360   \n",
       "308    4.0            0.75                        NaN       0.229811   \n",
       "309    4.0            0.75                        NaN       0.246902   \n",
       "310    4.0            0.75                        NaN       0.231769   \n",
       "\n",
       "     wmdp_bio accuracy  ...  edit_dataset  compression_dataset  \\\n",
       "0             0.243591  ...          zsre                   c4   \n",
       "1             0.242419  ...          zsre                   c4   \n",
       "2             0.241424  ...          zsre                   c4   \n",
       "3             0.594580  ...          zsre                   c4   \n",
       "4             0.604242  ...          zsre                   c4   \n",
       "..                 ...  ...           ...                  ...   \n",
       "306           0.249018  ...          zsre                   c4   \n",
       "307           0.249542  ...          zsre                   c4   \n",
       "308           0.249018  ...          zsre                   c4   \n",
       "309           0.247447  ...          zsre                   c4   \n",
       "310           0.250982  ...          zsre                   c4   \n",
       "\n",
       "     qa_question_count_limit_y                                      PPL  \\\n",
       "0                         None                               33638.4375   \n",
       "1                         None                             141960.90625   \n",
       "2                         None                               1735678.75   \n",
       "3                         None                                 7.569585   \n",
       "4                         None                                55.437218   \n",
       "..                         ...                                      ...   \n",
       "306                       None                             16268.005859   \n",
       "307                       None                                45.534767   \n",
       "308                       None  8125971838094910145582358934348365824.0   \n",
       "309                       None                               10226426.0   \n",
       "310                       None                               391.873352   \n",
       "\n",
       "                         PPL edits                     PPl QA          FLOPs  \\\n",
       "0                     338052.34375              102475.617188             -1   \n",
       "1                     37042.675781                267692.4375             -1   \n",
       "2                      996271.5625                1198751.125             -1   \n",
       "3                     50514.761719                 627.454346             -1   \n",
       "4                     334348.21875                9609.279297             -1   \n",
       "..                             ...                        ...            ...   \n",
       "306                      5423145.5                 81961160.0  447.25 GFLOPS   \n",
       "307                   83373.453125                1477.523804     760 GFLOPS   \n",
       "308  176520989218511062933962752.0  31936578699555629957120.0  357.84 GFLOPS   \n",
       "309                    743558.8125               145133.09375  357.84 GFLOPS   \n",
       "310                     151281.125                3917.281982  581.18 GFLOPS   \n",
       "\n",
       "            PPl edits unmasked                        date_y  \\\n",
       "0                 78554.226562 2024-05-20 17:38:54.680141568   \n",
       "1                 93121.015625 2024-06-14 09:58:09.475097088   \n",
       "2                  1074956.375 2024-05-20 17:01:28.464071680   \n",
       "3                   945.212585 2024-06-17 08:37:00.186556416   \n",
       "4                      16729.5 2024-06-17 07:55:00.258938112   \n",
       "..                         ...                           ...   \n",
       "306                 89329664.0 2024-06-12 14:53:38.419730176   \n",
       "307                1549.510742 2024-05-20 19:27:06.522696448   \n",
       "308  31529243875858664390656.0 2024-06-12 12:09:17.554701056   \n",
       "309              128967.648438 2024-06-12 14:55:21.376342784   \n",
       "310                3727.288574 2024-05-22 05:36:22.919470336   \n",
       "\n",
       "                             Label  \n",
       "0       AWQ (2bit) $\\rightarrow$FT  \n",
       "1     AWQ (2bit) $\\rightarrow$LoRA  \n",
       "2    AWQ (2bit) $\\rightarrow$MEMIT  \n",
       "3       AWQ (3bit) $\\rightarrow$FT  \n",
       "4     AWQ (3bit) $\\rightarrow$LoRA  \n",
       "..                             ...  \n",
       "306      Wanda 0.65$\\rightarrow$GD  \n",
       "307     Wanda 0.65$\\rightarrow$RMU  \n",
       "308      Wanda 0.75$\\rightarrow$GA  \n",
       "309      Wanda 0.75$\\rightarrow$GD  \n",
       "310     Wanda 0.75$\\rightarrow$RMU  \n",
       "\n",
       "[311 rows x 56 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "technique_formatting_map = {\n",
    "    \"awq\": \"AWQ\",\n",
    "    \"gptq\": \"GPTQ\",\n",
    "    \"sparsegpt\": \"SparseGPT\",\n",
    "    \"wanda\": \"Wanda\",\n",
    "    \"ft\": \"FT\",\n",
    "    \"memit\": \"MEMIT\",\n",
    "    \"lora\": \"LoRA\",\n",
    "    \"ga\": \"GA\",\n",
    "    \"gd\": \"GD\",\n",
    "    \"rmu\": \"RMU\",\n",
    "}\n",
    "appendix_compositions_order = [\n",
    "    [],\n",
    "    [\"compress\"],\n",
    "    [\"edit\"],\n",
    "    [\"edit\", \"compress\"],\n",
    "    [\"compress\", \"edit\"],\n",
    "    [\"unlearn\"],\n",
    "    [\"unlearn\", \"compress\"],\n",
    "    [\"compress\", \"unlearn\"],\n",
    "    [\"edit\", \"unlearn\"],\n",
    "    [\"unlearn\", \"edit\"],\n",
    "]\n",
    "appendix_table_columns_map = {\n",
    "    \"tag\": \"Composition\",\n",
    "    \"Rewrite accuracy\": \"Edit Success\",\n",
    "    \"Generalization\": \"Generalization\",\n",
    "    \"Locality\": \"Locality\",\n",
    "    \"Average bits\": \"Avg. Bits\",\n",
    "    \"Avg WMDP\": \"Avg. WMDP\",\n",
    "    \"mmlu accuracy\": \"MMLU\",\n",
    "    \"PPL\": \"WikiText PPL\",\n",
    "}\n",
    "appendix_technique_ordering = {\n",
    "    \"edit\": [\"Fine-tune\", \"MEMIT\", \"LoRA\"],\n",
    "    \"compress\": [\"SparseGPT\", \"Wanda\", \"GPTQ\", \"AWQ\"],\n",
    "    \"unlearn\": [\"GA\", \"GD\", \"RMU\"],\n",
    "}\n",
    "\n",
    "\n",
    "def get_composition_label(row):\n",
    "    composition = row[\"interventions\"]\n",
    "    if composition == []:\n",
    "        return \"None\"\n",
    "    \n",
    "    first_intervention_type = composition[0] if composition[0] != \"compress\" else \"compression\"\n",
    "    first_intervention = technique_formatting_map[row[first_intervention_type]]\n",
    "    if first_intervention in [\"SparseGPT\", \"Wanda\"]:\n",
    "        first_intervention += \" \" + str(row[\"sparsity_ratio\"])\n",
    "    elif first_intervention in [\"GPTQ\", \"AWQ\"]:\n",
    "        first_intervention += \" (\" + str(int(row[\"wbits\"])) + \"bit) \"\n",
    "    \n",
    "    if len(composition) == 1:\n",
    "        return first_intervention\n",
    "    \n",
    "    second_intervention_type = composition[1] if composition[1] != \"compress\" else \"compression\"\n",
    "    second_intervention = technique_formatting_map[row[second_intervention_type]]\n",
    "    if second_intervention in [\"SparseGPT\", \"Wanda\"]:\n",
    "        second_intervention += \" \" + str(row[\"sparsity_ratio\"])\n",
    "    elif second_intervention in [\"GPTQ\", \"AWQ\"]:\n",
    "        second_intervention += \" (\" + str(int(row[\"wbits\"])) + \"bit) \"\n",
    "    \n",
    "    return first_intervention + r\"$\\rightarrow$\" + second_intervention\n",
    "\n",
    "\n",
    "appendix_results = all_runs_df_sorted_averaged.copy()\n",
    "appendix_results[\"interventions\"] = appendix_results[\"interventions\"].apply(lambda x : ast.literal_eval(x))\n",
    "appendix_results[\"Label\"] = appendix_results.apply(get_composition_label, axis=1)\n",
    "appendix_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tag', '_timestamp', 'edit_set', 'number_of_edits', 'rmu_layer_id',\n",
       "       'wbits', 'sparsity_ratio', 'qa_question_count_limit_x', 'mmlu accuracy',\n",
       "       'wmdp_bio accuracy', 'wmdp_cyber accuracy', 'Generalization',\n",
       "       'Success recall', 'Generalization recall', 'Locality', 'Average bits',\n",
       "       'Rewrite accuracy', 'Local recall', 'Latency', 'date_x', 'Avg WMDP',\n",
       "       '_timestamp_se', 'edit_set_se', 'number_of_edits_se', 'rmu_layer_id_se',\n",
       "       'wbits_se', 'sparsity_ratio_se', 'qa_question_count_limit_se',\n",
       "       'mmlu accuracy_se', 'wmdp_bio accuracy_se', 'wmdp_cyber accuracy_se',\n",
       "       'Generalization_se', 'Success recall_se', 'Generalization recall_se',\n",
       "       'Locality_se', 'Average bits_se', 'Rewrite accuracy_se',\n",
       "       'Local recall_se', 'Latency_se', 'date_se', 'Avg WMDP_se',\n",
       "       'interventions', 'edit', 'unlearn', 'compression', 'model_name',\n",
       "       'edit_dataset', 'compression_dataset', 'qa_question_count_limit_y',\n",
       "       'PPL', 'PPL edits', 'PPl QA', 'FLOPs', 'PPl edits unmasked', 'date_y',\n",
       "       'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appendix_results.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Table: Single Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rewrite accuracy',\n",
       " 'Generalization',\n",
       " 'Locality',\n",
       " 'Average bits',\n",
       " 'Avg WMDP',\n",
       " 'mmlu accuracy',\n",
       " 'PPL']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(appendix_table_columns_map.keys())[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
      "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
      "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
      "    \\midrule\n",
      "   {None} & 0.02 & 0.02 & 0.04 & 16.00 & 0.58 & 0.62 & 5.54 \\\\\n",
      "    \\cdashlinelr{1-9}\n",
      "    {FT}$\\rightarrow$None& 0.99& 0.82& 0.11& 16.0& 0.57& 0.61& 5.57 \\\\\n",
      "    {MEMIT}$\\rightarrow$None& 0.89& 0.85& 0.04& 16.0& 0.57& 0.61& 5.57 \\\\\n",
      "    {LoRA}$\\rightarrow$None& 1.0& 0.71& 0.06& 16.0& 0.56& 0.61& 19.25 \\\\\n",
      "    \\cdashlinelr{1-9}\n",
      "    {SparseGPT (0.25) }$\\rightarrow$None & 0.02 & 0.02 & 0.03 & 12.25 & 0.56 & 0.61 & 5.87 \\\\\n",
      "    {SparseGPT (0.35) }$\\rightarrow$None & 0.02 & 0.02 & 0.03 & 10.75 & 0.54 & 0.58 & 6.34 \\\\\n",
      "    {SparseGPT (0.45) }$\\rightarrow$None & 0.02 & 0.01 & 0.03 & 9.25 & 0.51 & 0.54 & 7.43 \\\\\n",
      "    {SparseGPT (0.55) }$\\rightarrow$None & 0.01 & 0.01 & 0.03 & 7.75 & 0.45 & 0.44 & 10.43 \\\\\n",
      "    {SparseGPT (0.65) }$\\rightarrow$None & 0.0 & 0.01 & 0.03 & 6.25 & 0.3 & 0.27 & 20.83 \\\\\n",
      "    {SparseGPT (0.75) }$\\rightarrow$None & 0.01 & 0.01 & 0.03 & 4.75 & 0.26 & 0.23 & 81.84 \\\\\n",
      "    {Wanda (0.25) }$\\rightarrow$None & 0.02 & 0.02 & 0.03 & 12.25 & 0.56 & 0.61 & 5.84 \\\\\n",
      "    {Wanda (0.35) }$\\rightarrow$None & 0.02 & 0.02 & 0.04 & 10.75 & 0.54 & 0.58 & 6.31 \\\\\n",
      "    {Wanda (0.45) }$\\rightarrow$None & 0.02 & 0.02 & 0.03 & 9.25 & 0.5 & 0.52 & 7.52 \\\\\n",
      "    {Wanda (0.55) }$\\rightarrow$None & 0.02 & 0.01 & 0.02 & 7.75 & 0.36 & 0.37 & 12.5 \\\\\n",
      "    {Wanda (0.65) }$\\rightarrow$None & 0.02 & 0.01 & 0.02 & 6.25 & 0.26 & 0.23 & 47.18 \\\\\n",
      "    {Wanda (0.75) }$\\rightarrow$None & 0.02 & 0.03 & 0.03 & 4.75 & 0.26 & 0.23 & 257.46 \\\\\n",
      "    {GPTQ (2-Bit) }$\\rightarrow$None & 0.0 & 0.0 & 0.03 & 2.25 & 0.25 & 0.24 & 3681.23 \\\\\n",
      "    {GPTQ (3-Bit) }$\\rightarrow$None & 0.01 & 0.01 & 0.03 & 3.25 & 0.45 & 0.46 & 8.6 \\\\\n",
      "    {GPTQ (4-Bit) }$\\rightarrow$None & 0.02 & 0.02 & 0.03 & 4.25 & 0.56 & 0.6 & 9.97 \\\\\n",
      "    {GPTQ (8-Bit) }$\\rightarrow$None & 0.02 & 0.02 & 0.04 & 8.25 & 0.58 & 0.62 & 5.54 \\\\\n",
      "    {AWQ (2-Bit) }$\\rightarrow$None & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.27 & 1748954.75 \\\\\n",
      "    {AWQ (3-Bit) }$\\rightarrow$None & 0.01 & 0.01 & 0.03 & 3.25 & 0.52 & 0.53 & 7.47 \\\\\n",
      "    {AWQ (4-Bit) }$\\rightarrow$None & 0.02 & 0.02 & 0.03 & 4.25 & 0.57 & 0.6 & 5.91 \\\\\n",
      "    {AWQ (5-Bit) }$\\rightarrow$None & 0.02 & 0.03 & 0.03 & 5.25 & 0.57 & 0.62 & 5.62 \\\\\n",
      "    {AWQ (6-Bit) }$\\rightarrow$None & 0.02 & 0.03 & 0.04 & 6.25 & 0.58 & 0.62 & 5.56 \\\\\n",
      "    {AWQ (8-Bit) }$\\rightarrow$None & 0.02 & 0.02 & 0.04 & 8.25 & 0.58 & 0.62 & 5.54 \\\\\n",
      "    \\cdashlinelr{1-9}\n",
      "    {GA}$\\rightarrow$None & 0.0 & 0.0 & 0.0 & 16.0 & 0.46 & 0.5 & inf \\\\\n",
      "    {GD}$\\rightarrow$None & 0.02 & 0.0 & 0.09 & 16.0 & 0.26 & 0.52 & 4.48 \\\\\n",
      "    {RMU}$\\rightarrow$None & 0.02 & 0.02 & 0.03 & 16.0 & 0.29 & 0.57 & 5.56 \\\\\n",
      "    \\bottomrule \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "latex_code = r\"\"\"\n",
    "\\begin{tabular}{lcccccccc}\n",
    "    \\toprule\n",
    "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
    "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
    "    \\midrule\n",
    "\"\"\"\n",
    "\n",
    "# None\n",
    "appendix_no_compositons = appendix_results[appendix_results[\"interventions\"].apply(lambda x: len(x) == 0)]\n",
    "latex_code += r\"   {None}\"\n",
    "for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "    latex_code += f\" & {appendix_no_compositons[col].mean():.2f}\"\n",
    "\n",
    "latex_code += r\" \\\\\" + \"\\n\"\n",
    "\n",
    "# Edit Only\n",
    "latex_code += r\"    \\cdashlinelr{1-9}\" + \"\\n\"\n",
    "\n",
    "appendix_edit_only = appendix_results[appendix_results[\"interventions\"].apply(lambda x: x == [\"edit\"])]\n",
    "for edit_technique in appendix_technique_ordering[\"edit\"]:\n",
    "    formatted_edit_technique = \"FT\" if edit_technique == \"Fine-tune\" else edit_technique\n",
    "    appendix_edit_only_technique = appendix_edit_only[appendix_edit_only[\"edit\"] == formatted_edit_technique.lower()]\n",
    "    assert len(appendix_edit_only_technique) > 0, f\"No data found for {edit_technique}\"\n",
    "    technique_row_label = \"FT\" if edit_technique == \"Fine-tune\" else edit_technique\n",
    "    latex_code += f\"    {{{technique_row_label}}}$\\\\rightarrow$None\"\n",
    "    for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "        latex_code += f\"& {round(appendix_edit_only_technique[col].mean(), 2)}\"\n",
    "    \n",
    "    latex_code += r\" \\\\\" + \"\\n\"\n",
    "\n",
    "# Compress Only\n",
    "latex_code += r\"    \\cdashlinelr{1-9}\" + \"\\n\"\n",
    "\n",
    "appendix_compress_only = appendix_results[appendix_results[\"interventions\"].apply(lambda x: x == [\"compress\"])]\n",
    "for compress_technique in appendix_technique_ordering[\"compress\"]:\n",
    "    formatted_compress_technique = compress_technique\n",
    "    appendix_compress_only_technique = appendix_compress_only[appendix_compress_only[\"compression\"] == formatted_compress_technique.lower()]\n",
    "    assert len(appendix_compress_only_technique) > 0, f\"No data found for {compress_technique}\"\n",
    "\n",
    "    compression_strength_column = \"sparsity_ratio\" if compress_technique in [\"SparseGPT\", \"Wanda\"] else \"wbits\"\n",
    "    compression_strength_ordering = sorted(appendix_compress_only_technique[compression_strength_column].unique())\n",
    "    for compression_strength in compression_strength_ordering:\n",
    "        technique_row_label = compress_technique\n",
    "        current_compression = appendix_compress_only_technique[appendix_compress_only_technique[compression_strength_column] == compression_strength]\n",
    "        if compress_technique in [\"SparseGPT\", \"Wanda\"]:\n",
    "            technique_row_label += \" (\" + str(current_compression[\"sparsity_ratio\"].iloc[0]) + \") \"\n",
    "        elif compress_technique in [\"GPTQ\", \"AWQ\"]:\n",
    "            technique_row_label += \" (\" + str(int(current_compression[\"wbits\"].iloc[0])) + \"-Bit) \"\n",
    "        \n",
    "        latex_code += f\"    {{{technique_row_label}}}$\\\\rightarrow$None\"\n",
    "        for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "            latex_code += f\" & {round(current_compression[col].mean(), 2)}\"\n",
    "        \n",
    "        latex_code += r\" \\\\\" + \"\\n\"\n",
    "\n",
    "# Unlearn Only\n",
    "latex_code += r\"    \\cdashlinelr{1-9}\" + \"\\n\"\n",
    "\n",
    "appendix_unlearn_only = appendix_results[appendix_results[\"interventions\"].apply(lambda x: x == [\"unlearn\"])]\n",
    "for unlearn_technique in appendix_technique_ordering[\"unlearn\"]:\n",
    "    formatted_unlearn_technique = unlearn_technique\n",
    "    appendix_unlearn_only_technique = appendix_unlearn_only[appendix_unlearn_only[\"unlearn\"] == formatted_unlearn_technique.lower()]\n",
    "    assert len(appendix_unlearn_only_technique) > 0, f\"No data found for {unlearn_technique}\"\n",
    "    technique_row_label = unlearn_technique\n",
    "    latex_code += f\"    {{{technique_row_label}}}$\\\\rightarrow$None\"\n",
    "    for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "        latex_code += f\" & {round(appendix_unlearn_only_technique[col].mean(), 2)}\"\n",
    "    \n",
    "    latex_code += r\" \\\\\" + \"\\n\"\n",
    "\n",
    "# end of table\n",
    "latex_code += r\"    \\bottomrule \\\\\" + \"\\n\"\n",
    "latex_code += r\"\\end{tabular}\"\n",
    "\n",
    "# Pring the table\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Table: KE ←→ MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
      "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
      "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
      "    \\midrule\n",
      "    {FT}$\\rightarrow${SparseGPT (0.25) } & 0.96 & 0.76 & 0.11 & 12.25 & 0.55 & 0.6 & 5.9 \\\\\n",
      "    {FT}$\\rightarrow${SparseGPT (0.35) } & 0.84 & 0.66 & 0.09 & 10.75 & 0.53 & 0.57 & 6.38 \\\\\n",
      "    {FT}$\\rightarrow${SparseGPT (0.45) } & 0.64 & 0.5 & 0.07 & 9.25 & 0.5 & 0.53 & 7.48 \\\\\n",
      "    {FT}$\\rightarrow${SparseGPT (0.55) } & 0.33 & 0.26 & 0.04 & 7.75 & 0.44 & 0.44 & 10.35 \\\\\n",
      "    {FT}$\\rightarrow${SparseGPT (0.65) } & 0.12 & 0.1 & 0.03 & 6.25 & 0.33 & 0.3 & 21.54 \\\\\n",
      "    {FT}$\\rightarrow${SparseGPT (0.75) } & 0.03 & 0.02 & 0.03 & 4.75 & 0.26 & 0.23 & 88.73 \\\\\n",
      "    {FT}$\\rightarrow${Wanda (0.25) } & 0.96 & 0.76 & 0.11 & 12.25 & 0.56 & 0.6 & 5.88 \\\\\n",
      "    {FT}$\\rightarrow${Wanda (0.35) } & 0.88 & 0.68 & 0.09 & 10.75 & 0.53 & 0.57 & 6.35 \\\\\n",
      "    {FT}$\\rightarrow${Wanda (0.45) } & 0.65 & 0.5 & 0.06 & 9.25 & 0.49 & 0.52 & 7.57 \\\\\n",
      "    {FT}$\\rightarrow${Wanda (0.55) } & 0.31 & 0.24 & 0.04 & 7.75 & 0.35 & 0.37 & 12.48 \\\\\n",
      "    {FT}$\\rightarrow${Wanda (0.65) } & 0.08 & 0.07 & 0.03 & 6.25 & 0.26 & 0.23 & 46.37 \\\\\n",
      "    {FT}$\\rightarrow${Wanda (0.75) } & 0.02 & 0.02 & 0.03 & 4.75 & 0.26 & 0.23 & 344.62 \\\\\n",
      "    {FT}$\\rightarrow${GPTQ (2-Bit) } & 0.0 & 0.0 & 0.02 & 2.25 & 0.25 & 0.24 & 1995.71 \\\\\n",
      "    {FT}$\\rightarrow${GPTQ (3-Bit) } & 0.27 & 0.2 & 0.04 & 3.25 & 0.43 & 0.46 & 9.03 \\\\\n",
      "    {FT}$\\rightarrow${GPTQ (4-Bit) } & 0.79 & 0.6 & 0.11 & 4.25 & 0.54 & 0.59 & 12.93 \\\\\n",
      "    {FT}$\\rightarrow${GPTQ (8-Bit) } & 0.99 & 0.83 & 0.11 & 8.25 & 0.57 & 0.61 & 5.57 \\\\\n",
      "    {FT}$\\rightarrow${AWQ (2-Bit) } & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.27 & 1739216.25 \\\\\n",
      "    {FT}$\\rightarrow${AWQ (3-Bit) } & 0.7 & 0.55 & 0.07 & 3.25 & 0.5 & 0.51 & 7.56 \\\\\n",
      "    {FT}$\\rightarrow${AWQ (4-Bit) } & 0.95 & 0.79 & 0.1 & 4.25 & 0.55 & 0.59 & 5.95 \\\\\n",
      "    {FT}$\\rightarrow${AWQ (5-Bit) } & 0.99 & 0.84 & 0.11 & 5.25 & 0.56 & 0.61 & 5.66 \\\\\n",
      "    {FT}$\\rightarrow${AWQ (6-Bit) } & 0.99 & 0.84 & 0.1 & 6.25 & 0.57 & 0.61 & 5.59 \\\\\n",
      "    {FT}$\\rightarrow${AWQ (8-Bit) } & 0.99 & 0.83 & 0.11 & 8.25 & 0.57 & 0.61 & 5.57 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {MEMIT}$\\rightarrow${SparseGPT (0.25) } & 0.87 & 0.83 & 0.04 & 12.25 & 0.56 & 0.6 & 5.89 \\\\\n",
      "    {MEMIT}$\\rightarrow${SparseGPT (0.35) } & 0.81 & 0.74 & 0.03 & 10.75 & 0.54 & 0.57 & 6.4 \\\\\n",
      "    {MEMIT}$\\rightarrow${SparseGPT (0.45) } & 0.62 & 0.56 & 0.04 & 9.25 & 0.51 & 0.53 & 7.43 \\\\\n",
      "    {MEMIT}$\\rightarrow${SparseGPT (0.55) } & 0.28 & 0.2 & 0.03 & 7.75 & 0.43 & 0.43 & 10.37 \\\\\n",
      "    {MEMIT}$\\rightarrow${SparseGPT (0.65) } & 0.03 & 0.03 & 0.03 & 6.25 & 0.3 & 0.29 & 21.02 \\\\\n",
      "    {MEMIT}$\\rightarrow${SparseGPT (0.75) } & 0.01 & 0.01 & 0.02 & 4.75 & 0.26 & 0.23 & 71.62 \\\\\n",
      "    {MEMIT}$\\rightarrow${Wanda (0.25) } & 0.7 & 0.69 & 0.03 & 12.25 & 0.54 & 0.59 & 5.87 \\\\\n",
      "    {MEMIT}$\\rightarrow${Wanda (0.35) } & 0.85 & 0.78 & 0.03 & 10.75 & 0.53 & 0.57 & 6.34 \\\\\n",
      "    {MEMIT}$\\rightarrow${Wanda (0.45) } & 0.75 & 0.67 & 0.03 & 9.25 & 0.48 & 0.51 & 7.56 \\\\\n",
      "    {MEMIT}$\\rightarrow${Wanda (0.55) } & 0.32 & 0.26 & 0.03 & 7.75 & 0.35 & 0.36 & 12.33 \\\\\n",
      "    {MEMIT}$\\rightarrow${Wanda (0.65) } & 0.01 & 0.01 & 0.02 & 6.25 & 0.26 & 0.23 & 46.98 \\\\\n",
      "    {MEMIT}$\\rightarrow${Wanda (0.75) } & 0.02 & 0.02 & 0.03 & 4.75 & 0.26 & 0.23 & 290.27 \\\\\n",
      "    {MEMIT}$\\rightarrow${GPTQ (2-Bit) } & 0.0 & 0.0 & 0.02 & 2.25 & 0.25 & 0.24 & 1663.86 \\\\\n",
      "    {MEMIT}$\\rightarrow${GPTQ (3-Bit) } & 0.35 & 0.29 & 0.04 & 3.25 & 0.45 & 0.46 & 8.89 \\\\\n",
      "    {MEMIT}$\\rightarrow${GPTQ (4-Bit) } & 0.84 & 0.8 & 0.03 & 4.25 & 0.55 & 0.59 & 13.17 \\\\\n",
      "    {MEMIT}$\\rightarrow${GPTQ (8-Bit) } & 0.94 & 0.9 & 0.04 & 8.25 & 0.57 & 0.61 & 5.56 \\\\\n",
      "    {MEMIT}$\\rightarrow${AWQ (2-Bit) } & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.27 & 1738085.5 \\\\\n",
      "    {MEMIT}$\\rightarrow${AWQ (3-Bit) } & 0.83 & 0.79 & 0.04 & 3.25 & 0.5 & 0.5 & 7.54 \\\\\n",
      "    {MEMIT}$\\rightarrow${AWQ (4-Bit) } & 0.93 & 0.89 & 0.03 & 4.25 & 0.55 & 0.59 & 5.94 \\\\\n",
      "    {MEMIT}$\\rightarrow${AWQ (5-Bit) } & 0.92 & 0.89 & 0.03 & 5.25 & 0.57 & 0.61 & 5.65 \\\\\n",
      "    {MEMIT}$\\rightarrow${AWQ (6-Bit) } & 0.93 & 0.89 & 0.04 & 6.25 & 0.57 & 0.61 & 5.59 \\\\\n",
      "    {MEMIT}$\\rightarrow${AWQ (8-Bit) } & 0.94 & 0.9 & 0.04 & 8.25 & 0.57 & 0.61 & 5.56 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {LoRA}$\\rightarrow${SparseGPT (0.25) } & 0.86 & 0.52 & 0.04 & 12.25 & 0.55 & 0.6 & 13.81 \\\\\n",
      "    {LoRA}$\\rightarrow${SparseGPT (0.35) } & 0.62 & 0.37 & 0.05 & 10.75 & 0.52 & 0.57 & 10.6 \\\\\n",
      "    {LoRA}$\\rightarrow${SparseGPT (0.45) } & 0.36 & 0.22 & 0.04 & 9.25 & 0.48 & 0.52 & 16.48 \\\\\n",
      "    {LoRA}$\\rightarrow${SparseGPT (0.55) } & 0.17 & 0.14 & 0.04 & 7.75 & 0.43 & 0.43 & 23.46 \\\\\n",
      "    {LoRA}$\\rightarrow${SparseGPT (0.65) } & 0.07 & 0.06 & 0.03 & 6.25 & 0.32 & 0.3 & 53.89 \\\\\n",
      "    {LoRA}$\\rightarrow${SparseGPT (0.75) } & 0.03 & 0.03 & 0.02 & 4.75 & 0.26 & 0.25 & 186.65 \\\\\n",
      "    {LoRA}$\\rightarrow${Wanda (0.25) } & 0.87 & 0.53 & 0.04 & 12.25 & 0.55 & 0.6 & 9.23 \\\\\n",
      "    {LoRA}$\\rightarrow${Wanda (0.35) } & 0.64 & 0.4 & 0.05 & 10.75 & 0.52 & 0.57 & 11.01 \\\\\n",
      "    {LoRA}$\\rightarrow${Wanda (0.45) } & 0.37 & 0.25 & 0.03 & 9.25 & 0.46 & 0.49 & 12.6 \\\\\n",
      "    {LoRA}$\\rightarrow${Wanda (0.55) } & 0.18 & 0.14 & 0.03 & 7.75 & 0.36 & 0.36 & 33.59 \\\\\n",
      "    {LoRA}$\\rightarrow${Wanda (0.65) } & 0.05 & 0.04 & 0.02 & 6.25 & 0.26 & 0.24 & 194.26 \\\\\n",
      "    {LoRA}$\\rightarrow${Wanda (0.75) } & 0.02 & 0.02 & 0.03 & 4.75 & 0.25 & 0.25 & 914.43 \\\\\n",
      "    {LoRA}$\\rightarrow${GPTQ (2-Bit) } & 0.0 & 0.0 & 0.01 & 2.25 & 0.25 & 0.24 & 10465.63 \\\\\n",
      "    {LoRA}$\\rightarrow${GPTQ (3-Bit) } & 0.08 & 0.06 & 0.02 & 3.25 & 0.4 & 0.42 & 40.02 \\\\\n",
      "    {LoRA}$\\rightarrow${GPTQ (4-Bit) } & 0.58 & 0.41 & 0.05 & 4.25 & 0.54 & 0.58 & 63.45 \\\\\n",
      "    {LoRA}$\\rightarrow${GPTQ (8-Bit) } & 1.0 & 0.71 & 0.06 & 8.25 & 0.57 & 0.61 & 18.86 \\\\\n",
      "    {LoRA}$\\rightarrow${AWQ (2-Bit) } & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.25 & 1764088.0 \\\\\n",
      "    {LoRA}$\\rightarrow${AWQ (3-Bit) } & 0.46 & 0.31 & 0.05 & 3.25 & 0.5 & 0.51 & 24.29 \\\\\n",
      "    {LoRA}$\\rightarrow${AWQ (4-Bit) } & 0.91 & 0.6 & 0.04 & 4.25 & 0.55 & 0.59 & 16.37 \\\\\n",
      "    {LoRA}$\\rightarrow${AWQ (5-Bit) } & 0.99 & 0.7 & 0.05 & 5.25 & 0.56 & 0.6 & 16.31 \\\\\n",
      "    {LoRA}$\\rightarrow${AWQ (6-Bit) } & 1.0 & 0.71 & 0.06 & 6.25 & 0.56 & 0.61 & 15.34 \\\\\n",
      "    {LoRA}$\\rightarrow${AWQ (8-Bit) } & 1.0 & 0.72 & 0.06 & 8.25 & 0.57 & 0.61 & 15.25 \\\\\n",
      "    \\bottomrule \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "latex_code = r\"\"\"\n",
    "\\begin{tabular}{lcccccccc}\n",
    "    \\toprule\n",
    "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
    "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
    "    \\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Editing -> Compression\n",
    "appendix_edit_compress = appendix_results[appendix_results[\"interventions\"].apply(lambda x: x == [\"edit\", \"compress\"])]\n",
    "for edit_technique in appendix_technique_ordering[\"edit\"]:\n",
    "    formatted_edit_technique = \"FT\" if edit_technique == \"Fine-tune\" else edit_technique\n",
    "    appendix_edit_compress_edit_technique = appendix_edit_compress[appendix_edit_compress[\"edit\"] == formatted_edit_technique.lower()]\n",
    "    assert len(appendix_edit_compress_edit_technique) > 0, f\"No data found for {edit_technique}\"\n",
    "    for compress_technique in appendix_technique_ordering[\"compress\"]:\n",
    "        formatted_compress_technique = compress_technique\n",
    "        appendix_edit_compress_technique_frame = appendix_edit_compress_edit_technique[appendix_edit_compress_edit_technique[\"compression\"] == formatted_compress_technique.lower()]\n",
    "        assert len(appendix_edit_compress_technique_frame) > 0, f\"No data found for {compress_technique}\"\n",
    "        compression_strength_column = \"sparsity_ratio\" if compress_technique in [\"SparseGPT\", \"Wanda\"] else \"wbits\"\n",
    "        compression_strength_ordering = sorted(set([round(strength, 2) for strength in appendix_edit_compress_technique_frame[compression_strength_column] if strength not in [0, 16]]))\n",
    "        for compression_strength in compression_strength_ordering:\n",
    "            technique_row_label = compress_technique\n",
    "            current_compression = appendix_edit_compress_technique_frame[appendix_edit_compress_technique_frame[compression_strength_column] == compression_strength]\n",
    "            if compress_technique in [\"SparseGPT\", \"Wanda\"]:\n",
    "                technique_row_label += \" (\" + str(current_compression[\"sparsity_ratio\"].iloc[0]) + \") \"\n",
    "            elif compress_technique in [\"GPTQ\", \"AWQ\"]:\n",
    "                technique_row_label += \" (\" + str(int(current_compression[\"wbits\"].iloc[0])) + \"-Bit) \"\n",
    "            \n",
    "            latex_code += f\"    {{{formatted_edit_technique}}}$\\\\rightarrow${{{technique_row_label}}}\"\n",
    "            for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "                latex_code += f\" & {round(current_compression[col].mean(), 2)}\"\n",
    "            \n",
    "            latex_code += r\" \\\\\" + \"\\n\"\n",
    "    \n",
    "    if edit_technique != appendix_technique_ordering[\"edit\"][-1]:\n",
    "        latex_code += r\"    \\cdashlinelr{1-8}\" + \"\\n\"\n",
    "\n",
    "# end of table\n",
    "latex_code += r\"    \\bottomrule \\\\\" + \"\\n\"\n",
    "latex_code += r\"\\end{tabular}\"\n",
    "\n",
    "# Pring the table\n",
    "print(latex_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technique: SparseGPT, Strengths: [0.25, 0.35, 0.45, 0.55, 0.65, 0.75]\n",
      "Technique: Wanda, Strengths: [0.25, 0.35, 0.45, 0.55, 0.65, 0.75]\n",
      "Technique: GPTQ, Strengths: [2.0, 3.0, 4.0, 8.0]\n",
      "Technique: AWQ, Strengths: [2.0, 3.0, 4.0, 5.0, 6.0, 8.0]\n",
      "\n",
      "\\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
      "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
      "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
      "    \\midrule\n",
      "    {SparseGPT (0.25) }$\\rightarrow${FT} & 0.99 & 0.77 & 0.08 & 12.25 & 0.55 & 0.6 & 5.91 \\\\\n",
      "    {SparseGPT (0.35) }$\\rightarrow${FT} & 0.98 & 0.75 & 0.06 & 10.75 & 0.54 & 0.58 & 6.37 \\\\\n",
      "    {SparseGPT (0.45) }$\\rightarrow${FT} & 0.96 & 0.71 & 0.06 & 9.25 & 0.51 & 0.54 & 7.49 \\\\\n",
      "    {SparseGPT (0.55) }$\\rightarrow${FT} & 0.95 & 0.66 & 0.05 & 7.75 & 0.44 & 0.44 & 10.19 \\\\\n",
      "    {SparseGPT (0.65) }$\\rightarrow${FT} & 0.89 & 0.54 & 0.04 & 6.25 & 0.33 & 0.3 & 17.67 \\\\\n",
      "    {SparseGPT (0.75) }$\\rightarrow${FT} & 0.58 & 0.25 & 0.04 & 4.75 & 0.26 & 0.24 & 98.18 \\\\\n",
      "    {SparseGPT (0.25) }$\\rightarrow${MEMIT} & 0.91 & 0.86 & 0.04 & 12.25 & 0.56 & 0.59 & 5.89 \\\\\n",
      "    {SparseGPT (0.35) }$\\rightarrow${MEMIT} & 0.89 & 0.8 & 0.03 & 10.75 & 0.54 & 0.57 & 6.38 \\\\\n",
      "    {SparseGPT (0.45) }$\\rightarrow${MEMIT} & 0.62 & 0.48 & 0.03 & 9.25 & 0.51 & 0.53 & 7.49 \\\\\n",
      "    {SparseGPT (0.55) }$\\rightarrow${MEMIT} & 0.37 & 0.27 & 0.03 & 7.75 & 0.44 & 0.43 & 10.62 \\\\\n",
      "    {SparseGPT (0.65) }$\\rightarrow${MEMIT} & 0.18 & 0.13 & 0.03 & 6.25 & 0.32 & 0.29 & 22.83 \\\\\n",
      "    {SparseGPT (0.75) }$\\rightarrow${MEMIT} & 0.11 & 0.08 & 0.04 & 4.75 & 0.25 & 0.23 & 394.59 \\\\\n",
      "    {SparseGPT (0.25) }$\\rightarrow${LoRA} & 0.85 & 0.45 & 0.04 & 12.25 & 0.54 & 0.6 & 10.69 \\\\\n",
      "    {SparseGPT (0.35) }$\\rightarrow${LoRA} & 0.76 & 0.42 & 0.04 & 10.75 & 0.52 & 0.57 & 9.54 \\\\\n",
      "    {SparseGPT (0.45) }$\\rightarrow${LoRA} & 0.64 & 0.33 & 0.04 & 9.25 & 0.48 & 0.52 & 10.52 \\\\\n",
      "    {SparseGPT (0.55) }$\\rightarrow${LoRA} & 0.55 & 0.29 & 0.03 & 7.75 & 0.43 & 0.43 & 11.5 \\\\\n",
      "    {SparseGPT (0.65) }$\\rightarrow${LoRA} & 0.37 & 0.17 & 0.03 & 6.25 & 0.32 & 0.29 & 23.13 \\\\\n",
      "    {SparseGPT (0.75) }$\\rightarrow${LoRA} & 0.08 & 0.06 & 0.03 & 4.75 & 0.26 & 0.25 & 89.47 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {Wanda (0.25) }$\\rightarrow${FT} & 0.99 & 0.8 & 0.07 & 12.25 & 0.56 & 0.6 & 5.86 \\\\\n",
      "    {Wanda (0.35) }$\\rightarrow${FT} & 0.97 & 0.76 & 0.05 & 10.75 & 0.54 & 0.58 & 6.32 \\\\\n",
      "    {Wanda (0.45) }$\\rightarrow${FT} & 0.94 & 0.69 & 0.04 & 9.25 & 0.49 & 0.52 & 7.52 \\\\\n",
      "    {Wanda (0.55) }$\\rightarrow${FT} & 0.8 & 0.5 & 0.03 & 7.75 & 0.36 & 0.38 & 11.98 \\\\\n",
      "    {Wanda (0.65) }$\\rightarrow${FT} & 0.5 & 0.28 & 0.03 & 6.25 & 0.26 & 0.23 & 41.41 \\\\\n",
      "    {Wanda (0.75) }$\\rightarrow${FT} & 0.05 & 0.04 & 0.03 & 4.75 & 0.26 & 0.23 & 249.29 \\\\\n",
      "    {Wanda (0.25) }$\\rightarrow${MEMIT} & 0.95 & 0.9 & 0.03 & 12.25 & 0.56 & 0.59 & 5.87 \\\\\n",
      "    {Wanda (0.35) }$\\rightarrow${MEMIT} & 0.86 & 0.76 & 0.03 & 10.75 & 0.54 & 0.57 & 6.34 \\\\\n",
      "    {Wanda (0.45) }$\\rightarrow${MEMIT} & 0.72 & 0.59 & 0.03 & 9.25 & 0.49 & 0.51 & 7.54 \\\\\n",
      "    {Wanda (0.55) }$\\rightarrow${MEMIT} & 0.37 & 0.26 & 0.03 & 7.75 & 0.37 & 0.37 & 12.07 \\\\\n",
      "    {Wanda (0.65) }$\\rightarrow${MEMIT} & 0.04 & 0.04 & 0.02 & 6.25 & 0.26 & 0.23 & 40.49 \\\\\n",
      "    {Wanda (0.75) }$\\rightarrow${MEMIT} & 0.04 & 0.04 & 0.02 & 4.75 & 0.26 & 0.23 & 280.04 \\\\\n",
      "    {Wanda (0.25) }$\\rightarrow${LoRA} & 0.95 & 0.54 & 0.03 & 12.25 & 0.55 & 0.6 & 10.65 \\\\\n",
      "    {Wanda (0.35) }$\\rightarrow${LoRA} & 0.86 & 0.48 & 0.04 & 10.75 & 0.52 & 0.57 & 7.98 \\\\\n",
      "    {Wanda (0.45) }$\\rightarrow${LoRA} & 0.74 & 0.43 & 0.04 & 9.25 & 0.47 & 0.5 & 9.18 \\\\\n",
      "    {Wanda (0.55) }$\\rightarrow${LoRA} & 0.68 & 0.37 & 0.03 & 7.75 & 0.36 & 0.37 & 15.01 \\\\\n",
      "    {Wanda (0.65) }$\\rightarrow${LoRA} & 0.34 & 0.19 & 0.02 & 6.25 & 0.26 & 0.23 & 57.63 \\\\\n",
      "    {Wanda (0.75) }$\\rightarrow${LoRA} & 0.06 & 0.06 & 0.03 & 4.75 & 0.26 & 0.23 & 318.06 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {GPTQ (2-Bit) }$\\rightarrow${FT} & 0.0 & 0.0 & 0.01 & 2.25 & 0.25 & 0.24 & 970816.25 \\\\\n",
      "    {GPTQ (3-Bit) }$\\rightarrow${FT} & 0.1 & 0.08 & 0.03 & 3.25 & 0.41 & 0.42 & 9.96 \\\\\n",
      "    {GPTQ (4-Bit) }$\\rightarrow${FT} & 0.44 & 0.24 & 0.04 & 4.25 & 0.55 & 0.58 & 6.4 \\\\\n",
      "    {GPTQ (8-Bit) }$\\rightarrow${FT} & 0.99 & 0.83 & 0.11 & 8.25 & 0.57 & 0.61 & 5.57 \\\\\n",
      "    {GPTQ (2-Bit) }$\\rightarrow${MEMIT} & 0.0 & 0.0 & 0.01 & 2.25 & 0.25 & 0.24 & 739248.62 \\\\\n",
      "    {GPTQ (3-Bit) }$\\rightarrow${MEMIT} & 0.12 & 0.1 & 0.03 & 3.25 & 0.41 & 0.42 & 9.81 \\\\\n",
      "    {GPTQ (4-Bit) }$\\rightarrow${MEMIT} & 0.81 & 0.69 & 0.04 & 4.25 & 0.55 & 0.58 & 6.38 \\\\\n",
      "    {GPTQ (8-Bit) }$\\rightarrow${MEMIT} & 0.95 & 0.91 & 0.03 & 8.25 & 0.57 & 0.61 & 5.57 \\\\\n",
      "    {GPTQ (2-Bit) }$\\rightarrow${LoRA} & 0.0 & 0.0 & 0.01 & 2.25 & 0.25 & 0.24 & 695958.19 \\\\\n",
      "    {GPTQ (3-Bit) }$\\rightarrow${LoRA} & 0.01 & 0.02 & 0.02 & 3.25 & 0.37 & 0.38 & 11.14 \\\\\n",
      "    {GPTQ (4-Bit) }$\\rightarrow${LoRA} & 0.21 & 0.14 & 0.05 & 4.25 & 0.53 & 0.57 & 9.54 \\\\\n",
      "    {GPTQ (8-Bit) }$\\rightarrow${LoRA} & 1.0 & 0.72 & 0.06 & 8.25 & 0.56 & 0.61 & 19.99 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {AWQ (2-Bit) }$\\rightarrow${FT} & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.26 & 33638.44 \\\\\n",
      "    {AWQ (3-Bit) }$\\rightarrow${FT} & 1.0 & 0.82 & 0.08 & 3.25 & 0.5 & 0.51 & 7.57 \\\\\n",
      "    {AWQ (4-Bit) }$\\rightarrow${FT} & 0.99 & 0.84 & 0.15 & 4.25 & 0.54 & 0.57 & 5.98 \\\\\n",
      "    {AWQ (5-Bit) }$\\rightarrow${FT} & 0.99 & 0.85 & 0.12 & 5.25 & 0.55 & 0.6 & 5.68 \\\\\n",
      "    {AWQ (6-Bit) }$\\rightarrow${FT} & 0.99 & 0.88 & 0.13 & 6.25 & 0.56 & 0.6 & 5.61 \\\\\n",
      "    {AWQ (8-Bit) }$\\rightarrow${FT} & 0.99 & 0.87 & 0.13 & 8.25 & 0.56 & 0.6 & 5.6 \\\\\n",
      "    {AWQ (2-Bit) }$\\rightarrow${MEMIT} & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.26 & 1735678.75 \\\\\n",
      "    {AWQ (3-Bit) }$\\rightarrow${MEMIT} & 0.92 & 0.87 & 0.03 & 3.25 & 0.49 & 0.47 & 7.57 \\\\\n",
      "    {AWQ (4-Bit) }$\\rightarrow${MEMIT} & 0.92 & 0.89 & 0.04 & 4.25 & 0.55 & 0.58 & 5.97 \\\\\n",
      "    {AWQ (5-Bit) }$\\rightarrow${MEMIT} & 0.98 & 0.96 & 0.03 & 5.25 & 0.56 & 0.6 & 5.68 \\\\\n",
      "    {AWQ (6-Bit) }$\\rightarrow${MEMIT} & 0.98 & 0.92 & 0.04 & 6.25 & 0.57 & 0.6 & 5.62 \\\\\n",
      "    {AWQ (8-Bit) }$\\rightarrow${MEMIT} & 0.98 & 0.94 & 0.04 & 8.25 & 0.57 & 0.6 & 5.59 \\\\\n",
      "    {AWQ (2-Bit) }$\\rightarrow${LoRA} & 0.06 & 0.03 & 0.0 & 2.25 & 0.24 & 0.26 & 141960.91 \\\\\n",
      "    {AWQ (3-Bit) }$\\rightarrow${LoRA} & 1.0 & 0.71 & 0.05 & 3.25 & 0.5 & 0.51 & 55.44 \\\\\n",
      "    {AWQ (4-Bit) }$\\rightarrow${LoRA} & 1.0 & 0.74 & 0.07 & 4.25 & 0.55 & 0.58 & 29.71 \\\\\n",
      "    {AWQ (5-Bit) }$\\rightarrow${LoRA} & 1.0 & 0.69 & 0.05 & 5.25 & 0.56 & 0.6 & 17.32 \\\\\n",
      "    {AWQ (6-Bit) }$\\rightarrow${LoRA} & 1.0 & 0.69 & 0.06 & 6.25 & 0.57 & 0.61 & 16.6 \\\\\n",
      "    {AWQ (8-Bit) }$\\rightarrow${LoRA} & 1.0 & 0.73 & 0.06 & 8.25 & 0.57 & 0.61 & 21.64 \\\\\n",
      "    \\bottomrule \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "latex_code = r\"\"\"\n",
    "\\begin{tabular}{lcccccccc}\n",
    "    \\toprule\n",
    "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
    "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
    "    \\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Editing -> Compression\n",
    "appendix_compress_edit = appendix_results[appendix_results[\"interventions\"].apply(lambda x: x == [\"compress\", \"edit\"])]\n",
    "for compress_technique in appendix_technique_ordering[\"compress\"]:\n",
    "    appendix_compress_edit_technique_frame = appendix_compress_edit[appendix_compress_edit[\"compression\"] == compress_technique.lower()]\n",
    "    assert len(appendix_compress_edit_technique_frame) > 0, f\"No data found for {compress_technique}\"\n",
    "\n",
    "    compression_strength_column = \"sparsity_ratio\" if compress_technique in [\"SparseGPT\", \"Wanda\"] else \"wbits\"\n",
    "    compression_strength_ordering = sorted(set([round(strength, 2) for strength in appendix_compress_edit_technique_frame[compression_strength_column] if strength not in [0, 16]]))\n",
    "    print(f\"Technique: {compress_technique}, Strengths: {compression_strength_ordering}\")\n",
    "\n",
    "    for edit_technique in appendix_technique_ordering[\"edit\"]:\n",
    "        formatted_edit_technique = \"FT\" if edit_technique == \"Fine-tune\" else edit_technique\n",
    "        appendix_compress_edit_edit_technique = appendix_compress_edit_technique_frame[appendix_compress_edit_technique_frame[\"edit\"] == formatted_edit_technique.lower()]\n",
    "        assert len(appendix_compress_edit_edit_technique) > 0, f\"No data found for {edit_technique}\"\n",
    "        \n",
    "        for compression_strength in compression_strength_ordering:\n",
    "            technique_row_label = compress_technique\n",
    "            current_compression = appendix_compress_edit_edit_technique[round(appendix_compress_edit_edit_technique[compression_strength_column], 2) == compression_strength]\n",
    "            if compress_technique in [\"SparseGPT\", \"Wanda\"]:\n",
    "                technique_row_label += \" (\" + str(compression_strength) + \") \"\n",
    "            elif compress_technique in [\"GPTQ\", \"AWQ\"]:\n",
    "                technique_row_label += \" (\" + str(int(compression_strength)) + \"-Bit) \"\n",
    "            \n",
    "            latex_code += f\"    {{{technique_row_label}}}$\\\\rightarrow${{{formatted_edit_technique}}}\"\n",
    "            for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "                latex_code += f\" & {round(current_compression[col].mean(), 2)}\"\n",
    "            \n",
    "            latex_code += r\" \\\\\" + \"\\n\"\n",
    "    \n",
    "    if compress_technique != appendix_technique_ordering[\"compress\"][-1]:\n",
    "        latex_code += r\"    \\cdashlinelr{1-8}\" + \"\\n\"\n",
    "\n",
    "# end of table\n",
    "latex_code += r\"    \\bottomrule \\\\\" + \"\\n\"\n",
    "latex_code += r\"\\end{tabular}\"\n",
    "\n",
    "# Pring the table\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Table: MU ←→ MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unlearn First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
      "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
      "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
      "    \\midrule\n",
      "    {GA}$\\rightarrow${SparseGPT (0.25) } & 0.0 & 0.0 & 0.0 & 12.25 & 0.46 & 0.51 & inf \\\\\n",
      "    {GA}$\\rightarrow${SparseGPT (0.35) } & 0.0 & 0.0 & 0.0 & 10.75 & 0.44 & 0.49 & inf \\\\\n",
      "    {GA}$\\rightarrow${SparseGPT (0.45) } & 0.0 & 0.0 & 0.0 & 9.25 & 0.42 & 0.45 & inf \\\\\n",
      "    {GA}$\\rightarrow${SparseGPT (0.55) } & 0.0 & 0.0 & 0.0 & 7.75 & 0.3 & 0.32 & inf \\\\\n",
      "    {GA}$\\rightarrow${SparseGPT (0.65) } & 0.0 & 0.0 & 0.0 & 6.25 & 0.25 & 0.25 & inf \\\\\n",
      "    {GA}$\\rightarrow${SparseGPT (0.75) } & 0.0 & 0.0 & 0.0 & 4.75 & 0.25 & 0.25 & inf \\\\\n",
      "    {GA}$\\rightarrow${Wanda (0.25) } & 0.0 & 0.0 & 0.0 & 12.25 & 0.46 & 0.52 & inf \\\\\n",
      "    {GA}$\\rightarrow${Wanda (0.35) } & 0.0 & 0.0 & 0.0 & 10.75 & 0.45 & 0.49 & inf \\\\\n",
      "    {GA}$\\rightarrow${Wanda (0.45) } & 0.0 & 0.0 & 0.0 & 9.25 & 0.4 & 0.43 & inf \\\\\n",
      "    {GA}$\\rightarrow${Wanda (0.55) } & 0.0 & 0.0 & 0.0 & 7.75 & 0.3 & 0.32 & inf \\\\\n",
      "    {GA}$\\rightarrow${Wanda (0.65) } & 0.0 & 0.0 & 0.0 & 6.25 & 0.25 & 0.25 & inf \\\\\n",
      "    {GA}$\\rightarrow${Wanda (0.75) } & 0.0 & 0.0 & 0.0 & 4.75 & 0.25 & 0.26 & inf \\\\\n",
      "    {GA}$\\rightarrow${GPTQ (2-Bit) } & 0.0 & 0.0 & 0.0 & 2.25 & 0.25 & 0.25 & inf \\\\\n",
      "    {GA}$\\rightarrow${GPTQ (3-Bit) } & 0.0 & 0.0 & 0.0 & 3.25 & 0.33 & 0.39 & inf \\\\\n",
      "    {GA}$\\rightarrow${GPTQ (4-Bit) } & 0.0 & 0.0 & 0.0 & 4.25 & 0.43 & 0.48 & inf \\\\\n",
      "    {GA}$\\rightarrow${GPTQ (8-Bit) } & 0.0 & 0.0 & 0.0 & 8.25 & 0.45 & 0.49 & inf \\\\\n",
      "    {GA}$\\rightarrow${AWQ (2-Bit) } & 0.0 & 0.0 & 0.0 & 2.25 & 0.25 & 0.25 & 1769133.88 \\\\\n",
      "    {GA}$\\rightarrow${AWQ (3-Bit) } & 0.0 & 0.0 & 0.0 & 3.25 & 0.41 & 0.44 & inf \\\\\n",
      "    {GA}$\\rightarrow${AWQ (4-Bit) } & 0.0 & 0.0 & 0.0 & 4.25 & 0.43 & 0.47 & inf \\\\\n",
      "    {GA}$\\rightarrow${AWQ (5-Bit) } & 0.0 & 0.0 & 0.0 & 5.25 & 0.46 & 0.5 & inf \\\\\n",
      "    {GA}$\\rightarrow${AWQ (6-Bit) } & 0.0 & 0.0 & 0.0 & 6.25 & 0.45 & 0.49 & inf \\\\\n",
      "    {GA}$\\rightarrow${AWQ (8-Bit) } & 0.0 & 0.0 & 0.0 & 8.25 & 0.45 & 0.49 & inf \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {GD}$\\rightarrow${SparseGPT (0.25) } & 0.0 & 0.01 & 0.02 & 12.25 & 0.27 & 0.58 & 5.07 \\\\\n",
      "    {GD}$\\rightarrow${SparseGPT (0.35) } & 0.0 & 0.0 & 0.02 & 10.75 & 0.28 & 0.55 & 5.64 \\\\\n",
      "    {GD}$\\rightarrow${SparseGPT (0.45) } & 0.0 & 0.0 & 0.02 & 9.25 & 0.28 & 0.51 & 7.02 \\\\\n",
      "    {GD}$\\rightarrow${SparseGPT (0.55) } & 0.0 & 0.0 & 0.01 & 7.75 & 0.27 & 0.42 & 10.6 \\\\\n",
      "    {GD}$\\rightarrow${SparseGPT (0.65) } & 0.0 & 0.0 & 0.02 & 6.25 & 0.26 & 0.27 & 25.97 \\\\\n",
      "    {GD}$\\rightarrow${SparseGPT (0.75) } & 0.01 & 0.0 & 0.01 & 4.75 & 0.24 & 0.23 & 119.71 \\\\\n",
      "    {GD}$\\rightarrow${Wanda (0.25) } & 0.0 & 0.01 & 0.03 & 12.25 & 0.35 & 0.58 & 5.34 \\\\\n",
      "    {GD}$\\rightarrow${Wanda (0.35) } & 0.0 & 0.0 & 0.02 & 10.75 & 0.38 & 0.56 & 6.61 \\\\\n",
      "    {GD}$\\rightarrow${Wanda (0.45) } & 0.0 & 0.0 & 0.01 & 9.25 & 0.46 & 0.51 & 9.9 \\\\\n",
      "    {GD}$\\rightarrow${Wanda (0.55) } & 0.0 & 0.0 & 0.0 & 7.75 & 0.37 & 0.39 & 22.22 \\\\\n",
      "    {GD}$\\rightarrow${Wanda (0.65) } & 0.0 & 0.0 & 0.0 & 6.25 & 0.26 & 0.23 & 184.75 \\\\\n",
      "    {GD}$\\rightarrow${Wanda (0.75) } & 0.0 & 0.0 & 0.0 & 4.75 & 0.26 & 0.23 & 3298.92 \\\\\n",
      "    {GD}$\\rightarrow${GPTQ (2-Bit) } & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.27 & inf \\\\\n",
      "    {GD}$\\rightarrow${GPTQ (3-Bit) } & 0.0 & 0.0 & 0.0 & 3.25 & 0.24 & 0.3 & inf \\\\\n",
      "    {GD}$\\rightarrow${GPTQ (4-Bit) } & 0.0 & 0.0 & 0.0 & 4.25 & 0.24 & 0.32 & 5.106230513313957e+35 \\\\\n",
      "    {GD}$\\rightarrow${GPTQ (8-Bit) } & 0.0 & 0.0 & 0.0 & 8.25 & 0.24 & 0.35 & 2.445586872346403e+26 \\\\\n",
      "    {GD}$\\rightarrow${AWQ (2-Bit) } & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.27 & 2598854.5 \\\\\n",
      "    {GD}$\\rightarrow${AWQ (3-Bit) } & 0.0 & 0.0 & 0.0 & 3.25 & 0.33 & 0.33 & inf \\\\\n",
      "    {GD}$\\rightarrow${AWQ (4-Bit) } & 0.0 & 0.0 & 0.0 & 4.25 & 0.24 & 0.35 & 2.0452847856930923e+28 \\\\\n",
      "    {GD}$\\rightarrow${AWQ (5-Bit) } & 0.0 & 0.0 & 0.0 & 5.25 & 0.25 & 0.36 & 1.0052004503012292e+28 \\\\\n",
      "    {GD}$\\rightarrow${AWQ (6-Bit) } & 0.0 & 0.0 & 0.0 & 6.25 & 0.25 & 0.35 & 6.499345951176105e+25 \\\\\n",
      "    {GD}$\\rightarrow${AWQ (8-Bit) } & 0.0 & 0.0 & 0.0 & 8.25 & 0.25 & 0.35 & 9.430016153317283e+25 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {RMU}$\\rightarrow${SparseGPT (0.25) } & 0.02 & 0.02 & 0.03 & 12.25 & 0.28 & 0.56 & 5.93 \\\\\n",
      "    {RMU}$\\rightarrow${SparseGPT (0.35) } & 0.02 & 0.02 & 0.03 & 10.75 & 0.28 & 0.53 & 6.39 \\\\\n",
      "    {RMU}$\\rightarrow${SparseGPT (0.45) } & 0.02 & 0.02 & 0.03 & 9.25 & 0.27 & 0.48 & 8.14 \\\\\n",
      "    {RMU}$\\rightarrow${SparseGPT (0.55) } & 0.01 & 0.01 & 0.03 & 7.75 & 0.26 & 0.39 & 10.58 \\\\\n",
      "    {RMU}$\\rightarrow${SparseGPT (0.65) } & 0.0 & 0.01 & 0.03 & 6.25 & 0.25 & 0.26 & 35.97 \\\\\n",
      "    {RMU}$\\rightarrow${SparseGPT (0.75) } & 0.01 & 0.01 & 0.03 & 4.75 & 0.25 & 0.23 & 101.32 \\\\\n",
      "    {RMU}$\\rightarrow${Wanda (0.25) } & 0.02 & 0.02 & 0.04 & 12.25 & 0.29 & 0.56 & 5.88 \\\\\n",
      "    {RMU}$\\rightarrow${Wanda (0.35) } & 0.02 & 0.02 & 0.03 & 10.75 & 0.28 & 0.53 & 6.36 \\\\\n",
      "    {RMU}$\\rightarrow${Wanda (0.45) } & 0.02 & 0.01 & 0.03 & 9.25 & 0.29 & 0.48 & 7.71 \\\\\n",
      "    {RMU}$\\rightarrow${Wanda (0.55) } & 0.01 & 0.01 & 0.03 & 7.75 & 0.27 & 0.35 & 13.73 \\\\\n",
      "    {RMU}$\\rightarrow${Wanda (0.65) } & 0.01 & 0.01 & 0.03 & 6.25 & 0.25 & 0.23 & 52.67 \\\\\n",
      "    {RMU}$\\rightarrow${Wanda (0.75) } & 0.03 & 0.02 & 0.03 & 4.75 & 0.25 & 0.23 & 334.45 \\\\\n",
      "    {RMU}$\\rightarrow${GPTQ (2-Bit) } & 0.0 & 0.01 & 0.03 & 2.25 & 0.25 & 0.24 & 3451.67 \\\\\n",
      "    {RMU}$\\rightarrow${GPTQ (3-Bit) } & 0.01 & 0.01 & 0.03 & 3.25 & 0.26 & 0.42 & 9.09 \\\\\n",
      "    {RMU}$\\rightarrow${GPTQ (4-Bit) } & 0.02 & 0.02 & 0.03 & 4.25 & 0.27 & 0.53 & 12.88 \\\\\n",
      "    {RMU}$\\rightarrow${GPTQ (8-Bit) } & 0.02 & 0.02 & 0.03 & 8.25 & 0.29 & 0.57 & 5.56 \\\\\n",
      "    {RMU}$\\rightarrow${AWQ (2-Bit) } & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.26 & 1726409.12 \\\\\n",
      "    {RMU}$\\rightarrow${AWQ (3-Bit) } & 0.01 & 0.01 & 0.03 & 3.25 & 0.27 & 0.45 & 7.55 \\\\\n",
      "    {RMU}$\\rightarrow${AWQ (4-Bit) } & 0.02 & 0.02 & 0.03 & 4.25 & 0.29 & 0.56 & 5.95 \\\\\n",
      "    {RMU}$\\rightarrow${AWQ (5-Bit) } & 0.02 & 0.03 & 0.04 & 5.25 & 0.28 & 0.56 & 5.68 \\\\\n",
      "    {RMU}$\\rightarrow${AWQ (6-Bit) } & 0.02 & 0.02 & 0.03 & 6.25 & 0.28 & 0.56 & 5.62 \\\\\n",
      "    {RMU}$\\rightarrow${AWQ (8-Bit) } & 0.02 & 0.02 & 0.03 & 8.25 & 0.29 & 0.57 & 5.56 \\\\\n",
      "    \\bottomrule \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "latex_code = r\"\"\"\n",
    "\\begin{tabular}{lcccccccc}\n",
    "    \\toprule\n",
    "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
    "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
    "    \\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Unlearn -> Compression\n",
    "appendix_unlearn_compress = appendix_results[appendix_results[\"interventions\"].apply(lambda x: x == [\"unlearn\", \"compress\"])]\n",
    "for unlearn_technique in appendix_technique_ordering[\"unlearn\"]:\n",
    "    formatted_unlearn_technique = unlearn_technique\n",
    "    appendix_unlearn_compress_unlearn_technique = appendix_unlearn_compress[appendix_unlearn_compress[\"unlearn\"] == formatted_unlearn_technique.lower()]\n",
    "    assert len(appendix_unlearn_compress_unlearn_technique) > 0, f\"No data found for {unlearn_technique}\"\n",
    "    for compress_technique in appendix_technique_ordering[\"compress\"]:\n",
    "        formatted_compress_technique = compress_technique\n",
    "        appendix_unlearn_compress_technique_frame = appendix_unlearn_compress_unlearn_technique[appendix_unlearn_compress_unlearn_technique[\"compression\"] == formatted_compress_technique.lower()]\n",
    "        assert len(appendix_unlearn_compress_technique_frame) > 0, f\"No data found for {compress_technique}\"\n",
    "        compression_strength_column = \"sparsity_ratio\" if compress_technique in [\"SparseGPT\", \"Wanda\"] else \"wbits\"\n",
    "        compression_strength_ordering = sorted(set([round(strength, 2) for strength in appendix_unlearn_compress_technique_frame[compression_strength_column] if strength not in [0, 16]]))\n",
    "        for compression_strength in compression_strength_ordering:\n",
    "            technique_row_label = compress_technique\n",
    "            current_compression = appendix_unlearn_compress_technique_frame[appendix_unlearn_compress_technique_frame[compression_strength_column] == compression_strength]\n",
    "            if compress_technique in [\"SparseGPT\", \"Wanda\"]:\n",
    "                technique_row_label += \" (\" + str(current_compression[\"sparsity_ratio\"].iloc[0]) + \") \"\n",
    "            elif compress_technique in [\"GPTQ\", \"AWQ\"]:\n",
    "                technique_row_label += \" (\" + str(int(current_compression[\"wbits\"].iloc[0])) + \"-Bit) \"\n",
    "            \n",
    "            latex_code += f\"    {{{formatted_unlearn_technique}}}$\\\\rightarrow${{{technique_row_label}}}\"\n",
    "            for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "                latex_code += f\" & {round(current_compression[col].mean(), 2)}\"\n",
    "            \n",
    "            latex_code += r\" \\\\\" + \"\\n\"\n",
    "    \n",
    "    if unlearn_technique != appendix_technique_ordering[\"unlearn\"][-1]:\n",
    "        latex_code += r\"    \\cdashlinelr{1-8}\" + \"\\n\"\n",
    "    \n",
    "# end of table\n",
    "latex_code += r\"    \\bottomrule \\\\\" + \"\\n\"\n",
    "latex_code += r\"\\end{tabular}\"\n",
    "\n",
    "# Pring the table\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compression First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
      "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
      "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
      "    \\midrule\n",
      "    {SparseGPT (0.25) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 12.25 & 0.43 & 0.45 & inf \\\\\n",
      "    {SparseGPT (0.35) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 10.75 & 0.34 & 0.36 & inf \\\\\n",
      "    {SparseGPT (0.45) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 9.25 & 0.31 & 0.33 & inf \\\\\n",
      "    {SparseGPT (0.55) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 7.75 & 0.25 & 0.25 & inf \\\\\n",
      "    {SparseGPT (0.65) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 6.25 & 0.27 & 0.28 & inf \\\\\n",
      "    {SparseGPT (0.75) }$\\rightarrow${GA} & 0.01 & 0.01 & 0.03 & 4.75 & 0.26 & 0.23 & inf \\\\\n",
      "    {SparseGPT (0.25) }$\\rightarrow${GD} & 0.01 & 0.0 & 0.02 & 12.25 & 0.5 & 0.48 & 13.32 \\\\\n",
      "    {SparseGPT (0.35) }$\\rightarrow${GD} & 0.01 & 0.01 & 0.01 & 10.75 & 0.29 & 0.25 & 2712.26 \\\\\n",
      "    {SparseGPT (0.45) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.0 & 9.25 & 0.25 & 0.48 & 1.246813090597988e+19 \\\\\n",
      "    {SparseGPT (0.55) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.02 & 7.75 & 0.35 & 0.36 & 9.35 \\\\\n",
      "    {SparseGPT (0.65) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.01 & 6.25 & 0.24 & 0.26 & inf \\\\\n",
      "    {SparseGPT (0.75) }$\\rightarrow${GD} & 0.0 & 0.01 & 0.01 & 4.75 & 0.26 & 0.23 & 1437619072.0 \\\\\n",
      "    {SparseGPT (0.25) }$\\rightarrow${RMU} & 0.01 & 0.02 & 0.04 & 12.25 & 0.31 & 0.57 & 5.92 \\\\\n",
      "    {SparseGPT (0.35) }$\\rightarrow${RMU} & 0.0 & 0.02 & 0.02 & 10.75 & 0.31 & 0.54 & 6.35 \\\\\n",
      "    {SparseGPT (0.45) }$\\rightarrow${RMU} & 0.01 & 0.01 & 0.03 & 9.25 & 0.31 & 0.51 & 7.86 \\\\\n",
      "    {SparseGPT (0.55) }$\\rightarrow${RMU} & 0.01 & 0.01 & 0.03 & 7.75 & 0.29 & 0.41 & 10.39 \\\\\n",
      "    {SparseGPT (0.65) }$\\rightarrow${RMU} & 0.0 & 0.0 & 0.02 & 6.25 & 0.26 & 0.24 & 30.26 \\\\\n",
      "    {SparseGPT (0.75) }$\\rightarrow${RMU} & 0.01 & 0.01 & 0.02 & 4.75 & 0.25 & 0.23 & 97.03 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {Wanda (0.25) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 12.25 & 0.49 & 0.54 & inf \\\\\n",
      "    {Wanda (0.35) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 10.75 & 0.44 & 0.51 & inf \\\\\n",
      "    {Wanda (0.45) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 9.25 & 0.41 & 0.43 & inf \\\\\n",
      "    {Wanda (0.55) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 7.75 & 0.32 & 0.34 & inf \\\\\n",
      "    {Wanda (0.65) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 6.25 & 0.26 & 0.26 & inf \\\\\n",
      "    {Wanda (0.75) }$\\rightarrow${GA} & 0.01 & 0.01 & 0.03 & 4.75 & 0.26 & 0.23 & 8.12597183809491e+36 \\\\\n",
      "    {Wanda (0.25) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.0 & 12.25 & 0.52 & 0.57 & 13.26 \\\\\n",
      "    {Wanda (0.35) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.0 & 10.75 & 0.45 & 0.48 & 57480503296.0 \\\\\n",
      "    {Wanda (0.45) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.0 & 9.25 & 0.5 & 0.53 & 49.77 \\\\\n",
      "    {Wanda (0.55) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.0 & 7.75 & 0.39 & 0.35 & 121.07 \\\\\n",
      "    {Wanda (0.65) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.0 & 6.25 & 0.26 & 0.23 & 16268.01 \\\\\n",
      "    {Wanda (0.75) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.0 & 4.75 & 0.25 & 0.25 & 10226426.0 \\\\\n",
      "    {Wanda (0.25) }$\\rightarrow${RMU} & 0.02 & 0.02 & 0.04 & 12.25 & 0.32 & 0.57 & 5.86 \\\\\n",
      "    {Wanda (0.35) }$\\rightarrow${RMU} & 0.02 & 0.02 & 0.03 & 10.75 & 0.34 & 0.56 & 6.33 \\\\\n",
      "    {Wanda (0.45) }$\\rightarrow${RMU} & 0.02 & 0.02 & 0.03 & 9.25 & 0.39 & 0.52 & 7.55 \\\\\n",
      "    {Wanda (0.55) }$\\rightarrow${RMU} & 0.01 & 0.01 & 0.03 & 7.75 & 0.36 & 0.36 & 12.58 \\\\\n",
      "    {Wanda (0.65) }$\\rightarrow${RMU} & 0.01 & 0.01 & 0.02 & 6.25 & 0.26 & 0.23 & 45.53 \\\\\n",
      "    {Wanda (0.75) }$\\rightarrow${RMU} & 0.02 & 0.03 & 0.02 & 4.75 & 0.26 & 0.23 & 391.87 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {GPTQ (2-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 2.25 & 0.25 & 0.25 & inf \\\\\n",
      "    {GPTQ (3-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 3.25 & 0.24 & 0.27 & inf \\\\\n",
      "    {GPTQ (4-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 4.25 & 0.34 & 0.38 & inf \\\\\n",
      "    {GPTQ (8-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 8.25 & 0.45 & 0.5 & inf \\\\\n",
      "    {GPTQ (2-Bit) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.0 & 2.25 & 0.25 & 0.25 & inf \\\\\n",
      "    {GPTQ (3-Bit) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.0 & 3.25 & 0.26 & 0.24 & 628.24 \\\\\n",
      "    {GPTQ (4-Bit) }$\\rightarrow${GD} & 0.01 & 0.01 & 0.01 & 4.25 & 0.27 & 0.56 & 9.38 \\\\\n",
      "    {GPTQ (8-Bit) }$\\rightarrow${GD} & 0.01 & 0.02 & 0.05 & 8.25 & 0.26 & 0.47 & inf \\\\\n",
      "    {GPTQ (2-Bit) }$\\rightarrow${RMU} & 0.0 & 0.0 & 0.0 & 2.25 & 0.25 & 0.24 & 1017825.94 \\\\\n",
      "    {GPTQ (3-Bit) }$\\rightarrow${RMU} & 0.01 & 0.01 & 0.02 & 3.25 & 0.32 & 0.32 & 11.1 \\\\\n",
      "    {GPTQ (4-Bit) }$\\rightarrow${RMU} & 0.0 & 0.02 & 0.02 & 4.25 & 0.45 & 0.58 & 6.35 \\\\\n",
      "    {GPTQ (8-Bit) }$\\rightarrow${RMU} & 0.01 & 0.02 & 0.03 & 8.25 & 0.28 & 0.57 & 5.58 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {AWQ (2-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.27 & inf \\\\\n",
      "    {AWQ (3-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 3.25 & 0.24 & 0.27 & inf \\\\\n",
      "    {AWQ (4-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 4.25 & 0.45 & 0.45 & inf \\\\\n",
      "    {AWQ (5-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 5.25 & 0.35 & 0.39 & inf \\\\\n",
      "    {AWQ (6-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 6.25 & 0.25 & 0.3 & inf \\\\\n",
      "    {AWQ (8-Bit) }$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 8.25 & 0.28 & 0.37 & inf \\\\\n",
      "    {AWQ (2-Bit) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.02 & 2.25 & 0.24 & 0.27 & 4.496028624899119e+33 \\\\\n",
      "    {AWQ (3-Bit) }$\\rightarrow${GD} & 0.01 & 0.0 & 0.02 & 3.25 & 0.29 & 0.34 & 166895.91 \\\\\n",
      "    {AWQ (4-Bit) }$\\rightarrow${GD} & 0.01 & 0.0 & 0.04 & 4.25 & 0.24 & 0.42 & 7.05 \\\\\n",
      "    {AWQ (5-Bit) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.03 & 5.25 & 0.25 & 0.38 & 2685384.0 \\\\\n",
      "    {AWQ (6-Bit) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.05 & 6.25 & 0.25 & 0.31 & 27025.07 \\\\\n",
      "    {AWQ (8-Bit) }$\\rightarrow${GD} & 0.0 & 0.0 & 0.01 & 8.25 & 0.25 & 0.25 & 49224794112.0 \\\\\n",
      "    {AWQ (2-Bit) }$\\rightarrow${RMU} & 0.0 & 0.0 & 0.0 & 2.25 & 0.24 & 0.27 & 1749321.75 \\\\\n",
      "    {AWQ (3-Bit) }$\\rightarrow${RMU} & 0.01 & 0.01 & 0.03 & 3.25 & 0.27 & 0.46 & 7.51 \\\\\n",
      "    {AWQ (4-Bit) }$\\rightarrow${RMU} & 0.02 & 0.02 & 0.03 & 4.25 & 0.27 & 0.55 & 6.06 \\\\\n",
      "    {AWQ (5-Bit) }$\\rightarrow${RMU} & 0.02 & 0.03 & 0.04 & 5.25 & 0.27 & 0.56 & 5.68 \\\\\n",
      "    {AWQ (6-Bit) }$\\rightarrow${RMU} & 0.02 & 0.02 & 0.04 & 6.25 & 0.28 & 0.56 & 5.6 \\\\\n",
      "    {AWQ (8-Bit) }$\\rightarrow${RMU} & 0.02 & 0.03 & 0.03 & 8.25 & 0.28 & 0.57 & 5.71 \\\\\n",
      "    \\bottomrule \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "latex_code = r\"\"\"\n",
    "\\begin{tabular}{lcccccccc}\n",
    "    \\toprule\n",
    "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
    "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
    "    \\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Compression -> Unlearn\n",
    "appendix_compress_unlearn = appendix_results[appendix_results[\"interventions\"].apply(lambda x: x == [\"compress\", \"unlearn\"])]\n",
    "for compress_technique in appendix_technique_ordering[\"compress\"]:\n",
    "    appendix_compress_unlearn_technique_frame = appendix_compress_unlearn[appendix_compress_unlearn[\"compression\"] == compress_technique.lower()]\n",
    "    assert len(appendix_compress_unlearn_technique_frame) > 0, f\"No data found for {compress_technique}\"\n",
    "\n",
    "    compression_strength_column = \"sparsity_ratio\" if compress_technique in [\"SparseGPT\", \"Wanda\"] else \"wbits\"\n",
    "    compression_strength_ordering = sorted(set([round(strength, 2) for strength in appendix_compress_unlearn_technique_frame[compression_strength_column] if strength not in [0, 16]]))\n",
    "\n",
    "    for unlearn_technique in appendix_technique_ordering[\"unlearn\"]:\n",
    "        formatted_unlearn_technique = unlearn_technique\n",
    "        appendix_compress_unlearn_unlearn_technique = appendix_compress_unlearn_technique_frame[appendix_compress_unlearn_technique_frame[\"unlearn\"] == formatted_unlearn_technique.lower()]\n",
    "        assert len(appendix_compress_unlearn_unlearn_technique) > 0, f\"No data found for {unlearn_technique}\"\n",
    "        \n",
    "        for compression_strength in compression_strength_ordering:\n",
    "            technique_row_label = compress_technique\n",
    "            current_compression = appendix_compress_unlearn_unlearn_technique[round(appendix_compress_unlearn_unlearn_technique[compression_strength_column], 2) == compression_strength]\n",
    "            if compress_technique in [\"SparseGPT\", \"Wanda\"]:\n",
    "                technique_row_label += \" (\" + str(compression_strength) + \") \"\n",
    "            elif compress_technique in [\"GPTQ\", \"AWQ\"]:\n",
    "                technique_row_label += \" (\" + str(int(compression_strength)) + \"-Bit) \"\n",
    "            \n",
    "            latex_code += f\"    {{{technique_row_label}}}$\\\\rightarrow${{{formatted_unlearn_technique}}}\"\n",
    "            for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "                assert len(current_compression) == 1, f\"Multiple rows found for {compress_technique} -> {unlearn_technique} -> {compression_strength}\"\n",
    "                latex_code += f\" & {round(current_compression[col].mean(), 2)}\"\n",
    "            \n",
    "            latex_code += r\" \\\\\" + \"\\n\"\n",
    "    \n",
    "    if compress_technique != appendix_technique_ordering[\"compress\"][-1]:\n",
    "        latex_code += r\"    \\cdashlinelr{1-8}\" + \"\\n\"\n",
    "    \n",
    "# end of table\n",
    "latex_code += r\"    \\bottomrule \\\\\" + \"\\n\"\n",
    "latex_code += r\"\\end{tabular}\"\n",
    "\n",
    "# Pring the table\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Table: KE ←→ MU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
      "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
      "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
      "    \\midrule\n",
      "    {FT}$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 16.0 & 0.47 & 0.53 & inf \\\\\n",
      "    {FT}$\\rightarrow${GD} & 0.32 & 0.25 & 0.07 & 16.0 & 0.29 & 0.41 & 2.8236466916108585e+31 \\\\\n",
      "    {FT}$\\rightarrow${RMU} & 0.99 & 0.82 & 0.1 & 16.0 & 0.28 & 0.56 & 5.61 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {MEMIT}$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 16.0 & 0.4 & 0.45 & inf \\\\\n",
      "    {MEMIT}$\\rightarrow${GD} & 0.53 & 0.49 & 0.03 & 16.0 & 0.26 & 0.36 & 27645056122880.0 \\\\\n",
      "    {MEMIT}$\\rightarrow${RMU} & 0.96 & 0.89 & 0.03 & 16.0 & 0.29 & 0.56 & 5.58 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {LoRA}$\\rightarrow${GA} & 0.0 & 0.0 & 0.0 & 16.0 & 0.28 & 0.29 & inf \\\\\n",
      "    {LoRA}$\\rightarrow${GD} & 0.44 & 0.23 & 0.05 & 16.0 & 0.3 & 0.45 & 29.46 \\\\\n",
      "    {LoRA}$\\rightarrow${RMU} & 1.0 & 0.68 & 0.05 & 16.0 & 0.3 & 0.52 & 34.56 \\\\\n",
      "    \\bottomrule \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "latex_code = r\"\"\"\n",
    "\\begin{tabular}{lcccccccc}\n",
    "    \\toprule\n",
    "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
    "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
    "    \\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Edit -> Unlearn\n",
    "appendix_compress_edit = appendix_results[appendix_results[\"interventions\"].apply(lambda x: x == [\"edit\", \"unlearn\"])]\n",
    "for edit_technique in appendix_technique_ordering[\"edit\"]:\n",
    "    formatted_edit_technique = \"FT\" if edit_technique == \"Fine-tune\" else edit_technique\n",
    "    appendix_compress_edit_edit_technique = appendix_compress_edit[appendix_compress_edit[\"edit\"] == formatted_edit_technique.lower()]\n",
    "    assert len(appendix_compress_edit_edit_technique) > 0, f\"No data found for {edit_technique}\"\n",
    "    for unlearn_technique in appendix_technique_ordering[\"unlearn\"]:\n",
    "        formatted_unlearn_technique = unlearn_technique\n",
    "        appendix_compress_edit_technique_frame = appendix_compress_edit_edit_technique[appendix_compress_edit_edit_technique[\"unlearn\"] == formatted_unlearn_technique.lower()]\n",
    "        assert len(appendix_compress_edit_technique_frame) > 0, f\"No data found for {unlearn_technique}\"\n",
    "\n",
    "        # No compression strength for this composition\n",
    "        technique_row_label = edit_technique\n",
    "        latex_code += f\"    {{{formatted_edit_technique}}}$\\\\rightarrow${{{formatted_unlearn_technique}}}\"\n",
    "        for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "            latex_code += f\" & {round(appendix_compress_edit_technique_frame[col].mean(), 2)}\"\n",
    "\n",
    "        latex_code += r\" \\\\\" + \"\\n\"\n",
    "        \n",
    "    if edit_technique != appendix_technique_ordering[\"edit\"][-1]:\n",
    "        latex_code += r\"    \\cdashlinelr{1-8}\" + \"\\n\"\n",
    "\n",
    "# end of table\n",
    "latex_code += r\"    \\bottomrule \\\\\" + \"\\n\"\n",
    "latex_code += r\"\\end{tabular}\"\n",
    "\n",
    "# Pring the table\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unlearn First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{lcccccccc}\n",
      "    \\toprule\n",
      "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
      "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
      "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
      "    \\midrule\n",
      "    {GA}$\\rightarrow${FT} & 0.07 & 0.04 & 0.0 & 16.0 & 0.47 & 0.52 & inf \\\\\n",
      "    {GA}$\\rightarrow${MEMIT} & 0.48 & 0.41 & 0.0 & 16.0 & 0.45 & 0.49 & inf \\\\\n",
      "    {GA}$\\rightarrow${LoRA} & 1.0 & 0.78 & 0.03 & 16.0 & 0.34 & 0.36 & 56.91 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {GD}$\\rightarrow${FT} & 0.99 & 0.81 & 0.11 & 16.0 & 0.29 & 0.59 & 4.64 \\\\\n",
      "    {GD}$\\rightarrow${MEMIT} & 0.93 & 0.89 & 0.05 & 16.0 & 0.28 & 0.58 & 4.65 \\\\\n",
      "    {GD}$\\rightarrow${LoRA} & 1.0 & 0.71 & 0.08 & 16.0 & 0.54 & 0.59 & 4.99 \\\\\n",
      "    \\cdashlinelr{1-8}\n",
      "    {RMU}$\\rightarrow${FT} & 1.0 & 0.79 & 0.13 & 16.0 & 0.32 & 0.57 & 5.6 \\\\\n",
      "    {RMU}$\\rightarrow${MEMIT} & 0.97 & 0.93 & 0.03 & 16.0 & 0.3 & 0.56 & 5.58 \\\\\n",
      "    {RMU}$\\rightarrow${LoRA} & 1.0 & 0.71 & 0.05 & 16.0 & 0.29 & 0.56 & 12.83 \\\\\n",
      "    \\bottomrule \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "latex_code = r\"\"\"\n",
    "\\begin{tabular}{lcccccccc}\n",
    "    \\toprule\n",
    "    & \\multicolumn{3}{c}{Editing} & \\multicolumn{1}{c}{Compression} & \\multicolumn{1}{c}{Unlearning} & \\multicolumn{2}{c}{Utility} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-8}\n",
    "    & Edit Success & Generalization & Locality & Avg. Bits & Avg. WMDP & MMLU & WikiText PPL \\\\\n",
    "    \\midrule\n",
    "\"\"\"\n",
    "\n",
    "# Unlearn -> Edit\n",
    "appendix_unlearn_edit = appendix_results[appendix_results[\"interventions\"].apply(lambda x: x == [\"unlearn\", \"edit\"])]\n",
    "for unlearn_technique in appendix_technique_ordering[\"unlearn\"]:\n",
    "    formatted_unlearn_technique = unlearn_technique\n",
    "    appendix_unlearn_edit_unlearn_technique = appendix_unlearn_edit[appendix_unlearn_edit[\"unlearn\"] == formatted_unlearn_technique.lower()]\n",
    "    assert len(appendix_unlearn_edit_unlearn_technique) > 0, f\"No data found for {unlearn_technique}\"\n",
    "    for edit_technique in appendix_technique_ordering[\"edit\"]:\n",
    "        formatted_edit_technique = \"FT\" if edit_technique == \"Fine-tune\" else edit_technique\n",
    "        appendix_unlearn_edit_technique_frame = appendix_unlearn_edit_unlearn_technique[appendix_unlearn_edit_unlearn_technique[\"edit\"] == formatted_edit_technique.lower()]\n",
    "        assert len(appendix_unlearn_edit_technique_frame) > 0, f\"No data found for {edit_technique}\"\n",
    "        \n",
    "        # No compression strength for this composition\n",
    "        technique_row_label = unlearn_technique\n",
    "        latex_code += f\"    {{{formatted_unlearn_technique}}}$\\\\rightarrow${{{formatted_edit_technique}}}\"\n",
    "        for col in list(appendix_table_columns_map.keys())[1:]:\n",
    "            latex_code += f\" & {round(appendix_unlearn_edit_technique_frame[col].mean(), 2)}\"\n",
    "\n",
    "        latex_code += r\" \\\\\" + \"\\n\"\n",
    "        \n",
    "    if unlearn_technique != appendix_technique_ordering[\"unlearn\"][-1]:\n",
    "        latex_code += r\"    \\cdashlinelr{1-8}\" + \"\\n\"\n",
    "\n",
    "# end of table\n",
    "latex_code += r\"    \\bottomrule \\\\\" + \"\\n\"\n",
    "latex_code += r\"\\end{tabular}\"\n",
    "\n",
    "# Pring the table\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_order_label(row):\n",
    "    interventions = row[\"interventions\"]\n",
    "    first_method = \"\"\n",
    "    second_method = \"\"\n",
    "    if interventions[0] == \"edit\":\n",
    "        first_method = row[\"edit\"]\n",
    "    elif interventions[0] == \"compress\":\n",
    "        first_method = row[\"compression\"]\n",
    "    elif interventions[0] == \"unlearn\":\n",
    "        first_method = row[\"unlearn\"]\n",
    "    \n",
    "    if interventions[1] == \"edit\":\n",
    "        second_method = row[\"edit\"]\n",
    "    elif interventions[1] == \"compress\":\n",
    "        second_method = row[\"compression\"]\n",
    "    elif interventions[1] == \"unlearn\":\n",
    "        second_method = row[\"unlearn\"]\n",
    "    \n",
    "    return f\"{first_method}→{second_method}\"\n",
    "\n",
    "def wrap_label(interventions):\n",
    "    first_intervention, second_intervention = interventions[0], interventions[1]\n",
    "    first_letter_upper = first_intervention[0].upper()\n",
    "    second_letter_upper = second_intervention[0].upper()\n",
    "    \n",
    "    # EX: E $\\rightarrow$ C\n",
    "    return f\"{first_letter_upper}$\\\\rightarrow${second_letter_upper}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mock records for baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>edit_set</th>\n",
       "      <th>number_of_edits</th>\n",
       "      <th>rmu_layer_id</th>\n",
       "      <th>wbits</th>\n",
       "      <th>sparsity_ratio</th>\n",
       "      <th>qa_question_count_limit_x</th>\n",
       "      <th>mmlu accuracy</th>\n",
       "      <th>wmdp_bio accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>model_name</th>\n",
       "      <th>edit_dataset</th>\n",
       "      <th>compression_dataset</th>\n",
       "      <th>qa_question_count_limit_y</th>\n",
       "      <th>PPL</th>\n",
       "      <th>PPL edits</th>\n",
       "      <th>PPl QA</th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>PPl edits unmasked</th>\n",
       "      <th>date_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>lora_Edit</td>\n",
       "      <td>1.717923e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.691359</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>19.251539</td>\n",
       "      <td>1828405.625</td>\n",
       "      <td>6347.870605</td>\n",
       "      <td>1.79 TFLOPS</td>\n",
       "      <td>15581.417969</td>\n",
       "      <td>2024-06-09 08:47:23.456239616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>lora_Edit</td>\n",
       "      <td>1.717923e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.691359</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>19.251539</td>\n",
       "      <td>1828405.625</td>\n",
       "      <td>6347.870605</td>\n",
       "      <td>1.79 TFLOPS</td>\n",
       "      <td>15581.417969</td>\n",
       "      <td>2024-06-09 08:47:23.456239616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>lora_Edit</td>\n",
       "      <td>1.717923e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.691359</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>19.251539</td>\n",
       "      <td>1828405.625</td>\n",
       "      <td>6347.870605</td>\n",
       "      <td>1.79 TFLOPS</td>\n",
       "      <td>15581.417969</td>\n",
       "      <td>2024-06-09 08:47:23.456239616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>lora_Edit</td>\n",
       "      <td>1.717923e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.691359</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>19.251539</td>\n",
       "      <td>1828405.625</td>\n",
       "      <td>6347.870605</td>\n",
       "      <td>1.79 TFLOPS</td>\n",
       "      <td>15581.417969</td>\n",
       "      <td>2024-06-09 08:47:23.456239616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>lora_Edit</td>\n",
       "      <td>1.717923e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.691359</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>19.251539</td>\n",
       "      <td>1828405.625</td>\n",
       "      <td>6347.870605</td>\n",
       "      <td>1.79 TFLOPS</td>\n",
       "      <td>15581.417969</td>\n",
       "      <td>2024-06-09 08:47:23.456239616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>lora_Edit</td>\n",
       "      <td>1.717923e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.691359</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>19.251539</td>\n",
       "      <td>1828405.625</td>\n",
       "      <td>6347.870605</td>\n",
       "      <td>1.79 TFLOPS</td>\n",
       "      <td>15581.417969</td>\n",
       "      <td>2024-06-09 08:47:23.456239616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>lora_Edit</td>\n",
       "      <td>1.717923e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.691359</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>19.251539</td>\n",
       "      <td>1828405.625</td>\n",
       "      <td>6347.870605</td>\n",
       "      <td>1.79 TFLOPS</td>\n",
       "      <td>15581.417969</td>\n",
       "      <td>2024-06-09 08:47:23.456239616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>lora_Edit</td>\n",
       "      <td>1.717923e+09</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.607114</td>\n",
       "      <td>0.691359</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>19.251539</td>\n",
       "      <td>1828405.625</td>\n",
       "      <td>6347.870605</td>\n",
       "      <td>1.79 TFLOPS</td>\n",
       "      <td>15581.417969</td>\n",
       "      <td>2024-06-09 08:47:23.456239616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>memit_Edit</td>\n",
       "      <td>1.717453e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610080</td>\n",
       "      <td>0.701885</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.56542</td>\n",
       "      <td>10351.842773</td>\n",
       "      <td>379.738983</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>447.97879</td>\n",
       "      <td>2024-05-21 14:15:49.471601152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>memit_Edit</td>\n",
       "      <td>1.717453e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610080</td>\n",
       "      <td>0.701885</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.56542</td>\n",
       "      <td>10351.842773</td>\n",
       "      <td>379.738983</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>447.97879</td>\n",
       "      <td>2024-05-21 14:15:49.471601152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>memit_Edit</td>\n",
       "      <td>1.717453e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610080</td>\n",
       "      <td>0.701885</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.56542</td>\n",
       "      <td>10351.842773</td>\n",
       "      <td>379.738983</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>447.97879</td>\n",
       "      <td>2024-05-21 14:15:49.471601152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>memit_Edit</td>\n",
       "      <td>1.717453e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610080</td>\n",
       "      <td>0.701885</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.56542</td>\n",
       "      <td>10351.842773</td>\n",
       "      <td>379.738983</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>447.97879</td>\n",
       "      <td>2024-05-21 14:15:49.471601152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>memit_Edit</td>\n",
       "      <td>1.717453e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610080</td>\n",
       "      <td>0.701885</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.56542</td>\n",
       "      <td>10351.842773</td>\n",
       "      <td>379.738983</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>447.97879</td>\n",
       "      <td>2024-05-21 14:15:49.471601152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>memit_Edit</td>\n",
       "      <td>1.717453e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610080</td>\n",
       "      <td>0.701885</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.56542</td>\n",
       "      <td>10351.842773</td>\n",
       "      <td>379.738983</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>447.97879</td>\n",
       "      <td>2024-05-21 14:15:49.471601152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>memit_Edit</td>\n",
       "      <td>1.717453e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610080</td>\n",
       "      <td>0.701885</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.56542</td>\n",
       "      <td>10351.842773</td>\n",
       "      <td>379.738983</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>447.97879</td>\n",
       "      <td>2024-05-21 14:15:49.471601152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>memit_Edit</td>\n",
       "      <td>1.717453e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.610080</td>\n",
       "      <td>0.701885</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.56542</td>\n",
       "      <td>10351.842773</td>\n",
       "      <td>379.738983</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>447.97879</td>\n",
       "      <td>2024-05-21 14:15:49.471601152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ft_Edit</td>\n",
       "      <td>1.717740e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.613931</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.57129</td>\n",
       "      <td>19718.240234</td>\n",
       "      <td>469.199768</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>544.861145</td>\n",
       "      <td>2024-05-21 14:14:55.217775104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ft_Edit</td>\n",
       "      <td>1.717740e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.613931</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.57129</td>\n",
       "      <td>19718.240234</td>\n",
       "      <td>469.199768</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>544.861145</td>\n",
       "      <td>2024-05-21 14:14:55.217775104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ft_Edit</td>\n",
       "      <td>1.717740e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.613931</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.57129</td>\n",
       "      <td>19718.240234</td>\n",
       "      <td>469.199768</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>544.861145</td>\n",
       "      <td>2024-05-21 14:14:55.217775104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ft_Edit</td>\n",
       "      <td>1.717740e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.613931</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.57129</td>\n",
       "      <td>19718.240234</td>\n",
       "      <td>469.199768</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>544.861145</td>\n",
       "      <td>2024-05-21 14:14:55.217775104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ft_Edit</td>\n",
       "      <td>1.717740e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.613931</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.57129</td>\n",
       "      <td>19718.240234</td>\n",
       "      <td>469.199768</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>544.861145</td>\n",
       "      <td>2024-05-21 14:14:55.217775104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ft_Edit</td>\n",
       "      <td>1.717740e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.613931</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.57129</td>\n",
       "      <td>19718.240234</td>\n",
       "      <td>469.199768</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>544.861145</td>\n",
       "      <td>2024-05-21 14:14:55.217775104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ft_Edit</td>\n",
       "      <td>1.717740e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.613931</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.57129</td>\n",
       "      <td>19718.240234</td>\n",
       "      <td>469.199768</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>544.861145</td>\n",
       "      <td>2024-05-21 14:14:55.217775104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ft_Edit</td>\n",
       "      <td>1.717740e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.613931</td>\n",
       "      <td>0.696208</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.57129</td>\n",
       "      <td>19718.240234</td>\n",
       "      <td>469.199768</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>544.861145</td>\n",
       "      <td>2024-05-21 14:14:55.217775104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>rmu-none</td>\n",
       "      <td>1.717803e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.555923</td>\n",
       "      <td>25474.263672</td>\n",
       "      <td>400.781067</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>444.527161</td>\n",
       "      <td>2024-05-20 18:20:06.459071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>rmu-none</td>\n",
       "      <td>1.717803e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.555923</td>\n",
       "      <td>25474.263672</td>\n",
       "      <td>400.781067</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>444.527161</td>\n",
       "      <td>2024-05-20 18:20:06.459071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>rmu-none</td>\n",
       "      <td>1.717803e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.555923</td>\n",
       "      <td>25474.263672</td>\n",
       "      <td>400.781067</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>444.527161</td>\n",
       "      <td>2024-05-20 18:20:06.459071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>rmu-none</td>\n",
       "      <td>1.717803e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.555923</td>\n",
       "      <td>25474.263672</td>\n",
       "      <td>400.781067</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>444.527161</td>\n",
       "      <td>2024-05-20 18:20:06.459071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>rmu-none</td>\n",
       "      <td>1.717803e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.555923</td>\n",
       "      <td>25474.263672</td>\n",
       "      <td>400.781067</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>444.527161</td>\n",
       "      <td>2024-05-20 18:20:06.459071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>rmu-none</td>\n",
       "      <td>1.717803e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.555923</td>\n",
       "      <td>25474.263672</td>\n",
       "      <td>400.781067</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>444.527161</td>\n",
       "      <td>2024-05-20 18:20:06.459071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>rmu-none</td>\n",
       "      <td>1.717803e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.555923</td>\n",
       "      <td>25474.263672</td>\n",
       "      <td>400.781067</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>444.527161</td>\n",
       "      <td>2024-05-20 18:20:06.459071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>rmu-none</td>\n",
       "      <td>1.717803e+09</td>\n",
       "      <td>9.545455</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.570263</td>\n",
       "      <td>0.291580</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>zsre</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>5.555923</td>\n",
       "      <td>25474.263672</td>\n",
       "      <td>400.781067</td>\n",
       "      <td>1.92 TFLOPS</td>\n",
       "      <td>444.527161</td>\n",
       "      <td>2024-05-20 18:20:06.459071232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ga-none</td>\n",
       "      <td>1.718186e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501709</td>\n",
       "      <td>0.565593</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 09:58:53.057244672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ga-none</td>\n",
       "      <td>1.718186e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501709</td>\n",
       "      <td>0.565593</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 09:58:53.057244672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ga-none</td>\n",
       "      <td>1.718186e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501709</td>\n",
       "      <td>0.565593</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 09:58:53.057244672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ga-none</td>\n",
       "      <td>1.718186e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501709</td>\n",
       "      <td>0.565593</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 09:58:53.057244672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ga-none</td>\n",
       "      <td>1.718186e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501709</td>\n",
       "      <td>0.565593</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 09:58:53.057244672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ga-none</td>\n",
       "      <td>1.718186e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501709</td>\n",
       "      <td>0.565593</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 09:58:53.057244672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ga-none</td>\n",
       "      <td>1.718186e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501709</td>\n",
       "      <td>0.565593</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 09:58:53.057244672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ga-none</td>\n",
       "      <td>1.718186e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.501709</td>\n",
       "      <td>0.565593</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>2024-06-12 09:58:53.057244672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>gd-none</td>\n",
       "      <td>1.718197e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523002</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>4.476031</td>\n",
       "      <td>59003.925781</td>\n",
       "      <td>912.782715</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>1600.669312</td>\n",
       "      <td>2024-06-12 12:50:30.144293888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>gd-none</td>\n",
       "      <td>1.718197e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523002</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>4.476031</td>\n",
       "      <td>59003.925781</td>\n",
       "      <td>912.782715</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>1600.669312</td>\n",
       "      <td>2024-06-12 12:50:30.144293888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>gd-none</td>\n",
       "      <td>1.718197e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523002</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>4.476031</td>\n",
       "      <td>59003.925781</td>\n",
       "      <td>912.782715</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>1600.669312</td>\n",
       "      <td>2024-06-12 12:50:30.144293888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>gd-none</td>\n",
       "      <td>1.718197e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523002</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>4.476031</td>\n",
       "      <td>59003.925781</td>\n",
       "      <td>912.782715</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>1600.669312</td>\n",
       "      <td>2024-06-12 12:50:30.144293888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>gd-none</td>\n",
       "      <td>1.718197e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523002</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>4.476031</td>\n",
       "      <td>59003.925781</td>\n",
       "      <td>912.782715</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>1600.669312</td>\n",
       "      <td>2024-06-12 12:50:30.144293888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>gd-none</td>\n",
       "      <td>1.718197e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523002</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>4.476031</td>\n",
       "      <td>59003.925781</td>\n",
       "      <td>912.782715</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>1600.669312</td>\n",
       "      <td>2024-06-12 12:50:30.144293888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>gd-none</td>\n",
       "      <td>1.718197e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523002</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>4.476031</td>\n",
       "      <td>59003.925781</td>\n",
       "      <td>912.782715</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>1600.669312</td>\n",
       "      <td>2024-06-12 12:50:30.144293888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>gd-none</td>\n",
       "      <td>1.718197e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523002</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama-3 (8b)</td>\n",
       "      <td>mquake</td>\n",
       "      <td>c4</td>\n",
       "      <td>None</td>\n",
       "      <td>4.476031</td>\n",
       "      <td>59003.925781</td>\n",
       "      <td>912.782715</td>\n",
       "      <td>1.03 TFLOPS</td>\n",
       "      <td>1600.669312</td>\n",
       "      <td>2024-06-12 12:50:30.144293888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tag    _timestamp   edit_set  number_of_edits  rmu_layer_id  \\\n",
       "222   lora_Edit  1.717923e+09   5.500000             50.0     -1.000000   \n",
       "222   lora_Edit  1.717923e+09   5.500000             50.0     -1.000000   \n",
       "222   lora_Edit  1.717923e+09   5.500000             50.0     -1.000000   \n",
       "222   lora_Edit  1.717923e+09   5.500000             50.0     -1.000000   \n",
       "222   lora_Edit  1.717923e+09   5.500000             50.0     -1.000000   \n",
       "222   lora_Edit  1.717923e+09   5.500000             50.0     -1.000000   \n",
       "222   lora_Edit  1.717923e+09   5.500000             50.0     -1.000000   \n",
       "222   lora_Edit  1.717923e+09   5.500000             50.0     -1.000000   \n",
       "248  memit_Edit  1.717453e+09  11.000000             50.0     -1.000000   \n",
       "248  memit_Edit  1.717453e+09  11.000000             50.0     -1.000000   \n",
       "248  memit_Edit  1.717453e+09  11.000000             50.0     -1.000000   \n",
       "248  memit_Edit  1.717453e+09  11.000000             50.0     -1.000000   \n",
       "248  memit_Edit  1.717453e+09  11.000000             50.0     -1.000000   \n",
       "248  memit_Edit  1.717453e+09  11.000000             50.0     -1.000000   \n",
       "248  memit_Edit  1.717453e+09  11.000000             50.0     -1.000000   \n",
       "248  memit_Edit  1.717453e+09  11.000000             50.0     -1.000000   \n",
       "132     ft_Edit  1.717740e+09   9.545455             50.0     -1.000000   \n",
       "132     ft_Edit  1.717740e+09   9.545455             50.0     -1.000000   \n",
       "132     ft_Edit  1.717740e+09   9.545455             50.0     -1.000000   \n",
       "132     ft_Edit  1.717740e+09   9.545455             50.0     -1.000000   \n",
       "132     ft_Edit  1.717740e+09   9.545455             50.0     -1.000000   \n",
       "132     ft_Edit  1.717740e+09   9.545455             50.0     -1.000000   \n",
       "132     ft_Edit  1.717740e+09   9.545455             50.0     -1.000000   \n",
       "132     ft_Edit  1.717740e+09   9.545455             50.0     -1.000000   \n",
       "262    rmu-none  1.717803e+09   9.545455             50.0      3.181818   \n",
       "262    rmu-none  1.717803e+09   9.545455             50.0      3.181818   \n",
       "262    rmu-none  1.717803e+09   9.545455             50.0      3.181818   \n",
       "262    rmu-none  1.717803e+09   9.545455             50.0      3.181818   \n",
       "262    rmu-none  1.717803e+09   9.545455             50.0      3.181818   \n",
       "262    rmu-none  1.717803e+09   9.545455             50.0      3.181818   \n",
       "262    rmu-none  1.717803e+09   9.545455             50.0      3.181818   \n",
       "262    rmu-none  1.717803e+09   9.545455             50.0      3.181818   \n",
       "146     ga-none  1.718186e+09   1.000000             50.0     -1.000000   \n",
       "146     ga-none  1.718186e+09   1.000000             50.0     -1.000000   \n",
       "146     ga-none  1.718186e+09   1.000000             50.0     -1.000000   \n",
       "146     ga-none  1.718186e+09   1.000000             50.0     -1.000000   \n",
       "146     ga-none  1.718186e+09   1.000000             50.0     -1.000000   \n",
       "146     ga-none  1.718186e+09   1.000000             50.0     -1.000000   \n",
       "146     ga-none  1.718186e+09   1.000000             50.0     -1.000000   \n",
       "146     ga-none  1.718186e+09   1.000000             50.0     -1.000000   \n",
       "172     gd-none  1.718197e+09   1.000000             50.0     -1.000000   \n",
       "172     gd-none  1.718197e+09   1.000000             50.0     -1.000000   \n",
       "172     gd-none  1.718197e+09   1.000000             50.0     -1.000000   \n",
       "172     gd-none  1.718197e+09   1.000000             50.0     -1.000000   \n",
       "172     gd-none  1.718197e+09   1.000000             50.0     -1.000000   \n",
       "172     gd-none  1.718197e+09   1.000000             50.0     -1.000000   \n",
       "172     gd-none  1.718197e+09   1.000000             50.0     -1.000000   \n",
       "172     gd-none  1.718197e+09   1.000000             50.0     -1.000000   \n",
       "\n",
       "     wbits  sparsity_ratio  qa_question_count_limit_x  mmlu accuracy  \\\n",
       "222   16.0             0.0                        NaN       0.607114   \n",
       "222   16.0             0.0                        NaN       0.607114   \n",
       "222   16.0             0.0                        NaN       0.607114   \n",
       "222   16.0             0.0                        NaN       0.607114   \n",
       "222   16.0             0.0                        NaN       0.607114   \n",
       "222   16.0             0.0                        NaN       0.607114   \n",
       "222   16.0             0.0                        NaN       0.607114   \n",
       "222   16.0             0.0                        NaN       0.607114   \n",
       "248   16.0             0.0                        NaN       0.610080   \n",
       "248   16.0             0.0                        NaN       0.610080   \n",
       "248   16.0             0.0                        NaN       0.610080   \n",
       "248   16.0             0.0                        NaN       0.610080   \n",
       "248   16.0             0.0                        NaN       0.610080   \n",
       "248   16.0             0.0                        NaN       0.610080   \n",
       "248   16.0             0.0                        NaN       0.610080   \n",
       "248   16.0             0.0                        NaN       0.610080   \n",
       "132   16.0             0.0                        NaN       0.613931   \n",
       "132   16.0             0.0                        NaN       0.613931   \n",
       "132   16.0             0.0                        NaN       0.613931   \n",
       "132   16.0             0.0                        NaN       0.613931   \n",
       "132   16.0             0.0                        NaN       0.613931   \n",
       "132   16.0             0.0                        NaN       0.613931   \n",
       "132   16.0             0.0                        NaN       0.613931   \n",
       "132   16.0             0.0                        NaN       0.613931   \n",
       "262   16.0             0.0                        NaN       0.570263   \n",
       "262   16.0             0.0                        NaN       0.570263   \n",
       "262   16.0             0.0                        NaN       0.570263   \n",
       "262   16.0             0.0                        NaN       0.570263   \n",
       "262   16.0             0.0                        NaN       0.570263   \n",
       "262   16.0             0.0                        NaN       0.570263   \n",
       "262   16.0             0.0                        NaN       0.570263   \n",
       "262   16.0             0.0                        NaN       0.570263   \n",
       "146   16.0             0.0                        NaN       0.501709   \n",
       "146   16.0             0.0                        NaN       0.501709   \n",
       "146   16.0             0.0                        NaN       0.501709   \n",
       "146   16.0             0.0                        NaN       0.501709   \n",
       "146   16.0             0.0                        NaN       0.501709   \n",
       "146   16.0             0.0                        NaN       0.501709   \n",
       "146   16.0             0.0                        NaN       0.501709   \n",
       "146   16.0             0.0                        NaN       0.501709   \n",
       "172   16.0             0.0                        NaN       0.523002   \n",
       "172   16.0             0.0                        NaN       0.523002   \n",
       "172   16.0             0.0                        NaN       0.523002   \n",
       "172   16.0             0.0                        NaN       0.523002   \n",
       "172   16.0             0.0                        NaN       0.523002   \n",
       "172   16.0             0.0                        NaN       0.523002   \n",
       "172   16.0             0.0                        NaN       0.523002   \n",
       "172   16.0             0.0                        NaN       0.523002   \n",
       "\n",
       "     wmdp_bio accuracy  ...    model_name  edit_dataset  compression_dataset  \\\n",
       "222           0.691359  ...  Llama-3 (8b)          zsre                   c4   \n",
       "222           0.691359  ...  Llama-3 (8b)          zsre                   c4   \n",
       "222           0.691359  ...  Llama-3 (8b)          zsre                   c4   \n",
       "222           0.691359  ...  Llama-3 (8b)          zsre                   c4   \n",
       "222           0.691359  ...  Llama-3 (8b)          zsre                   c4   \n",
       "222           0.691359  ...  Llama-3 (8b)          zsre                   c4   \n",
       "222           0.691359  ...  Llama-3 (8b)          zsre                   c4   \n",
       "222           0.691359  ...  Llama-3 (8b)          zsre                   c4   \n",
       "248           0.701885  ...  Llama-3 (8b)          zsre                   c4   \n",
       "248           0.701885  ...  Llama-3 (8b)          zsre                   c4   \n",
       "248           0.701885  ...  Llama-3 (8b)          zsre                   c4   \n",
       "248           0.701885  ...  Llama-3 (8b)          zsre                   c4   \n",
       "248           0.701885  ...  Llama-3 (8b)          zsre                   c4   \n",
       "248           0.701885  ...  Llama-3 (8b)          zsre                   c4   \n",
       "248           0.701885  ...  Llama-3 (8b)          zsre                   c4   \n",
       "248           0.701885  ...  Llama-3 (8b)          zsre                   c4   \n",
       "132           0.696208  ...  Llama-3 (8b)          zsre                   c4   \n",
       "132           0.696208  ...  Llama-3 (8b)          zsre                   c4   \n",
       "132           0.696208  ...  Llama-3 (8b)          zsre                   c4   \n",
       "132           0.696208  ...  Llama-3 (8b)          zsre                   c4   \n",
       "132           0.696208  ...  Llama-3 (8b)          zsre                   c4   \n",
       "132           0.696208  ...  Llama-3 (8b)          zsre                   c4   \n",
       "132           0.696208  ...  Llama-3 (8b)          zsre                   c4   \n",
       "132           0.696208  ...  Llama-3 (8b)          zsre                   c4   \n",
       "262           0.291580  ...  Llama-3 (8b)          zsre                   c4   \n",
       "262           0.291580  ...  Llama-3 (8b)          zsre                   c4   \n",
       "262           0.291580  ...  Llama-3 (8b)          zsre                   c4   \n",
       "262           0.291580  ...  Llama-3 (8b)          zsre                   c4   \n",
       "262           0.291580  ...  Llama-3 (8b)          zsre                   c4   \n",
       "262           0.291580  ...  Llama-3 (8b)          zsre                   c4   \n",
       "262           0.291580  ...  Llama-3 (8b)          zsre                   c4   \n",
       "262           0.291580  ...  Llama-3 (8b)          zsre                   c4   \n",
       "146           0.565593  ...  Llama-3 (8b)        mquake                   c4   \n",
       "146           0.565593  ...  Llama-3 (8b)        mquake                   c4   \n",
       "146           0.565593  ...  Llama-3 (8b)        mquake                   c4   \n",
       "146           0.565593  ...  Llama-3 (8b)        mquake                   c4   \n",
       "146           0.565593  ...  Llama-3 (8b)        mquake                   c4   \n",
       "146           0.565593  ...  Llama-3 (8b)        mquake                   c4   \n",
       "146           0.565593  ...  Llama-3 (8b)        mquake                   c4   \n",
       "146           0.565593  ...  Llama-3 (8b)        mquake                   c4   \n",
       "172           0.267086  ...  Llama-3 (8b)        mquake                   c4   \n",
       "172           0.267086  ...  Llama-3 (8b)        mquake                   c4   \n",
       "172           0.267086  ...  Llama-3 (8b)        mquake                   c4   \n",
       "172           0.267086  ...  Llama-3 (8b)        mquake                   c4   \n",
       "172           0.267086  ...  Llama-3 (8b)        mquake                   c4   \n",
       "172           0.267086  ...  Llama-3 (8b)        mquake                   c4   \n",
       "172           0.267086  ...  Llama-3 (8b)        mquake                   c4   \n",
       "172           0.267086  ...  Llama-3 (8b)        mquake                   c4   \n",
       "\n",
       "     qa_question_count_limit_y        PPL     PPL edits       PPl QA  \\\n",
       "222                       None  19.251539   1828405.625  6347.870605   \n",
       "222                       None  19.251539   1828405.625  6347.870605   \n",
       "222                       None  19.251539   1828405.625  6347.870605   \n",
       "222                       None  19.251539   1828405.625  6347.870605   \n",
       "222                       None  19.251539   1828405.625  6347.870605   \n",
       "222                       None  19.251539   1828405.625  6347.870605   \n",
       "222                       None  19.251539   1828405.625  6347.870605   \n",
       "222                       None  19.251539   1828405.625  6347.870605   \n",
       "248                       None    5.56542  10351.842773   379.738983   \n",
       "248                       None    5.56542  10351.842773   379.738983   \n",
       "248                       None    5.56542  10351.842773   379.738983   \n",
       "248                       None    5.56542  10351.842773   379.738983   \n",
       "248                       None    5.56542  10351.842773   379.738983   \n",
       "248                       None    5.56542  10351.842773   379.738983   \n",
       "248                       None    5.56542  10351.842773   379.738983   \n",
       "248                       None    5.56542  10351.842773   379.738983   \n",
       "132                       None    5.57129  19718.240234   469.199768   \n",
       "132                       None    5.57129  19718.240234   469.199768   \n",
       "132                       None    5.57129  19718.240234   469.199768   \n",
       "132                       None    5.57129  19718.240234   469.199768   \n",
       "132                       None    5.57129  19718.240234   469.199768   \n",
       "132                       None    5.57129  19718.240234   469.199768   \n",
       "132                       None    5.57129  19718.240234   469.199768   \n",
       "132                       None    5.57129  19718.240234   469.199768   \n",
       "262                       None   5.555923  25474.263672   400.781067   \n",
       "262                       None   5.555923  25474.263672   400.781067   \n",
       "262                       None   5.555923  25474.263672   400.781067   \n",
       "262                       None   5.555923  25474.263672   400.781067   \n",
       "262                       None   5.555923  25474.263672   400.781067   \n",
       "262                       None   5.555923  25474.263672   400.781067   \n",
       "262                       None   5.555923  25474.263672   400.781067   \n",
       "262                       None   5.555923  25474.263672   400.781067   \n",
       "146                       None   Infinity      Infinity     Infinity   \n",
       "146                       None   Infinity      Infinity     Infinity   \n",
       "146                       None   Infinity      Infinity     Infinity   \n",
       "146                       None   Infinity      Infinity     Infinity   \n",
       "146                       None   Infinity      Infinity     Infinity   \n",
       "146                       None   Infinity      Infinity     Infinity   \n",
       "146                       None   Infinity      Infinity     Infinity   \n",
       "146                       None   Infinity      Infinity     Infinity   \n",
       "172                       None   4.476031  59003.925781   912.782715   \n",
       "172                       None   4.476031  59003.925781   912.782715   \n",
       "172                       None   4.476031  59003.925781   912.782715   \n",
       "172                       None   4.476031  59003.925781   912.782715   \n",
       "172                       None   4.476031  59003.925781   912.782715   \n",
       "172                       None   4.476031  59003.925781   912.782715   \n",
       "172                       None   4.476031  59003.925781   912.782715   \n",
       "172                       None   4.476031  59003.925781   912.782715   \n",
       "\n",
       "           FLOPs  PPl edits unmasked                        date_y  \n",
       "222  1.79 TFLOPS        15581.417969 2024-06-09 08:47:23.456239616  \n",
       "222  1.79 TFLOPS        15581.417969 2024-06-09 08:47:23.456239616  \n",
       "222  1.79 TFLOPS        15581.417969 2024-06-09 08:47:23.456239616  \n",
       "222  1.79 TFLOPS        15581.417969 2024-06-09 08:47:23.456239616  \n",
       "222  1.79 TFLOPS        15581.417969 2024-06-09 08:47:23.456239616  \n",
       "222  1.79 TFLOPS        15581.417969 2024-06-09 08:47:23.456239616  \n",
       "222  1.79 TFLOPS        15581.417969 2024-06-09 08:47:23.456239616  \n",
       "222  1.79 TFLOPS        15581.417969 2024-06-09 08:47:23.456239616  \n",
       "248  1.92 TFLOPS           447.97879 2024-05-21 14:15:49.471601152  \n",
       "248  1.92 TFLOPS           447.97879 2024-05-21 14:15:49.471601152  \n",
       "248  1.92 TFLOPS           447.97879 2024-05-21 14:15:49.471601152  \n",
       "248  1.92 TFLOPS           447.97879 2024-05-21 14:15:49.471601152  \n",
       "248  1.92 TFLOPS           447.97879 2024-05-21 14:15:49.471601152  \n",
       "248  1.92 TFLOPS           447.97879 2024-05-21 14:15:49.471601152  \n",
       "248  1.92 TFLOPS           447.97879 2024-05-21 14:15:49.471601152  \n",
       "248  1.92 TFLOPS           447.97879 2024-05-21 14:15:49.471601152  \n",
       "132  1.92 TFLOPS          544.861145 2024-05-21 14:14:55.217775104  \n",
       "132  1.92 TFLOPS          544.861145 2024-05-21 14:14:55.217775104  \n",
       "132  1.92 TFLOPS          544.861145 2024-05-21 14:14:55.217775104  \n",
       "132  1.92 TFLOPS          544.861145 2024-05-21 14:14:55.217775104  \n",
       "132  1.92 TFLOPS          544.861145 2024-05-21 14:14:55.217775104  \n",
       "132  1.92 TFLOPS          544.861145 2024-05-21 14:14:55.217775104  \n",
       "132  1.92 TFLOPS          544.861145 2024-05-21 14:14:55.217775104  \n",
       "132  1.92 TFLOPS          544.861145 2024-05-21 14:14:55.217775104  \n",
       "262  1.92 TFLOPS          444.527161 2024-05-20 18:20:06.459071232  \n",
       "262  1.92 TFLOPS          444.527161 2024-05-20 18:20:06.459071232  \n",
       "262  1.92 TFLOPS          444.527161 2024-05-20 18:20:06.459071232  \n",
       "262  1.92 TFLOPS          444.527161 2024-05-20 18:20:06.459071232  \n",
       "262  1.92 TFLOPS          444.527161 2024-05-20 18:20:06.459071232  \n",
       "262  1.92 TFLOPS          444.527161 2024-05-20 18:20:06.459071232  \n",
       "262  1.92 TFLOPS          444.527161 2024-05-20 18:20:06.459071232  \n",
       "262  1.92 TFLOPS          444.527161 2024-05-20 18:20:06.459071232  \n",
       "146  1.03 TFLOPS            Infinity 2024-06-12 09:58:53.057244672  \n",
       "146  1.03 TFLOPS            Infinity 2024-06-12 09:58:53.057244672  \n",
       "146  1.03 TFLOPS            Infinity 2024-06-12 09:58:53.057244672  \n",
       "146  1.03 TFLOPS            Infinity 2024-06-12 09:58:53.057244672  \n",
       "146  1.03 TFLOPS            Infinity 2024-06-12 09:58:53.057244672  \n",
       "146  1.03 TFLOPS            Infinity 2024-06-12 09:58:53.057244672  \n",
       "146  1.03 TFLOPS            Infinity 2024-06-12 09:58:53.057244672  \n",
       "146  1.03 TFLOPS            Infinity 2024-06-12 09:58:53.057244672  \n",
       "172  1.03 TFLOPS         1600.669312 2024-06-12 12:50:30.144293888  \n",
       "172  1.03 TFLOPS         1600.669312 2024-06-12 12:50:30.144293888  \n",
       "172  1.03 TFLOPS         1600.669312 2024-06-12 12:50:30.144293888  \n",
       "172  1.03 TFLOPS         1600.669312 2024-06-12 12:50:30.144293888  \n",
       "172  1.03 TFLOPS         1600.669312 2024-06-12 12:50:30.144293888  \n",
       "172  1.03 TFLOPS         1600.669312 2024-06-12 12:50:30.144293888  \n",
       "172  1.03 TFLOPS         1600.669312 2024-06-12 12:50:30.144293888  \n",
       "172  1.03 TFLOPS         1600.669312 2024-06-12 12:50:30.144293888  \n",
       "\n",
       "[48 rows x 55 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want instances where editing has been applied but there is no unlearning or compression. In these cases, set wbits=16 and sparsity=0 \n",
    "baseline_editors = data[(data[\"edit\"].notnull()) & (data[\"unlearn\"].isnull()) & (data[\"compression\"].isnull()) & (data[\"interventions\"].apply(lambda x: x == [\"edit\"]))].copy()\n",
    "baseline_editors[\"wbits\"] = 16\n",
    "baseline_editors[\"sparsity_ratio\"] = 0\n",
    "news_records = []\n",
    "\n",
    "# Edit and Compress\n",
    "for editing_method in [\"LoRA\", \"MEMIT\", \"Fine-tune\"]:\n",
    "    baseline_record = baseline_editors[baseline_editors[\"edit\"] == editing_method]\n",
    "    for compression_method in [\"SparseGPT\", \"Wanda\", \"GPTQ\", \"AWQ\"]:\n",
    "        edit_first_record = baseline_record.copy()\n",
    "        edit_first_record[\"compression\"] = compression_method\n",
    "        edit_first_record[\"interventions\"] = [[\"edit\", \"compress\"]]\n",
    "        edit_first_record[\"sparsity_ratio\"] = 0\n",
    "        edit_first_record[\"wbits\"] = 16\n",
    "        news_records.append(edit_first_record)\n",
    "\n",
    "        compress_first_record = baseline_record.copy()\n",
    "        compress_first_record[\"compression\"] = compression_method\n",
    "        compress_first_record[\"interventions\"] = [[\"compress\", \"edit\"]]\n",
    "        compress_first_record[\"sparsity_ratio\"] = 0\n",
    "        compress_first_record[\"wbits\"] = 16\n",
    "        news_records.append(compress_first_record)\n",
    "\n",
    "baseline_unlearners = data[(data[\"edit\"].isnull()) & (data[\"unlearn\"].notnull()) & (data[\"compression\"].isnull()) & (data[\"interventions\"].apply(lambda x: x == [\"unlearn\"]))].copy()\n",
    "\n",
    "# Compress and Unlearn\n",
    "for unlearn_method in [\"RMU\", \"GA\", \"GD\"]:\n",
    "    baseline_record = baseline_unlearners[baseline_unlearners[\"unlearn\"] == unlearn_method]\n",
    "\n",
    "    for compression_method in [\"SparseGPT\", \"Wanda\", \"GPTQ\", \"AWQ\"]:\n",
    "        compress_first_record = baseline_record.copy()\n",
    "        compress_first_record[\"unlearn\"] = unlearn_method\n",
    "        compress_first_record[\"compression\"] = compression_method\n",
    "        compress_first_record[\"interventions\"] = [[\"compress\", \"unlearn\"]]\n",
    "        news_records.append(compress_first_record)\n",
    "\n",
    "        unlearn_first_record = baseline_record.copy()\n",
    "        unlearn_first_record[\"unlearn\"] = unlearn_method\n",
    "        unlearn_first_record[\"compression\"] = compression_method\n",
    "        unlearn_first_record[\"interventions\"] = [[\"unlearn\", \"compress\"]]\n",
    "        news_records.append(unlearn_first_record)\n",
    "\n",
    "baseline_records = pd.concat(news_records)\n",
    "data = pd.concat([data, baseline_records])\n",
    "baseline_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing and Compresion Single Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: KE ←→ Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "compositions_by_col = {\n",
    "    # MEMIT and WANDA + SparseGPT\n",
    "    0: [(\"MEMIT→SparseGPT\", \"SparseGPT→MEMIT\"), (\"MEMIT→Wanda\", \"Wanda→MEMIT\")],\n",
    "    # LoRA and WANDA + SparseGPT\n",
    "    1: [(\"LoRA→SparseGPT\", \"SparseGPT→LoRA\"), (\"LoRA→Wanda\", \"Wanda→LoRA\")],\n",
    "    # FT and WANDA + SparseGPT\n",
    "    2: [(\"Fine-tune→SparseGPT\", \"SparseGPT→Fine-tune\"), (\"Fine-tune→Wanda\", \"Wanda→Fine-tune\")],\n",
    "    # MEMIT and GPTQ + AWQ\n",
    "    3: [(\"MEMIT→GPTQ\", \"GPTQ→MEMIT\"), (\"MEMIT→AWQ\", \"AWQ→MEMIT\")],\n",
    "    # LoRA and GPTQ + AWQ\n",
    "    4: [(\"LoRA→GPTQ\", \"GPTQ→LoRA\"), (\"LoRA→AWQ\", \"AWQ→LoRA\")],\n",
    "    # FT and GPTQ + AWQ\n",
    "    5: [(\"Fine-tune→GPTQ\", \"GPTQ→Fine-tune\"), (\"Fine-tune→AWQ\", \"AWQ→Fine-tune\")],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pruning_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpruning_frame\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pruning_frame' is not defined"
     ]
    }
   ],
   "source": [
    "pruning_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_frame = data[((data[\"compression\"] == \"SparseGPT\") | (data[\"compression\"] == \"Wanda\")) & (data[\"edit\"] != None) & (data[\"interventions\"].apply(lambda x: len(x) > 1))]\n",
    "pruning_frame[\"order\"] = pruning_frame.apply(get_order_label, axis=1)\n",
    "pruning_frame = pruning_frame.sort_values(by=\"order\")\n",
    "\n",
    "quantization_frame = data[((data[\"compression\"] == \"GPTQ\") | (data[\"compression\"] == \"AWQ\")) & (data[\"edit\"] != None) & (data[\"interventions\"].apply(lambda x: len(x) > 1))]\n",
    "quantization_frame[\"order\"] = quantization_frame.apply(get_order_label, axis=1)\n",
    "quantization_frame = quantization_frame.sort_values(by=\"order\")\n",
    "\n",
    "# 4 columns and 3 rows\n",
    "fig, axes = plt.subplots(3, 6, figsize=(6 * FIG_SIZE, 3 * FIG_SIZE))\n",
    "row_metrics = {\n",
    "    0: \"Rewrite accuracy\",\n",
    "    1: \"Generalization\",\n",
    "    2: \"mmlu accuracy\",\n",
    "}\n",
    "row_labels = {\n",
    "    0: r\"Edit Success$ \\uparrow$\",\n",
    "    1: r\"Generalization$ \\uparrow$\",\n",
    "    2: r\"MMLU$ \\uparrow$\"\n",
    "}\n",
    "column_edit_methods = {\n",
    "    0: \"MEMIT\",\n",
    "    1: \"LoRA\",\n",
    "    2: \"Fine-tune\",\n",
    "    3: \"MEMIT\",\n",
    "    4: \"LoRA\",\n",
    "    5: \"Fine-tune\"\n",
    "}\n",
    "\n",
    "compositions_by_col = {\n",
    "    # MEMIT and WANDA + SparseGPT\n",
    "    0: [(\"MEMIT→SparseGPT\", \"SparseGPT→MEMIT\"), (\"MEMIT→Wanda\", \"Wanda→MEMIT\")],\n",
    "    # LoRA and WANDA + SparseGPT\n",
    "    1: [(\"LoRA→SparseGPT\", \"SparseGPT→LoRA\"), (\"LoRA→Wanda\", \"Wanda→LoRA\")],\n",
    "    # FT and WANDA + SparseGPT\n",
    "    2: [(\"Fine-tune→SparseGPT\", \"SparseGPT→Fine-tune\"), (\"Fine-tune→Wanda\", \"Wanda→Fine-tune\")],\n",
    "    # MEMIT and GPTQ + AWQ\n",
    "    3: [(\"MEMIT→GPTQ\", \"GPTQ→MEMIT\"), (\"MEMIT→AWQ\", \"AWQ→MEMIT\")],\n",
    "    # LoRA and GPTQ + AWQ\n",
    "    4: [(\"LoRA→GPTQ\", \"GPTQ→LoRA\"), (\"LoRA→AWQ\", \"AWQ→LoRA\")],\n",
    "    # FT and GPTQ + AWQ\n",
    "    5: [(\"Fine-tune→GPTQ\", \"GPTQ→Fine-tune\"), (\"Fine-tune→AWQ\", \"AWQ→Fine-tune\")],\n",
    "}\n",
    "for row_index, y_metric in row_metrics.items():\n",
    "    for col_index, plotting_frame in enumerate([pruning_frame, pruning_frame, pruning_frame, quantization_frame, quantization_frame, quantization_frame]):\n",
    "        ax = axes[row_index][col_index]\n",
    "        x_metric = \"sparsity_ratio\" if col_index < 3 else \"wbits\"\n",
    "        plotting_frame = plotting_frame[plotting_frame[\"edit\"] == column_edit_methods[col_index]]\n",
    "\n",
    "        for composition in compositions_by_col[col_index]:\n",
    "            compression_method = [method for method in composition[0].split(\"→\") if method not in [\"MEMIT\", \"LoRA\", \"Fine-tune\"]][0]\n",
    "            first_line = plotting_frame[plotting_frame[\"order\"] == composition[0]]\n",
    "            first_line[\"label\"] = first_line[\"order\"].apply(wrap_label)\n",
    "            second_line = plotting_frame[plotting_frame[\"order\"] == composition[1]].sort_values(x_metric)\n",
    "            second_line[\"label\"] = second_line[\"order\"].apply(wrap_label)\n",
    "            if compression_method in [\"AWQ\", \"GPTQ\"]:\n",
    "                first_line = first_line.sort_values(x_metric, ascending=False)\n",
    "                second_line = second_line.sort_values(x_metric, ascending=False)\n",
    "            else:\n",
    "                first_line = first_line.sort_values(x_metric)\n",
    "                second_line = second_line.sort_values(x_metric)\n",
    "\n",
    "            ax.plot(first_line[x_metric], first_line[y_metric], marker=\"o\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{composition[0]}\")\n",
    "            ax.plot(second_line[x_metric], second_line[y_metric], markerfacecolor='none', marker=\"o\", ls=\"--\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{composition[1]}\")\n",
    "            ax.fill_between(\n",
    "                x=first_line[x_metric], y1=first_line[y_metric], y2=second_line[y_metric],\n",
    "                alpha=0.3,\n",
    "                color=colors[compression_method]\n",
    "            )\n",
    "\n",
    "        if x_metric == \"wbits\":\n",
    "            ax.set_xscale(\"log\", base=2)\n",
    "            ax.set_xticks([2, 4, 8, 16], [\"2\", \"4\", \"8\", \"16\"])\n",
    "\n",
    "        if row_index != 2:\n",
    "            ax.set_ylim(0, 1.05)\n",
    "        else:\n",
    "            ax.set_ylim(0.2, 0.65)\n",
    "            ax.axhline(y=0.25, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "        if row_index == 0:\n",
    "            title = column_edit_methods[col_index] if column_edit_methods[col_index] != \"Fine-tune\" else \"FT\"\n",
    "            ax.set_title(title, fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_title(\"\")\n",
    "\n",
    "        if col_index == 0:\n",
    "            ax.set_ylabel(row_labels[row_index], fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "\n",
    "        if row_index == 2:\n",
    "            ax.set_xlabel(\"Sparsity\" if col_index < 3 else \"Bits\", fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_xlabel(\"\")\n",
    "\n",
    "        if row_index == 2:\n",
    "            ax.legend(fontsize=LEGEND_FONT_SIZE, frameon=False, loc=\"upper center\", bbox_to_anchor=(0.5, -0.3), ncol=1)\n",
    "\n",
    "fig.subplots_adjust(wspace=WSPACE, hspace=WSPACE)\n",
    "plt.savefig(\"figures/main_results_editors_compression.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: Unlearning ←→ Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_frame = data[((data[\"compression\"] == \"SparseGPT\") | (data[\"compression\"] == \"Wanda\")) & (data[\"unlearn\"] != None) & (data[\"interventions\"].apply(lambda x: len(x) > 1))]\n",
    "pruning_frame[\"order\"] = pruning_frame.apply(get_order_label, axis=1)\n",
    "pruning_frame = pruning_frame.sort_values(by=\"order\")\n",
    "pruning_frame[\"unlearn\"] = pruning_frame[\"unlearn\"].apply(lambda x: x.upper() if x is not None else None)\n",
    "\n",
    "quantization_frame = data[((data[\"compression\"] == \"GPTQ\") | (data[\"compression\"] == \"AWQ\")) & (data[\"unlearn\"] != None) & (data[\"interventions\"].apply(lambda x: len(x) > 1))]\n",
    "quantization_frame[\"order\"] = quantization_frame.apply(get_order_label, axis=1)\n",
    "quantization_frame = quantization_frame.sort_values(by=\"order\")\n",
    "quantization_frame[\"unlearn\"] = quantization_frame[\"unlearn\"].apply(lambda x: x.upper() if x is not None else None)\n",
    "\n",
    "# 4 columns and 3 rows\n",
    "fig, axes = plt.subplots(2, 6, figsize=(6 * FIG_SIZE, 2 * FIG_SIZE))\n",
    "row_metrics = {\n",
    "    0: \"Avg WMDP\",\n",
    "    1: \"mmlu accuracy\",\n",
    "}\n",
    "row_labels = {\n",
    "    \"Avg WMDP\": r\"WMDP $\\downarrow$\",\n",
    "    \"mmlu accuracy\": r\"MMLU $\\uparrow$\"\n",
    "}\n",
    "row_label_map = {\n",
    "    0: \"Avg WMDP\",\n",
    "    1: \"mmlu accuracy\"\n",
    "}\n",
    "column_unlearn_methods = {\n",
    "    0: \"RMU\",\n",
    "    1: \"GA\",\n",
    "    2: \"GD\",\n",
    "    3: \"RMU\",\n",
    "    4: \"GA\",\n",
    "    5: \"GD\",\n",
    "}\n",
    "\n",
    "compositions_by_col = {\n",
    "    # RMU and WANDA + SparseGPT\n",
    "    0: [(\"RMU→SparseGPT\", \"SparseGPT→RMU\"), (\"RMU→Wanda\", \"Wanda→RMU\")],\n",
    "    # GA and WANDA + SparseGPT\n",
    "    1: [(\"GA→SparseGPT\", \"SparseGPT→GA\"), (\"GA→Wanda\", \"Wanda→GA\")],\n",
    "    # FT and WANDA + SparseGPT\n",
    "    2: [(\"GD→SparseGPT\", \"SparseGPT→GD\"), (\"GD→Wanda\", \"Wanda→GD\")],\n",
    "    # RMU and GPTQ + AWQ\n",
    "    3: [(\"RMU→GPTQ\", \"GPTQ→RMU\"), (\"RMU→AWQ\", \"AWQ→RMU\")],\n",
    "    # GA and GPTQ + AWQ\n",
    "    4: [(\"GA→GPTQ\", \"GPTQ→GA\"), (\"GA→AWQ\", \"AWQ→GA\")],\n",
    "    # FT and GPTQ + AWQ\n",
    "    5: [(\"GD→GPTQ\", \"GPTQ→GD\"), (\"GD→AWQ\", \"AWQ→GD\")],\n",
    "}\n",
    "for row_index, y_metric in row_metrics.items():\n",
    "    for col_index, plotting_frame in enumerate([pruning_frame, pruning_frame, pruning_frame, quantization_frame, quantization_frame, quantization_frame]):\n",
    "        ax = axes[row_index][col_index]\n",
    "        x_metric = \"sparsity_ratio\" if col_index < 3 else \"wbits\"\n",
    "        plotting_frame = plotting_frame[plotting_frame[\"unlearn\"] == column_unlearn_methods[col_index]]\n",
    "\n",
    "        for composition in compositions_by_col[col_index]:\n",
    "            compression_method = [method for method in composition[0].split(\"→\") if method not in [\"RMU\", \"GA\", \"GD\"]][0]\n",
    "            first_line = plotting_frame[plotting_frame[\"order\"] == composition[0]]\n",
    "            first_line[\"label\"] = first_line[\"order\"].apply(wrap_label)\n",
    "            second_line = plotting_frame[plotting_frame[\"order\"] == composition[1]].sort_values(x_metric)\n",
    "            second_line[\"label\"] = second_line[\"order\"].apply(wrap_label)\n",
    "            if compression_method in [\"AWQ\", \"GPTQ\"]:\n",
    "                first_line = first_line.sort_values(x_metric, ascending=False)\n",
    "                second_line = second_line.sort_values(x_metric, ascending=False)\n",
    "            else:\n",
    "                first_line = first_line.sort_values(x_metric)\n",
    "                second_line = second_line.sort_values(x_metric)\n",
    "\n",
    "            ax.plot(first_line[x_metric], first_line[y_metric], marker=\"o\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{composition[0]}\")\n",
    "            ax.plot(second_line[x_metric], second_line[y_metric], markerfacecolor='none', marker=\"o\", ls=\"--\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{composition[1]}\")\n",
    "            ax.fill_between(\n",
    "                x=first_line[x_metric], y1=first_line[y_metric], y2=second_line[y_metric],\n",
    "                alpha=0.3,\n",
    "                color=colors[compression_method]\n",
    "            )\n",
    "\n",
    "        ax.axhline(y=0.25, color=\"gray\", linestyle=\"--\")\n",
    "        ax.set_ylim(0.20, 0.65)\n",
    "\n",
    "        if x_metric == \"wbits\":\n",
    "            ax.set_xscale(\"log\", base=2)\n",
    "            ax.set_xticks([2, 4, 8, 16], [\"2\", \"4\", \"8\", \"16\"])\n",
    "\n",
    "        if row_index == 0:\n",
    "            title = column_unlearn_methods[col_index]\n",
    "            ax.set_title(title, fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_title(\"\")\n",
    "\n",
    "        if col_index == 0:\n",
    "            ax.set_ylabel(row_labels[list(row_labels.keys())[row_index]], fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "\n",
    "        if row_index == 1:\n",
    "            ax.set_xlabel(\"Sparsity\" if col_index < 3 else \"Bits\", fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_xlabel(\"\")\n",
    "\n",
    "        if row_index == 1:\n",
    "            ax.legend(fontsize=LEGEND_FONT_SIZE, frameon=False, loc=\"upper center\", bbox_to_anchor=(0.5, -0.3), ncol=1)\n",
    "\n",
    "fig.subplots_adjust(wspace=WSPACE, hspace=WSPACE)\n",
    "plt.savefig(\"figures/main_results_unlearn_compression.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
