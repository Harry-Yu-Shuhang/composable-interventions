{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       lr   sym                     tag  edit  rank  save  seed  wbit          dtype   wandb  wbits  device method dataset nearest unlearn alg_name  compress  edit_set new_eval  nsamples  percdamp act_order                                          ckpt_path  groupsize  kl_factor  load_ckpt lora_type  num_steps  save_ckpt              stats_dir  batch_size  lora_alpha  max_length                  model_name save_model zero_point compression use_variant edit_dataset  lora_dropout quant_method  weight_decay     interventions static_groups  eval_zero_shot  model_parallel  sparsity_ratio    target_modules  norm_constraint  number_of_edits true_sequential compression_dataset  Local recall  wmdp_cyber accuracy                                            Metrics  mmlu stderr  wmdp_bio accuracy  PPl edits unmasked  Success recall  Average bits  Generalization         PPL  wmdp_bio             _wandb  Rewrite accuracy  Generalization recall      mmlu  _step      PPL edits  mmlu accuracy  \\\n",
      "0   0.005  True        GPTQ8bit-to-lora  lora     8  out/     1     8  torch.float16  online    4.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True        gptq       False         zsre             0     autogptq             0  [compress, edit]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.023364             0.411676  {'_type': 'table-file', 'ncols': 1, 'nrows': 1...     0.003936           0.653574         4293.975586        0.140889      4.250000        0.045889   10.741209  0.653574  {'runtime': 6720}          0.140889               0.045889  0.565233      1   17056.351562       0.565233   \n",
      "1   0.005  True        GPTQ4bit-to-lora  lora     8  out/     1     4  torch.float16  online    4.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True        gptq       False         zsre             0     autogptq             0  [compress, edit]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.023364             0.411676  {'nrows': 13, 'sha256': '70ab931a22b2944fa99fd...     0.003936           0.653574         4293.975586        0.140889      4.250000        0.045889   10.741209  0.653574  {'runtime': 6686}          0.140889               0.045889  0.565233      1   17056.351562       0.565233   \n",
      "2   0.005  True         AWQ8bit-to-lora  lora     8  out/     1     8  torch.float16  online    8.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True         awq       False         zsre             0      autoawq             0  [compress, edit]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.092327             0.459487  {'sha256': 'c5a558def3eb1bda0cf6319be04a632202...     0.003829           0.688924        11900.338867        1.000000      8.250000        0.674000   12.323073  0.688924  {'runtime': 3107}          1.000000               0.674000  0.605683      1  311922.125000       0.605683   \n",
      "3   0.005  True        GPTQ2bit-to-lora  lora     8  out/     1     2  torch.float16  online    4.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True        gptq       False         zsre             0     autogptq             0  [compress, edit]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.023364             0.411676  {'size': 306, '_type': 'table-file', 'ncols': ...     0.003936           0.653574         4293.975586        0.140889      4.250000        0.045889   10.741209  0.653574  {'runtime': 6689}          0.140889               0.045889  0.565233      1   17056.351562       0.565233   \n",
      "4   0.005  True         AWQ4bit-to-lora  lora     8  out/     1     4  torch.float16  online    8.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True         awq       False         zsre             0      autoawq             0  [compress, edit]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.092327             0.459487  {'sha256': 'c5a558def3eb1bda0cf6319be04a632202...     0.003829           0.688924        11900.338867        1.000000      8.250000        0.674000   12.323073  0.688924  {'runtime': 3512}          1.000000               0.674000  0.605683      1  311922.125000       0.605683   \n",
      "5   0.005  True        lora-to-GPTQ4bit  lora     8  out/     1     4  torch.float16  online    4.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True        gptq       False         zsre             0     autogptq             0  [edit, compress]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.068677             0.431303  {'sha256': '5527b8c0bb34383f8d5744a0462e85afd1...     0.003903           0.673213         8182.892090        0.520333      4.250000        0.362222  116.237091  0.673213  {'runtime': 4304}          0.521051               0.360222  0.586384      1   40680.859375       0.586384   \n",
      "6   0.005  True         AWQ2bit-to-lora  lora     8  out/     1     2  torch.float16  online    8.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True         awq       False         zsre             0      autoawq             0  [compress, edit]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.092327             0.459487  {'_type': 'table-file', 'ncols': 1, 'nrows': 1...     0.003829           0.688924        11900.338867        1.000000      8.250000        0.674000   12.323073  0.688924  {'runtime': 3003}          1.000000               0.674000  0.605683      1  311922.125000       0.605683   \n",
      "7   0.005  True        lora-to-GPTQ8bit  lora     8  out/     1     8  torch.float16  online    4.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True        gptq       False         zsre             0     autogptq             0  [edit, compress]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.068677             0.431303  {'sha256': '5527b8c0bb34383f8d5744a0462e85afd1...     0.003903           0.673213         8182.892090        0.520333      4.250000        0.362222  116.237091  0.673213  {'runtime': 4226}          0.521051               0.360222  0.586384      1   40680.859375       0.586384   \n",
      "8   0.005  True        lora-to-GPTQ2bit  lora     8  out/     1     2  torch.float16  online    4.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True        gptq       False         zsre             0     autogptq             0  [edit, compress]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.068677             0.431303  {'size': 305, '_type': 'table-file', 'ncols': ...     0.003903           0.673213         8182.892090        0.520333      4.250000        0.362222  116.237091  0.673213  {'runtime': 4280}          0.521051               0.360222  0.586384      1   40680.859375       0.586384   \n",
      "9   0.005  True         lora-to-AWQ8bit  lora     8  out/     1     8  torch.float16  online    8.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True         awq       False         zsre             0      autoawq             0  [edit, compress]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.068560             0.440866  {'_latest_artifact_path': 'wandb-client-artifa...     0.003828           0.686567         9423.383789        1.000000      8.250000        0.680000   21.798035  0.686567  {'runtime': 2505}          1.000000               0.680000  0.608532      1  269726.343750       0.608532   \n",
      "10  0.005  True         lora-to-AWQ4bit  lora     8  out/     1     4  torch.float16  online    8.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True         awq       False         zsre             0      autoawq             0  [edit, compress]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.068560             0.440866  {'_latest_artifact_path': 'wandb-client-artifa...     0.003828           0.686567         9423.383789        1.000000      8.250000        0.680000   21.798035  0.686567  {'runtime': 2409}          1.000000               0.680000  0.608532      1  269726.343750       0.608532   \n",
      "11  0.005  True         lora-to-AWQ2bit  lora     8  out/     1     2  torch.float16  online    8.0       0  quant      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True         awq       False         zsre             0      autoawq             0  [edit, compress]         False           False            True            0.00  [q_proj, v_proj]            False               50           False                  c4      0.068560             0.440866  {'size': 244, '_type': 'table-file', 'ncols': ...     0.003828           0.686567         9423.383789        1.000000      8.250000        0.680000   21.798035  0.686567  {'runtime': 2434}          1.000000               0.680000  0.608532      1  269726.343750       0.608532   \n",
      "12  0.005  True  SparseGPT0.65%-to-lora  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True   sparsegpt       False         zsre             0          NaN             0  [compress, edit]         False           False            True            0.65  [q_proj, v_proj]            False               50           False                  c4      0.012868             0.278309  {'sha256': '457567c4cd664ef21baad8a9494d1b3ad8...     0.003783           0.330715         1176.053467        0.316333      6.249982        0.195222   23.885410  0.330715  {'runtime': 5486}          0.316333               0.195222  0.280231      1   17039.265625       0.280231   \n",
      "13  0.005  True  SparseGPT0.45%-to-lora  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True   sparsegpt       False         zsre             0          NaN             0  [compress, edit]         False           False            True            0.45  [q_proj, v_proj]            False               50           False                  c4      0.034028             0.378460  {'nrows': 13, 'sha256': 'd0b61878ab27c8851f4de...     0.004041           0.593087         1712.821167        0.712000      9.249988        0.329043   10.683194  0.593087  {'runtime': 5494}          0.711231               0.329556  0.527560      1   14094.289062       0.527560   \n",
      "14  0.005  True  SparseGPT0.25%-to-lora  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True   sparsegpt       False         zsre             0          NaN             0  [compress, edit]         False           False            True            0.25  [q_proj, v_proj]            False               50           False                  c4      0.036143             0.408656  {'sha256': 'fc70f6d77e07c01348cc4582654edf1843...     0.003891           0.680283         2798.604980        0.878667     12.249977        0.532059    8.722971  0.680283  {'runtime': 5499}          0.878667               0.528333  0.591654      1   23380.888672       0.591654   \n",
      "15  0.005  True      Wanda0.65%-to-lora  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True       wanda       False         zsre             0          NaN             0  [compress, edit]         False           False            True            0.65  [q_proj, v_proj]            False               50           False                  c4      0.028355             0.285858  {'path': 'media/table/Metrics_1_5c3f41300c08a8...     0.003574           0.250589         1061.044434        0.341889      6.251183        0.153556   56.063610  0.250589  {'runtime': 2709}          0.341889               0.150222  0.235294      1   16654.775391       0.235294   \n",
      "16  0.005  True      Wanda0.45%-to-lora  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True       wanda       False         zsre             0          NaN             0  [compress, edit]         False           False            True            0.45  [q_proj, v_proj]            False               50           False                  c4      0.030011             0.354303  {'_latest_artifact_path': 'wandb-client-artifa...     0.004066           0.556167         1826.006836        0.813333      9.250592        0.451111   10.392533  0.556167  {'runtime': 2777}          0.813077               0.451111  0.497935      1   17782.494141       0.497935   \n",
      "17  0.005  True  lora-to-SparseGPT0.45%  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True   sparsegpt       False         zsre             0          NaN             0  [edit, compress]         False           False            True            0.45  [q_proj, v_proj]            False               50           False                  c4      0.030844             0.369904  {'size': 310, '_type': 'table-file', 'ncols': ...     0.004031           0.605656         2524.844482        0.224667      9.249987        0.209059   12.610009  0.605656  {'runtime': 3663}          0.224667               0.207333  0.527845      1   20250.359375       0.527845   \n",
      "18  0.005  True      Wanda0.25%-to-lora  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True       wanda       False         zsre             0          NaN             0  [compress, edit]         False           False            True            0.25  [q_proj, v_proj]            False               50           False                  c4      0.046528             0.418722  {'nrows': 13, 'sha256': '27bf676f55966a28ddbc2...     0.003866           0.684211         4470.392578        0.930000     12.250000        0.513737   10.451231  0.684211  {'runtime': 2776}          0.930000               0.512222  0.597849      1   41205.246094       0.597849   \n",
      "19  0.005  True  lora-to-SparseGPT0.65%  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True   sparsegpt       False         zsre             0          NaN             0  [edit, compress]         False           False            True            0.65  [q_proj, v_proj]            False               50           False                  c4      0.015972             0.282335  {'size': 305, '_type': 'table-file', 'ncols': ...     0.003746           0.298507         7437.687500        0.069556      6.249982        0.028000   36.162312  0.298507  {'runtime': 3629}          0.072889               0.026222  0.271186      1   50016.320312       0.271186   \n",
      "20  0.005  True  lora-to-SparseGPT0.25%  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True   sparsegpt       False         zsre             0          NaN             0  [edit, compress]         False           False            True            0.25  [q_proj, v_proj]            False               50           False                  c4      0.019594             0.416205  {'artifact_path': 'wandb-client-artifact://uqn...     0.003876           0.687353         3136.567383        0.798000     12.249977        0.474564   11.579182  0.687353  {'runtime': 3889}          0.798000               0.475333  0.594431      1   39739.625000       0.594431   \n",
      "21  0.005  True      lora-to-Wanda0.65%  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True       wanda       False         zsre             0          NaN             0  [edit, compress]         False           False            True            0.65  [q_proj, v_proj]            False               50           False                  c4      0.008900             0.261701  {'nrows': 13, 'sha256': 'e9905e9c8696d2132a2a8...     0.003560           0.247447        10104.917969        0.035556      6.251183        0.028889  136.677673  0.247447  {'runtime': 2194}          0.035556               0.028889  0.232802      1   79418.671875       0.232802   \n",
      "22  0.005  True      lora-to-Wanda0.45%  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True       wanda       False         zsre             0          NaN             0  [edit, compress]         False           False            True            0.45  [q_proj, v_proj]            False               50           False                  c4      0.060979             0.359336  {'_latest_artifact_path': 'wandb-client-artifa...     0.004054           0.557738         2990.944824        0.269667      9.250592        0.183136   14.182425  0.557738  {'runtime': 2074}          0.271667               0.183000  0.500712      1   17090.904297       0.500712   \n",
      "23  0.005  True      lora-to-Wanda0.25%  lora     8  out/     1    16    torch.float  online    4.0       0  prune      c4   False    none     LoRA      True         1    False     128.0      0.01     False  /scratch/sux7mp/saved_models/checkpoint_202405...      128.0          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None       True       wanda       False         zsre             0          NaN             0  [edit, compress]         False           False            True            0.25  [q_proj, v_proj]            False               50           False                  c4      0.054393             0.422245  {'_type': 'table-file', 'ncols': 1, 'nrows': 1...     0.003877           0.679497         3293.062256        0.863000     12.250000        0.532059   11.944833  0.679497  {'runtime': 2096}          0.863000               0.531667  0.593434      1   41214.324219       0.593434   \n",
      "24  0.005   NaN               lora_Edit  lora     8  out/     1    16    torch.float  online    NaN       0   none      c4     NaN    none     LoRA     False         1      NaN       NaN       NaN       NaN  /scratch/sux7mp/saved_models/checkpoint_202405...        NaN          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None        NaN        none         NaN         zsre             0          NaN             0            [edit]           NaN           False            True            0.00  [q_proj, v_proj]            False               50             NaN                  c4      0.065483             0.432813  {'_type': 'table-file', 'ncols': 1, 'nrows': 1...     0.003831           0.681068         9245.526367        1.000000     16.000000        0.628333   20.835436  0.681068  {'runtime': 4878}          1.000000               0.625000  0.607036      1  310653.625000       0.607036   \n",
      "25  0.005   NaN               lora_Edit  lora     8  out/     1    16    torch.float  online    NaN       0   none      c4     NaN    none     LoRA     False         1      NaN       NaN       NaN       NaN  /scratch/sux7mp/saved_models/checkpoint_202405...        NaN          0      False   adalora         70      False  /scratch/sux7mp/stats          50          32          30  meta-llama/Meta-Llama-3-8B       None        NaN        none         NaN         zsre             0          NaN             0            [edit]           NaN           False            True            0.00  [q_proj, v_proj]            False               50             NaN                  c4      0.065483             0.432813  {'size': 258, '_type': 'table-file', 'ncols': ...     0.003831           0.681068         9245.526367        1.000000     16.000000        0.628333   20.835436  0.681068  {'runtime': 5012}          1.000000               0.625000  0.607036      1  310653.625000       0.607036   \n",
      "\n",
      "    wmdp_bio stderr  Locality     _runtime     Latency    _timestamp          FLOPs        PPl QA  wmdp_cyber  wmdp_cyber stderr prune_method sparsity_type  \n",
      "0          0.013342  0.023484  6721.256265   88.206251  1.716168e+09             -1   4467.038574    0.411676           0.011043          NaN           NaN  \n",
      "1          0.013342  0.023484  6686.885360   87.801237  1.716168e+09             -1   4467.038574    0.411676           0.011043          NaN           NaN  \n",
      "2          0.012980  0.091904  3107.301727   88.398943  1.716164e+09             -1   5120.088379    0.459487           0.011183          NaN           NaN  \n",
      "3          0.013342  0.023484  6689.302995   87.288736  1.716168e+09             -1   4467.038574    0.411676           0.011043          NaN           NaN  \n",
      "4          0.012980  0.091904  3513.070510   88.332665  1.716165e+09             -1   5120.088379    0.459487           0.011183          NaN           NaN  \n",
      "5          0.013151  0.068495  4305.124583   89.146341  1.716165e+09             -1   8908.090820    0.431303           0.011113          NaN           NaN  \n",
      "6          0.012980  0.091904  3003.519610   87.133411  1.716164e+09             -1   5120.088379    0.459487           0.011183          NaN           NaN  \n",
      "7          0.013151  0.068495  4226.307660   86.997235  1.716165e+09             -1   8908.090820    0.431303           0.011113          NaN           NaN  \n",
      "8          0.013151  0.068495  4281.079558   88.586398  1.716165e+09             -1   8908.090820    0.431303           0.011113          NaN           NaN  \n",
      "9          0.013007  0.068528  2505.308359   86.186679  1.716163e+09             -1   6732.206543    0.440866           0.011141          NaN           NaN  \n",
      "10         0.013007  0.068528  2409.430656   87.282426  1.716163e+09             -1   6732.206543    0.440866           0.011141          NaN           NaN  \n",
      "11         0.013007  0.068528  2435.119612   88.637070  1.716163e+09             -1   6732.206543    0.440866           0.011141          NaN           NaN  \n",
      "12         0.013191  0.013377  5487.175279   -1.000000  1.716164e+09  625.35 GFLOPS   1819.863892    0.278309           0.010057    sparsegpt  unstructured  \n",
      "13         0.013774  0.033835  5494.409255   -1.000000  1.716164e+09  982.69 GFLOPS   1882.848633    0.378460           0.010883    sparsegpt  unstructured  \n",
      "14         0.013076  0.036167  5499.828287   -1.000000  1.716164e+09    1.34 TFLOPS   2241.204834    0.408656           0.011031    sparsegpt  unstructured  \n",
      "15         0.012151  0.028507  2710.221536   -1.000000  1.716160e+09  625.49 GFLOPS   1451.647095    0.285858           0.010139        wanda  unstructured  \n",
      "16         0.013931  0.029763  2777.694628   -1.000000  1.716161e+09  982.76 GFLOPS   1957.150879    0.354303           0.010733        wanda  unstructured  \n",
      "17         0.013703  0.030890  3663.674436   -1.000000  1.716161e+09  982.69 GFLOPS   3010.068115    0.369904           0.010833    sparsegpt  unstructured  \n",
      "18         0.013033  0.046421  2776.934421   -1.000000  1.716160e+09    1.34 TFLOPS   3262.652344    0.418722           0.011070        wanda  unstructured  \n",
      "19         0.012831  0.016391  3629.438115   -1.000000  1.716161e+09  625.35 GFLOPS   8204.874023    0.282335           0.010101    sparsegpt  unstructured  \n",
      "20         0.012998  0.019578  3889.684889   -1.000000  1.716161e+09    1.34 TFLOPS   2814.028320    0.416205           0.011061    sparsegpt  unstructured  \n",
      "21         0.012099  0.009967  2195.195158   -1.000000  1.716159e+09  625.49 GFLOPS  10013.075195    0.261701           0.009863        wanda  unstructured  \n",
      "22         0.013926  0.060916  2075.089370   -1.000000  1.716159e+09  982.76 GFLOPS   3554.813477    0.359336           0.010767        wanda  unstructured  \n",
      "23         0.013085  0.053773  2097.189882   -1.000000  1.716158e+09    1.34 TFLOPS   3025.690186    0.422245           0.011083        wanda  unstructured  \n",
      "24         0.013068  0.065565  4878.766600  505.516880  1.716159e+09    1.79 TFLOPS   6264.785645    0.432813           0.011118          NaN           NaN  \n",
      "25         0.013068  0.065565  5012.793544  509.658921  1.716073e+09    1.79 TFLOPS   6264.785645    0.432813           0.011118          NaN           NaN  \n"
     ]
    }
   ],
   "source": [
    "# Replace 'username/project_name' with your specific project path\n",
    "project_path = 'dri-ice/AK_tests'\n",
    "\n",
    "# Get all runs from the project\n",
    "runs = api.runs(path=project_path)\n",
    "\n",
    "# # Define the start and end datetime for filtering\n",
    "# start_datetime = datetime(2024, 5, 18, 1, 0, 0)  # 1 AM on May 19, 2024\n",
    "# end_datetime = start_datetime + timedelta(days=1)  # 24 hours later\n",
    "\n",
    "# # Define the filters for runs created starting from 1 AM on May 19, 2024, with config edit='ft'\n",
    "# filters = {\n",
    "#     \"created_at\": {\n",
    "#         \"$gte\": start_datetime.isoformat(),\n",
    "#         \"$lt\": end_datetime.isoformat()\n",
    "#     },\n",
    "#     \"edit\": \"ft\"\n",
    "# }\n",
    "\n",
    "# # Get runs with the specified filters\n",
    "# runs = api.runs(path=project_path, filters=filters)\n",
    "\n",
    "filter_dict = {\n",
    "    \"config.edit\": \"lora\",\n",
    "    \"state\": \"finished\",\n",
    "    \"created_at\": {\"$gte\": \"2024-05-18\"}  # Use the correct date format\n",
    "}\n",
    "runs = api.runs(project_path, filters=filter_dict)\n",
    "\n",
    "# List to store individual DataFrames\n",
    "data_frames = []\n",
    "\n",
    "# Iterate over each run and capture the config and summary metrics\n",
    "for run in runs:\n",
    "    config_frame = pd.DataFrame([run.config])\n",
    "    summary_frame = pd.DataFrame([run.summary_metrics])\n",
    "    combined_frame = pd.concat([config_frame, summary_frame], axis=1)\n",
    "    data_frames.append(combined_frame)\n",
    "\n",
    "# Concatenate all the individual DataFrames into a single DataFrame\n",
    "all_runs_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Adjust pandas display settings\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.width', 1000)        # Set display width\n",
    "\n",
    "# Display the DataFrame\n",
    "print(all_runs_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
